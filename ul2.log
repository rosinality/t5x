Using ssh batch size of 8. Attempting to SSH into 1 nodes with a total of 8 workers.
SSH: Attempting to connect to worker 0...
SSH: Attempting to connect to worker 1...
SSH: Attempting to connect to worker 2...
SSH: Attempting to connect to worker 3...
SSH: Attempting to connect to worker 4...
SSH: Attempting to connect to worker 5...
SSH: Attempting to connect to worker 6...
SSH: Attempting to connect to worker 7...
2024-05-12 01:44:51.092782: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.152688: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.163592: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.187057: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.310653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.375795: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.425338: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
2024-05-12 01:44:51.443254: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'float'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'bytes'>" collision on scalar
WARNING:absl:Type handler registry overriding type "<class 'numpy.number'>" collision on scalar
I0512 01:44:53.319808 139995375843328 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.320711 139995375843328 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.373727 140139156215808 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.374589 140139156215808 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.382522 139861676046336 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.383417 139861676046336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.432608 140497406441472 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.433495 140497406441472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.439637 139933136287744 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.440465 139933136287744 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.569875 139995375843328 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.570342 139995375843328 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.570405 139995375843328 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.570455 139995375843328 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.621098 140139156215808 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.621563 140139156215808 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.621623 140139156215808 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.621668 140139156215808 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.626538 139995375843328 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.626712 139995375843328 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.626770 139995375843328 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.626815 139995375843328 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.627669 139995375843328 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.627800 139995375843328 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.627854 139995375843328 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.627896 139995375843328 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.615406 139836023437312 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.616293 139836023437312 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.633348 139861676046336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.633818 139861676046336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.633880 139861676046336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.633926 139861676046336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.671940 139995375843328 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.677736 140139156215808 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.677914 140139156215808 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.677971 140139156215808 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.678014 140139156215808 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.678890 140139156215808 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.679022 140139156215808 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.679075 140139156215808 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.679114 140139156215808 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.686215 140497406441472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.686730 140497406441472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.686792 140497406441472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.686838 140497406441472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.690048 139861676046336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.690229 139861676046336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.690289 139861676046336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.690332 139861676046336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.691232 139861676046336 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.691369 139861676046336 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.691423 139861676046336 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.691463 139861676046336 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.682806 139933136287744 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.683223 139933136287744 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.683284 139933136287744 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.683331 139933136287744 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.695425 139867290535936 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.696314 139867290535936 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.703403 140600233973760 resource_reader.py:50] system_path_file_exists:t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.704306 140600233973760 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/examples/t5/t5_1_1/examples/openmoe_large_ul2.gin
I0512 01:44:53.723467 140139156215808 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.672329 139995375843328 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.672838 139995375843328 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.673144 139995375843328 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.687304 139995375843328 gin_utils.py:83] Gin Configuration:
I0512 01:44:53.703096 139995375843328 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:53.703172 139995375843328 gin_utils.py:85] import __main__ as train_script
I0512 01:44:53.703215 139995375843328 gin_utils.py:85] from flax import linen
I0512 01:44:53.703250 139995375843328 gin_utils.py:85] import flaxformer
I0512 01:44:53.703284 139995375843328 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:53.703317 139995375843328 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:53.703350 139995375843328 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:53.703382 139995375843328 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:53.703416 139995375843328 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:53.703450 139995375843328 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:53.703483 139995375843328 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:53.703516 139995375843328 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:53.703548 139995375843328 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:53.703581 139995375843328 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:53.703613 139995375843328 gin_utils.py:85] from gin import config
I0512 01:44:53.703645 139995375843328 gin_utils.py:85] import seqio
I0512 01:44:53.703678 139995375843328 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:53.703710 139995375843328 gin_utils.py:85] from t5x import adafactor
I0512 01:44:53.703742 139995375843328 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:53.703774 139995375843328 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:53.703807 139995375843328 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:53.703840 139995375843328 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:53.703872 139995375843328 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:53.703904 139995375843328 gin_utils.py:85] from t5x import partitioning
I0512 01:44:53.703936 139995375843328 gin_utils.py:85] from t5x import trainer
I0512 01:44:53.703969 139995375843328 gin_utils.py:85] from t5x import utils
I0512 01:44:53.704000 139995375843328 gin_utils.py:85] 
I0512 01:44:53.704033 139995375843328 gin_utils.py:85] # Macros:
I0512 01:44:53.704066 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.704098 139995375843328 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:53.704131 139995375843328 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:53.704169 139995375843328 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:53.704203 139995375843328 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:53.704236 139995375843328 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:53.704268 139995375843328 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:53.704321 139995375843328 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:53.704359 139995375843328 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:53.704393 139995375843328 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:53.704427 139995375843328 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:53.704461 139995375843328 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:53.704493 139995375843328 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:53.704526 139995375843328 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:53.704558 139995375843328 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:53.704591 139995375843328 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:53.704623 139995375843328 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:53.704655 139995375843328 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:53.704687 139995375843328 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:53.704719 139995375843328 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:53.704752 139995375843328 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:53.704784 139995375843328 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:53.704816 139995375843328 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:53.704848 139995375843328 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:53.704881 139995375843328 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:53.704913 139995375843328 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:53.704945 139995375843328 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:53.704976 139995375843328 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:53.705009 139995375843328 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:53.705041 139995375843328 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:53.705073 139995375843328 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:53.735610 139861676046336 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.705105 139995375843328 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:53.705137 139995375843328 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:53.705176 139995375843328 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:53.705209 139995375843328 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:53.705242 139995375843328 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:53.705274 139995375843328 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:53.705307 139995375843328 gin_utils.py:85] SCALE = 0.1
I0512 01:44:53.705339 139995375843328 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:53.705371 139995375843328 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:53.705404 139995375843328 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:53.705439 139995375843328 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:53.705472 139995375843328 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:53.705504 139995375843328 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:53.705536 139995375843328 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:53.705569 139995375843328 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:53.705601 139995375843328 gin_utils.py:85] 
I0512 01:44:53.705633 139995375843328 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:53.705665 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.705698 139995375843328 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:53.705730 139995375843328 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:53.705762 139995375843328 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:53.705794 139995375843328 gin_utils.py:85] 
I0512 01:44:53.743454 140497406441472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.743634 140497406441472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.743692 140497406441472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.743736 140497406441472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.744613 140497406441472 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.744747 140497406441472 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.744800 140497406441472 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.744840 140497406441472 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.738772 139933136287744 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.738959 139933136287744 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.739015 139933136287744 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.739064 139933136287744 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.739920 139933136287744 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.740049 139933136287744 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.740101 139933136287744 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.740140 139933136287744 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.789338 140497406441472 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.705826 139995375843328 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:53.705859 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.705891 139995375843328 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:53.705924 139995375843328 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:53.705956 139995375843328 gin_utils.py:85] 
I0512 01:44:53.705988 139995375843328 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:53.706020 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.706053 139995375843328 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:53.706085 139995375843328 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.706117 139995375843328 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:53.706150 139995375843328 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:53.706188 139995375843328 gin_utils.py:85] 
I0512 01:44:53.706221 139995375843328 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:53.706253 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.706286 139995375843328 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:53.706318 139995375843328 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:53.706351 139995375843328 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:53.706383 139995375843328 gin_utils.py:85] 
I0512 01:44:53.706416 139995375843328 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:53.706450 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.706483 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:53.706516 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.706548 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.706580 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:53.706612 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:53.706645 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:53.706677 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:53.706710 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.706742 139995375843328 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.706774 139995375843328 gin_utils.py:85] 
I0512 01:44:53.706806 139995375843328 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:53.706839 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.706871 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:53.706903 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.706935 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.706968 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:53.707000 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:53.707033 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:53.707065 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:53.707097 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.707129 139995375843328 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.707166 139995375843328 gin_utils.py:85] 
I0512 01:44:53.707200 139995375843328 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:53.707232 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.707265 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.707298 139995375843328 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.707330 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.707363 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.707395 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.707429 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:53.707462 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:53.707494 139995375843328 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:53.707527 139995375843328 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.707559 139995375843328 gin_utils.py:85] 
I0512 01:44:53.707591 139995375843328 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:53.707623 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.707656 139995375843328 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:53.707689 139995375843328 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.707721 139995375843328 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:53.707753 139995375843328 gin_utils.py:85] 
I0512 01:44:53.707786 139995375843328 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:53.707818 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.707851 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:53.707883 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:53.707916 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:53.707948 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:53.707980 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:53.708012 139995375843328 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.708045 139995375843328 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:53.708077 139995375843328 gin_utils.py:85] 
I0512 01:44:53.708109 139995375843328 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:53.708141 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.708179 139995375843328 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:53.708212 139995375843328 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:53.708245 139995375843328 gin_utils.py:85] 
I0512 01:44:53.708277 139995375843328 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:53.708335 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.708371 139995375843328 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:53.708404 139995375843328 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:53.708438 139995375843328 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.708471 139995375843328 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:53.708503 139995375843328 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:53.708536 139995375843328 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:53.708568 139995375843328 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:53.708600 139995375843328 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:53.708633 139995375843328 gin_utils.py:85] 
I0512 01:44:53.708665 139995375843328 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:53.708697 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.708730 139995375843328 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.708762 139995375843328 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.708794 139995375843328 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.708827 139995375843328 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:53.708859 139995375843328 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.708892 139995375843328 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:53.708925 139995375843328 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:53.723824 140139156215808 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.724328 140139156215808 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.724663 140139156215808 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.738808 140139156215808 gin_utils.py:83] Gin Configuration:
I0512 01:44:53.754999 140139156215808 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:53.755070 140139156215808 gin_utils.py:85] import __main__ as train_script
I0512 01:44:53.755110 140139156215808 gin_utils.py:85] from flax import linen
I0512 01:44:53.755146 140139156215808 gin_utils.py:85] import flaxformer
I0512 01:44:53.755178 140139156215808 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:53.755210 140139156215808 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:53.755241 140139156215808 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:53.755272 140139156215808 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:53.755302 140139156215808 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:53.755333 140139156215808 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:53.755363 140139156215808 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:53.755402 140139156215808 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:53.755433 140139156215808 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:53.755463 140139156215808 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:53.755494 140139156215808 gin_utils.py:85] from gin import config
I0512 01:44:53.755524 140139156215808 gin_utils.py:85] import seqio
I0512 01:44:53.755555 140139156215808 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:53.755585 140139156215808 gin_utils.py:85] from t5x import adafactor
I0512 01:44:53.755616 140139156215808 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:53.755646 140139156215808 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:53.755676 140139156215808 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:53.755706 140139156215808 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:53.755737 140139156215808 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:53.755767 140139156215808 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:53.755797 140139156215808 gin_utils.py:85] from t5x import trainer
I0512 01:44:53.755827 140139156215808 gin_utils.py:85] from t5x import utils
I0512 01:44:53.755857 140139156215808 gin_utils.py:85] 
I0512 01:44:53.755888 140139156215808 gin_utils.py:85] # Macros:
I0512 01:44:53.755919 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.755949 140139156215808 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:53.755980 140139156215808 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:53.756011 140139156215808 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:53.756041 140139156215808 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:53.756072 140139156215808 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:53.756102 140139156215808 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:53.756133 140139156215808 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:53.756166 140139156215808 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:53.756196 140139156215808 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:53.756227 140139156215808 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:53.756257 140139156215808 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:53.756287 140139156215808 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:53.756318 140139156215808 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:53.756348 140139156215808 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:53.756383 140139156215808 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:53.756415 140139156215808 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:53.756445 140139156215808 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:53.756476 140139156215808 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:53.756506 140139156215808 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:53.756536 140139156215808 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:53.756567 140139156215808 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:53.756597 140139156215808 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:53.756627 140139156215808 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:53.756658 140139156215808 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:53.756688 140139156215808 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:53.756718 140139156215808 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:53.756751 140139156215808 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:53.756784 140139156215808 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:53.756814 140139156215808 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:53.756844 140139156215808 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:53.756874 140139156215808 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:53.756905 140139156215808 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:53.756935 140139156215808 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:53.756965 140139156215808 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:53.756995 140139156215808 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:53.757042 140139156215808 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:53.757082 140139156215808 gin_utils.py:85] SCALE = 0.1
I0512 01:44:53.757114 140139156215808 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:53.757146 140139156215808 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:53.757178 140139156215808 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:53.757208 140139156215808 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:53.757239 140139156215808 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:53.757270 140139156215808 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:53.757300 140139156215808 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:53.757330 140139156215808 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:53.757360 140139156215808 gin_utils.py:85] 
I0512 01:44:53.757396 140139156215808 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:53.757427 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.757458 140139156215808 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:53.757489 140139156215808 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:53.757519 140139156215808 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:53.757550 140139156215808 gin_utils.py:85] 
I0512 01:44:53.735968 139861676046336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.736477 139861676046336 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.736784 139861676046336 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.750855 139861676046336 gin_utils.py:83] Gin Configuration:
I0512 01:44:53.766590 139861676046336 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:53.766661 139861676046336 gin_utils.py:85] import __main__ as train_script
I0512 01:44:53.766707 139861676046336 gin_utils.py:85] from flax import linen
I0512 01:44:53.766742 139861676046336 gin_utils.py:85] import flaxformer
I0512 01:44:53.766776 139861676046336 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:53.766809 139861676046336 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:53.766841 139861676046336 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:53.766873 139861676046336 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:53.766905 139861676046336 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:53.766937 139861676046336 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:53.766968 139861676046336 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:53.767000 139861676046336 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:53.767032 139861676046336 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:53.767064 139861676046336 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:53.767122 139861676046336 gin_utils.py:85] from gin import config
I0512 01:44:53.767158 139861676046336 gin_utils.py:85] import seqio
I0512 01:44:53.767191 139861676046336 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:53.767228 139861676046336 gin_utils.py:85] from t5x import adafactor
I0512 01:44:53.767261 139861676046336 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:53.767293 139861676046336 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:53.767325 139861676046336 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:53.767357 139861676046336 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:53.767389 139861676046336 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:53.767421 139861676046336 gin_utils.py:85] from t5x import partitioning
I0512 01:44:53.767453 139861676046336 gin_utils.py:85] from t5x import trainer
I0512 01:44:53.767487 139861676046336 gin_utils.py:85] from t5x import utils
I0512 01:44:53.767520 139861676046336 gin_utils.py:85] 
I0512 01:44:53.767552 139861676046336 gin_utils.py:85] # Macros:
I0512 01:44:53.767584 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.767616 139861676046336 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:53.767648 139861676046336 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:53.767680 139861676046336 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:53.767712 139861676046336 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:53.767744 139861676046336 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:53.767776 139861676046336 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:53.767808 139861676046336 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:53.767839 139861676046336 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:53.767871 139861676046336 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:53.767903 139861676046336 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:53.767935 139861676046336 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:53.767966 139861676046336 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:53.767998 139861676046336 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:53.768030 139861676046336 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:53.768062 139861676046336 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:53.768094 139861676046336 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:53.768125 139861676046336 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:53.768157 139861676046336 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:53.768188 139861676046336 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:53.768226 139861676046336 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:53.768259 139861676046336 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:53.768292 139861676046336 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:53.768323 139861676046336 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:53.768355 139861676046336 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:53.768387 139861676046336 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:53.768419 139861676046336 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:53.768451 139861676046336 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:53.768485 139861676046336 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:53.768518 139861676046336 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:53.768549 139861676046336 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:53.768581 139861676046336 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:53.768613 139861676046336 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:53.768645 139861676046336 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:53.768677 139861676046336 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:53.768708 139861676046336 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:53.768740 139861676046336 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:53.768772 139861676046336 gin_utils.py:85] SCALE = 0.1
I0512 01:44:53.768803 139861676046336 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:53.768835 139861676046336 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:53.768867 139861676046336 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:53.768899 139861676046336 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:53.768931 139861676046336 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:53.768962 139861676046336 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:53.768994 139861676046336 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:53.769026 139861676046336 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:53.769057 139861676046336 gin_utils.py:85] 
I0512 01:44:53.769089 139861676046336 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:53.769122 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.769154 139861676046336 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:53.769186 139861676046336 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:53.769223 139861676046336 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:53.769257 139861676046336 gin_utils.py:85] 
I0512 01:44:53.783626 139933136287744 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.708957 139995375843328 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.708989 139995375843328 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.709021 139995375843328 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:53.709054 139995375843328 gin_utils.py:85] 
I0512 01:44:53.709086 139995375843328 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:53.709119 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.709151 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:53.709190 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.709223 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.709255 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:53.709288 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.709321 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:53.709353 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.709385 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:53.709419 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:53.709453 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:53.709485 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:53.709518 139995375843328 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.709550 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.757580 140139156215808 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:53.757610 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.757639 140139156215808 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:53.757670 140139156215808 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:53.757700 140139156215808 gin_utils.py:85] 
I0512 01:44:53.757730 140139156215808 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:53.757761 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.757791 140139156215808 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:53.757822 140139156215808 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.757853 140139156215808 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:53.757883 140139156215808 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:53.757913 140139156215808 gin_utils.py:85] 
I0512 01:44:53.757943 140139156215808 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:53.757973 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.758004 140139156215808 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:53.758034 140139156215808 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:53.758065 140139156215808 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:53.758095 140139156215808 gin_utils.py:85] 
I0512 01:44:53.758126 140139156215808 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:53.758159 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.758189 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:53.758220 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.758250 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.758281 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:53.758311 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:53.758342 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:53.758376 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:53.758409 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.758439 140139156215808 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.758470 140139156215808 gin_utils.py:85] 
I0512 01:44:53.758500 140139156215808 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:53.758531 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.758561 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:53.758591 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.758621 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.758652 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:53.758682 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:53.758713 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:53.758743 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:53.758773 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.758803 140139156215808 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.758834 140139156215808 gin_utils.py:85] 
I0512 01:44:53.758864 140139156215808 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:53.758894 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.758924 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.758955 140139156215808 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.758985 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.759016 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.759046 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.759076 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:53.759106 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:53.759138 140139156215808 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:53.759170 140139156215808 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.759201 140139156215808 gin_utils.py:85] 
I0512 01:44:53.759231 140139156215808 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:53.759261 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.759291 140139156215808 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:53.759322 140139156215808 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.759352 140139156215808 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:53.759388 140139156215808 gin_utils.py:85] 
I0512 01:44:53.759419 140139156215808 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:53.759450 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.759480 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:53.759510 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:53.759541 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:53.759571 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:53.759602 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:53.759632 140139156215808 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.759662 140139156215808 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:53.759693 140139156215808 gin_utils.py:85] 
I0512 01:44:53.759723 140139156215808 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:53.759754 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.759784 140139156215808 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:53.759815 140139156215808 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:53.759845 140139156215808 gin_utils.py:85] 
I0512 01:44:53.759875 140139156215808 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:53.759905 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.759936 140139156215808 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:53.759966 140139156215808 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:53.759997 140139156215808 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.760027 140139156215808 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:53.760057 140139156215808 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:53.760087 140139156215808 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:53.760118 140139156215808 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:53.760150 140139156215808 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:53.760181 140139156215808 gin_utils.py:85] 
I0512 01:44:53.760211 140139156215808 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:53.760242 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.760273 140139156215808 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.760304 140139156215808 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.760334 140139156215808 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.760365 140139156215808 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:53.760401 140139156215808 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.760432 140139156215808 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:53.760462 140139156215808 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:53.789710 140497406441472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.790225 140497406441472 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.790548 140497406441472 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.804867 140497406441472 gin_utils.py:83] Gin Configuration:
I0512 01:44:53.820791 140497406441472 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:53.820863 140497406441472 gin_utils.py:85] import __main__ as train_script
I0512 01:44:53.820904 140497406441472 gin_utils.py:85] from flax import linen
I0512 01:44:53.820937 140497406441472 gin_utils.py:85] import flaxformer
I0512 01:44:53.820970 140497406441472 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:53.821003 140497406441472 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:53.821035 140497406441472 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:53.821067 140497406441472 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:53.821099 140497406441472 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:53.821130 140497406441472 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:53.821162 140497406441472 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:53.821194 140497406441472 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:53.821225 140497406441472 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:53.821265 140497406441472 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:53.821299 140497406441472 gin_utils.py:85] from gin import config
I0512 01:44:53.821331 140497406441472 gin_utils.py:85] import seqio
I0512 01:44:53.821363 140497406441472 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:53.821394 140497406441472 gin_utils.py:85] from t5x import adafactor
I0512 01:44:53.821426 140497406441472 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:53.821458 140497406441472 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:53.821490 140497406441472 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:53.821524 140497406441472 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:53.821557 140497406441472 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:53.821589 140497406441472 gin_utils.py:85] from t5x import partitioning
I0512 01:44:53.821621 140497406441472 gin_utils.py:85] from t5x import trainer
I0512 01:44:53.821653 140497406441472 gin_utils.py:85] from t5x import utils
I0512 01:44:53.821685 140497406441472 gin_utils.py:85] 
I0512 01:44:53.821717 140497406441472 gin_utils.py:85] # Macros:
I0512 01:44:53.821748 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.821780 140497406441472 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:53.821813 140497406441472 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:53.821844 140497406441472 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:53.821876 140497406441472 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:53.821907 140497406441472 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:53.821939 140497406441472 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:53.821970 140497406441472 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:53.822002 140497406441472 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:53.822034 140497406441472 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:53.822066 140497406441472 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:53.822097 140497406441472 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:53.822129 140497406441472 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:53.822160 140497406441472 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:53.822192 140497406441472 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:53.822223 140497406441472 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:53.822260 140497406441472 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:53.822293 140497406441472 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:53.822325 140497406441472 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:53.822357 140497406441472 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:53.822388 140497406441472 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:53.822420 140497406441472 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:53.822451 140497406441472 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:53.822483 140497406441472 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:53.822516 140497406441472 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:53.822549 140497406441472 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:53.822581 140497406441472 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:53.822615 140497406441472 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:53.822649 140497406441472 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:53.822680 140497406441472 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:53.822712 140497406441472 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:53.769289 139861676046336 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:53.769322 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.769354 139861676046336 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:53.769386 139861676046336 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:53.769418 139861676046336 gin_utils.py:85] 
I0512 01:44:53.769450 139861676046336 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:53.769484 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.769517 139861676046336 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:53.769549 139861676046336 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.769581 139861676046336 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:53.769613 139861676046336 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:53.769645 139861676046336 gin_utils.py:85] 
I0512 01:44:53.769677 139861676046336 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:53.769710 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.769742 139861676046336 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:53.769774 139861676046336 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:53.769806 139861676046336 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:53.769838 139861676046336 gin_utils.py:85] 
I0512 01:44:53.769869 139861676046336 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:53.822743 140497406441472 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:53.822775 140497406441472 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:53.822806 140497406441472 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:53.822838 140497406441472 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:53.822869 140497406441472 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:53.822901 140497406441472 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:53.822933 140497406441472 gin_utils.py:85] SCALE = 0.1
I0512 01:44:53.822964 140497406441472 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:53.822995 140497406441472 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:53.823027 140497406441472 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:53.823059 140497406441472 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:53.823110 140497406441472 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:53.823148 140497406441472 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:53.823180 140497406441472 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:53.823212 140497406441472 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:53.823244 140497406441472 gin_utils.py:85] 
I0512 01:44:53.823284 140497406441472 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:53.823317 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823348 140497406441472 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:53.823380 140497406441472 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:53.823412 140497406441472 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:53.823443 140497406441472 gin_utils.py:85] 
I0512 01:44:53.769901 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.769934 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:53.769966 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.769998 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.770030 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:53.770061 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:53.770094 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:53.770125 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:53.770157 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.770189 139861676046336 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.770226 139861676046336 gin_utils.py:85] 
I0512 01:44:53.770260 139861676046336 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:53.770292 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.770324 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:53.770356 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.770389 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.770421 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:53.770452 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:53.770486 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:53.770519 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:53.770551 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.770583 139861676046336 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.770615 139861676046336 gin_utils.py:85] 
I0512 01:44:53.770647 139861676046336 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:53.770679 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.770711 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.770743 139861676046336 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.770775 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.770807 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.770839 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.770870 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:53.770902 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:53.770934 139861676046336 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:53.770966 139861676046336 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.770998 139861676046336 gin_utils.py:85] 
I0512 01:44:53.771029 139861676046336 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:53.771061 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.771114 139861676046336 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:53.771151 139861676046336 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.771184 139861676046336 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:53.771216 139861676046336 gin_utils.py:85] 
I0512 01:44:53.771255 139861676046336 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:53.771287 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.771319 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:53.771351 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:53.771383 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:53.771414 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:53.771446 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:53.771480 139861676046336 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.771515 139861676046336 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:53.771547 139861676046336 gin_utils.py:85] 
I0512 01:44:53.771579 139861676046336 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:53.771611 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.771643 139861676046336 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:53.771675 139861676046336 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:53.771707 139861676046336 gin_utils.py:85] 
I0512 01:44:53.771739 139861676046336 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:53.771771 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.771803 139861676046336 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:53.771835 139861676046336 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:53.771867 139861676046336 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.771899 139861676046336 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:53.771931 139861676046336 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:53.771962 139861676046336 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:53.771994 139861676046336 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:53.772026 139861676046336 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:53.772058 139861676046336 gin_utils.py:85] 
I0512 01:44:53.772090 139861676046336 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:53.772122 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.772155 139861676046336 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.772187 139861676046336 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.772225 139861676046336 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.772258 139861676046336 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:53.772291 139861676046336 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.772323 139861676046336 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:53.772354 139861676046336 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:53.867583 139836023437312 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.868100 139836023437312 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.868163 139836023437312 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.868208 139836023437312 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.783982 139933136287744 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.784482 139933136287744 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.784814 139933136287744 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.798908 139933136287744 gin_utils.py:83] Gin Configuration:
I0512 01:44:53.814851 139933136287744 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:53.814921 139933136287744 gin_utils.py:85] import __main__ as train_script
I0512 01:44:53.814963 139933136287744 gin_utils.py:85] from flax import linen
I0512 01:44:53.814997 139933136287744 gin_utils.py:85] import flaxformer
I0512 01:44:53.815030 139933136287744 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:53.815062 139933136287744 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:53.815094 139933136287744 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:53.815126 139933136287744 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:53.815158 139933136287744 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:53.815189 139933136287744 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:53.815221 139933136287744 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:53.815253 139933136287744 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:53.815284 139933136287744 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:53.815315 139933136287744 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:53.815347 139933136287744 gin_utils.py:85] from gin import config
I0512 01:44:53.815378 139933136287744 gin_utils.py:85] import seqio
I0512 01:44:53.815410 139933136287744 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:53.815448 139933136287744 gin_utils.py:85] from t5x import adafactor
I0512 01:44:53.815483 139933136287744 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:53.815514 139933136287744 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:53.815546 139933136287744 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:53.815578 139933136287744 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:53.815609 139933136287744 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:53.815641 139933136287744 gin_utils.py:85] from t5x import partitioning
I0512 01:44:53.815672 139933136287744 gin_utils.py:85] from t5x import trainer
I0512 01:44:53.815706 139933136287744 gin_utils.py:85] from t5x import utils
I0512 01:44:53.815740 139933136287744 gin_utils.py:85] 
I0512 01:44:53.815778 139933136287744 gin_utils.py:85] # Macros:
I0512 01:44:53.815814 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.815847 139933136287744 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:53.815879 139933136287744 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:53.815910 139933136287744 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:53.815942 139933136287744 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:53.815973 139933136287744 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:53.816004 139933136287744 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:53.816035 139933136287744 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:53.816066 139933136287744 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:53.816097 139933136287744 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:53.816128 139933136287744 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:53.816159 139933136287744 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:53.816191 139933136287744 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:53.816222 139933136287744 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:53.816253 139933136287744 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:53.816285 139933136287744 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:53.816316 139933136287744 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:53.816347 139933136287744 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:53.816378 139933136287744 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:53.816409 139933136287744 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:53.816446 139933136287744 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:53.816479 139933136287744 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:53.816511 139933136287744 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:53.816561 139933136287744 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:53.816597 139933136287744 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:53.816630 139933136287744 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:53.816661 139933136287744 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:53.816694 139933136287744 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:53.816727 139933136287744 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:53.816759 139933136287744 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:53.816790 139933136287744 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:53.816821 139933136287744 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:53.816852 139933136287744 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:53.816884 139933136287744 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:53.816915 139933136287744 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:53.816946 139933136287744 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:53.816977 139933136287744 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:53.817008 139933136287744 gin_utils.py:85] SCALE = 0.1
I0512 01:44:53.817039 139933136287744 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:53.817071 139933136287744 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:53.817102 139933136287744 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:53.817133 139933136287744 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:53.817164 139933136287744 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:53.817196 139933136287744 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:53.817227 139933136287744 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:53.817258 139933136287744 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:53.817289 139933136287744 gin_utils.py:85] 
I0512 01:44:53.817321 139933136287744 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:53.817352 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.817383 139933136287744 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:53.817414 139933136287744 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:53.817453 139933136287744 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:53.817486 139933136287744 gin_utils.py:85] 
I0512 01:44:53.760493 140139156215808 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.760523 140139156215808 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.760554 140139156215808 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:53.760584 140139156215808 gin_utils.py:85] 
I0512 01:44:53.760614 140139156215808 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:53.760644 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.760674 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:53.760705 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.760735 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.760766 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:53.760797 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.760827 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:53.760857 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.760888 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:53.760918 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:53.760948 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:53.760978 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:53.761009 140139156215808 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.761062 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.709583 139995375843328 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:53.709615 139995375843328 gin_utils.py:85] 
I0512 01:44:53.709647 139995375843328 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:53.709679 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.709711 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:53.709744 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:53.709776 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:53.709808 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:53.709841 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:53.709874 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:53.709907 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:53.709939 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:53.709971 139995375843328 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:53.710004 139995375843328 gin_utils.py:85] 
I0512 01:44:53.710036 139995375843328 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:53.710068 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.710101 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.710133 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.710171 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:53.710205 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:53.710238 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.710271 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:53.710303 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:53.710335 139995375843328 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:53.710368 139995375843328 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.710401 139995375843328 gin_utils.py:85] 
I0512 01:44:53.710435 139995375843328 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:53.710469 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.710502 139995375843328 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:53.710534 139995375843328 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:53.710567 139995375843328 gin_utils.py:85] 
I0512 01:44:53.710599 139995375843328 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:53.710632 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.710664 139995375843328 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:53.710697 139995375843328 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.710730 139995375843328 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.710762 139995375843328 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:53.710795 139995375843328 gin_utils.py:85] 
I0512 01:44:53.710828 139995375843328 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:53.710860 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.710893 139995375843328 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.710925 139995375843328 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.710958 139995375843328 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:53.710990 139995375843328 gin_utils.py:85] 
I0512 01:44:53.711023 139995375843328 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:53.711055 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.711088 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:53.711121 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:53.711153 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:53.711192 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.711225 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:53.711258 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:53.711290 139995375843328 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.711323 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:53.711356 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:53.711389 139995375843328 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:53.711423 139995375843328 gin_utils.py:85] 
I0512 01:44:53.711457 139995375843328 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:53.711490 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.711523 139995375843328 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:53.711556 139995375843328 gin_utils.py:85] 
I0512 01:44:53.711589 139995375843328 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:53.711622 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.711654 139995375843328 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:53.711687 139995375843328 gin_utils.py:85] 
I0512 01:44:53.711719 139995375843328 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:53.711752 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.711785 139995375843328 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:53.711817 139995375843328 gin_utils.py:85] 
I0512 01:44:53.711850 139995375843328 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:53.711882 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.711915 139995375843328 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:53.711947 139995375843328 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:53.711981 139995375843328 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:53.712013 139995375843328 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:53.712046 139995375843328 gin_utils.py:85] 
I0512 01:44:53.712079 139995375843328 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:53.712111 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.712144 139995375843328 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:53.712182 139995375843328 gin_utils.py:85] 
I0512 01:44:53.712215 139995375843328 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:53.712248 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.712280 139995375843328 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:53.712336 139995375843328 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:53.712371 139995375843328 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:53.712404 139995375843328 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:53.712439 139995375843328 gin_utils.py:85] 
I0512 01:44:53.712472 139995375843328 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:53.712505 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.712538 139995375843328 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:53.712570 139995375843328 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:53.712603 139995375843328 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:53.712636 139995375843328 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:53.712669 139995375843328 gin_utils.py:85] 
I0512 01:44:53.712701 139995375843328 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:53.712733 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.712766 139995375843328 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:53.712799 139995375843328 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:53.712831 139995375843328 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:53.712864 139995375843328 gin_utils.py:85] 
I0512 01:44:53.712897 139995375843328 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:53.712929 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.712962 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.712994 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.713027 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:53.713060 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.713093 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:53.713125 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:53.713162 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:53.713197 139995375843328 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:53.713230 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:53.713263 139995375843328 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:53.713296 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:53.713329 139995375843328 gin_utils.py:85] 
I0512 01:44:53.713361 139995375843328 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:53.713394 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.713428 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.713462 139995375843328 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.713495 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.713528 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.713561 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:53.713594 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.713627 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:53.713660 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:53.713692 139995375843328 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:53.713725 139995375843328 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.713759 139995375843328 gin_utils.py:85] 
I0512 01:44:53.713792 139995375843328 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:53.713824 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.713857 139995375843328 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.713890 139995375843328 gin_utils.py:85] 
I0512 01:44:53.713922 139995375843328 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:53.713955 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.713987 139995375843328 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:53.714020 139995375843328 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:53.714053 139995375843328 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:53.714085 139995375843328 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:53.714118 139995375843328 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:53.714150 139995375843328 gin_utils.py:85] 
I0512 01:44:53.714189 139995375843328 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.714222 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.714254 139995375843328 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.714287 139995375843328 gin_utils.py:85] 
I0512 01:44:53.714319 139995375843328 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.714352 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.714384 139995375843328 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.714418 139995375843328 gin_utils.py:85] 
I0512 01:44:53.714453 139995375843328 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:53.714486 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.714519 139995375843328 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:53.772386 139861676046336 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.772418 139861676046336 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.772449 139861676046336 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:53.772483 139861676046336 gin_utils.py:85] 
I0512 01:44:53.772516 139861676046336 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:53.772549 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.772580 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:53.772612 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.772644 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.772678 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:53.772712 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.772744 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:53.772776 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.772808 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:53.772840 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:53.772872 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:53.772904 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:53.772936 139861676046336 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.772968 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.823475 140497406441472 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:53.823507 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823541 140497406441472 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:53.823573 140497406441472 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:53.823605 140497406441472 gin_utils.py:85] 
I0512 01:44:53.823636 140497406441472 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:53.823668 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823699 140497406441472 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:53.823731 140497406441472 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.823762 140497406441472 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:53.823794 140497406441472 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:53.823826 140497406441472 gin_utils.py:85] 
I0512 01:44:53.823857 140497406441472 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:53.823889 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823920 140497406441472 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:53.823952 140497406441472 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:53.823984 140497406441472 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:53.824015 140497406441472 gin_utils.py:85] 
I0512 01:44:53.824046 140497406441472 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:53.824078 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824109 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:53.824140 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.824172 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.824203 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:53.824235 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:53.824273 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:53.824306 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:53.824338 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.824369 140497406441472 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.824401 140497406441472 gin_utils.py:85] 
I0512 01:44:53.824433 140497406441472 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:53.824464 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824496 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:53.824530 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.824562 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.824594 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:53.824625 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:53.824657 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:53.824688 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:53.824719 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.824751 140497406441472 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.824783 140497406441472 gin_utils.py:85] 
I0512 01:44:53.824814 140497406441472 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:53.824846 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824877 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.824909 140497406441472 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.824941 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.824972 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.825003 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.825034 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:53.825066 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:53.825097 140497406441472 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:53.825129 140497406441472 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.825160 140497406441472 gin_utils.py:85] 
I0512 01:44:53.825192 140497406441472 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:53.825223 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825260 140497406441472 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:53.825293 140497406441472 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.825325 140497406441472 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:53.825356 140497406441472 gin_utils.py:85] 
I0512 01:44:53.825388 140497406441472 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:53.825419 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825450 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:53.825482 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:53.825515 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:53.825548 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:53.825580 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:53.825611 140497406441472 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.825643 140497406441472 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:53.825674 140497406441472 gin_utils.py:85] 
I0512 01:44:53.825706 140497406441472 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:53.825737 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825768 140497406441472 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:53.825800 140497406441472 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:53.825831 140497406441472 gin_utils.py:85] 
I0512 01:44:53.825862 140497406441472 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:53.825893 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825925 140497406441472 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:53.825956 140497406441472 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:53.825987 140497406441472 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.826019 140497406441472 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:53.826050 140497406441472 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:53.826081 140497406441472 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:53.826112 140497406441472 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:53.826144 140497406441472 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:53.826175 140497406441472 gin_utils.py:85] 
I0512 01:44:53.826206 140497406441472 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:53.826237 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826276 140497406441472 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.826308 140497406441472 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.826340 140497406441472 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.826371 140497406441472 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:53.826402 140497406441472 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.826433 140497406441472 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:53.826465 140497406441472 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:53.924626 139836023437312 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.924798 139836023437312 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.924857 139836023437312 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:53.924902 139836023437312 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.925799 139836023437312 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.925941 139836023437312 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.925996 139836023437312 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:53.926037 139836023437312 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.817517 139933136287744 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:53.817549 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.817581 139933136287744 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:53.817612 139933136287744 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:53.817644 139933136287744 gin_utils.py:85] 
I0512 01:44:53.817675 139933136287744 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:53.817709 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.817742 139933136287744 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:53.817774 139933136287744 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.817805 139933136287744 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:53.817836 139933136287744 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:53.817867 139933136287744 gin_utils.py:85] 
I0512 01:44:53.817899 139933136287744 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:53.817930 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.817961 139933136287744 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:53.817992 139933136287744 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:53.818024 139933136287744 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:53.818055 139933136287744 gin_utils.py:85] 
I0512 01:44:53.818086 139933136287744 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:53.818118 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.818149 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:53.818180 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.818212 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.818243 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:53.818274 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:53.818305 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:53.818337 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:53.818368 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.818399 139933136287744 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.818431 139933136287744 gin_utils.py:85] 
I0512 01:44:53.818470 139933136287744 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:53.818502 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.818533 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:53.818565 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:53.818596 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:53.818628 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:53.818659 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:53.818691 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:53.818724 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:53.818756 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:53.818787 139933136287744 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:53.818819 139933136287744 gin_utils.py:85] 
I0512 01:44:53.818850 139933136287744 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:53.818881 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.818913 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.818944 139933136287744 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.818975 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.819007 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.819038 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.819070 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:53.819101 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:53.819133 139933136287744 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:53.819164 139933136287744 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.819195 139933136287744 gin_utils.py:85] 
I0512 01:44:53.819226 139933136287744 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:53.819257 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.819288 139933136287744 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:53.819319 139933136287744 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.819351 139933136287744 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:53.819382 139933136287744 gin_utils.py:85] 
I0512 01:44:53.819413 139933136287744 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:53.819451 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.819484 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:53.819515 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:53.819547 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:53.819578 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:53.819609 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:53.819640 139933136287744 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.819671 139933136287744 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:53.819705 139933136287744 gin_utils.py:85] 
I0512 01:44:53.819737 139933136287744 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:53.819768 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.819800 139933136287744 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:53.819831 139933136287744 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:53.819862 139933136287744 gin_utils.py:85] 
I0512 01:44:53.819894 139933136287744 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:53.819925 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.819957 139933136287744 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:53.819988 139933136287744 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:53.820020 139933136287744 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.820051 139933136287744 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:53.820082 139933136287744 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:53.820113 139933136287744 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:53.820144 139933136287744 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:53.820176 139933136287744 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:53.820207 139933136287744 gin_utils.py:85] 
I0512 01:44:53.820238 139933136287744 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:53.820270 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.820302 139933136287744 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.820333 139933136287744 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.820365 139933136287744 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.820396 139933136287744 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:53.820427 139933136287744 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.820466 139933136287744 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:53.820498 139933136287744 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:53.826497 140497406441472 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.826530 140497406441472 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.826563 140497406441472 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:53.826594 140497406441472 gin_utils.py:85] 
I0512 01:44:53.826626 140497406441472 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:53.826658 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826689 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:53.826721 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.826752 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.826784 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:53.826815 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.826846 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:53.826878 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.826910 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:53.826941 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:53.826973 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:53.827004 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:53.827036 140497406441472 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.827067 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.761097 140139156215808 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:53.761130 140139156215808 gin_utils.py:85] 
I0512 01:44:53.761162 140139156215808 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:53.761193 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.761223 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:53.761254 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:53.761284 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:53.761314 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:53.761345 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:53.761381 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:53.761413 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:53.761444 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:53.761475 140139156215808 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:53.761506 140139156215808 gin_utils.py:85] 
I0512 01:44:53.761537 140139156215808 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:53.761567 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.761598 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.761628 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.761659 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:53.761689 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:53.761720 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.761750 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:53.761781 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:53.761811 140139156215808 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:53.761842 140139156215808 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.761873 140139156215808 gin_utils.py:85] 
I0512 01:44:53.761903 140139156215808 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:53.761934 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.761964 140139156215808 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:53.761995 140139156215808 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:53.762025 140139156215808 gin_utils.py:85] 
I0512 01:44:53.762055 140139156215808 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:53.762086 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.762116 140139156215808 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.762149 140139156215808 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.762180 140139156215808 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:53.762211 140139156215808 gin_utils.py:85] 
I0512 01:44:53.762241 140139156215808 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:53.762272 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.762302 140139156215808 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.762333 140139156215808 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.762364 140139156215808 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:53.762400 140139156215808 gin_utils.py:85] 
I0512 01:44:53.762431 140139156215808 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:53.762462 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.762492 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:53.762523 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:53.762554 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:53.762584 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.762615 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:53.762645 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:53.762676 140139156215808 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.762706 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:53.762736 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:53.762767 140139156215808 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:53.762797 140139156215808 gin_utils.py:85] 
I0512 01:44:53.762827 140139156215808 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:53.762858 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.762888 140139156215808 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:53.762919 140139156215808 gin_utils.py:85] 
I0512 01:44:53.762951 140139156215808 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:53.762982 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763013 140139156215808 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:53.763044 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763074 140139156215808 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:53.763104 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763136 140139156215808 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:53.763168 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763199 140139156215808 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:53.763229 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763260 140139156215808 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:53.763290 140139156215808 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:53.763321 140139156215808 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:53.763351 140139156215808 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:53.763386 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763418 140139156215808 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:53.763448 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763478 140139156215808 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:53.763509 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763539 140139156215808 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:53.763569 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763600 140139156215808 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:53.763630 140139156215808 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:53.763660 140139156215808 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:53.763691 140139156215808 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:53.763721 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763751 140139156215808 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:53.763781 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.763812 140139156215808 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:53.763842 140139156215808 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:53.763872 140139156215808 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:53.763902 140139156215808 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:53.763932 140139156215808 gin_utils.py:85] 
I0512 01:44:53.763962 140139156215808 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:53.763993 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.764023 140139156215808 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:53.764053 140139156215808 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:53.764083 140139156215808 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:53.764114 140139156215808 gin_utils.py:85] 
I0512 01:44:53.764146 140139156215808 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:53.764177 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.764208 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.764238 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.764268 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:53.764299 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.764329 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:53.764359 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:53.764395 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:53.764448 140139156215808 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:53.764480 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:53.764510 140139156215808 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:53.764540 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:53.764571 140139156215808 gin_utils.py:85] 
I0512 01:44:53.764602 140139156215808 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:53.764632 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.764662 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.764693 140139156215808 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.764723 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.764753 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.764784 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:53.764814 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.764845 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:53.764876 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:53.764907 140139156215808 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:53.764938 140139156215808 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.764968 140139156215808 gin_utils.py:85] 
I0512 01:44:53.765001 140139156215808 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:53.765048 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.765085 140139156215808 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.765117 140139156215808 gin_utils.py:85] 
I0512 01:44:53.765150 140139156215808 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:53.765182 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.765213 140139156215808 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:53.765244 140139156215808 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:53.765274 140139156215808 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:53.765305 140139156215808 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:53.765336 140139156215808 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:53.765367 140139156215808 gin_utils.py:85] 
I0512 01:44:53.765403 140139156215808 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.765435 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.765466 140139156215808 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.765496 140139156215808 gin_utils.py:85] 
I0512 01:44:53.765527 140139156215808 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.765558 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.765589 140139156215808 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.765619 140139156215808 gin_utils.py:85] 
I0512 01:44:53.765650 140139156215808 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:53.765682 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.765713 140139156215808 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:53.765744 140139156215808 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:53.952986 139867290535936 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.953501 139867290535936 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.953564 139867290535936 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.772999 139861676046336 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:53.773031 139861676046336 gin_utils.py:85] 
I0512 01:44:53.773063 139861676046336 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:53.773095 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.773127 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:53.773159 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:53.773191 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:53.773229 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:53.773262 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:53.773295 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:53.773327 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:53.773359 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:53.773391 139861676046336 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:53.773423 139861676046336 gin_utils.py:85] 
I0512 01:44:53.773455 139861676046336 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:53.773489 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.773522 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.773554 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.773586 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:53.773618 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:53.773650 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.773682 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:53.773715 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:53.773746 139861676046336 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:53.773778 139861676046336 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.773810 139861676046336 gin_utils.py:85] 
I0512 01:44:53.773842 139861676046336 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:53.773874 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.773906 139861676046336 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:53.773938 139861676046336 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:53.773970 139861676046336 gin_utils.py:85] 
I0512 01:44:53.774002 139861676046336 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:53.774034 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.774066 139861676046336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:53.774097 139861676046336 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.774129 139861676046336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.774161 139861676046336 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:53.774194 139861676046336 gin_utils.py:85] 
I0512 01:44:53.774231 139861676046336 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:53.774265 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.774296 139861676046336 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.774328 139861676046336 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.774360 139861676046336 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:53.774392 139861676046336 gin_utils.py:85] 
I0512 01:44:53.774424 139861676046336 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:53.774456 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.774489 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:53.774522 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:53.774555 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:53.774587 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.774619 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:53.774651 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:53.774683 139861676046336 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.774714 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:53.774746 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:53.774778 139861676046336 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:53.774810 139861676046336 gin_utils.py:85] 
I0512 01:44:53.774842 139861676046336 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:53.774874 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.774906 139861676046336 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:53.774939 139861676046336 gin_utils.py:85] 
I0512 01:44:53.774971 139861676046336 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:53.775003 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775035 139861676046336 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:53.775066 139861676046336 gin_utils.py:85] 
I0512 01:44:53.775119 139861676046336 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:53.775154 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775186 139861676046336 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:53.775222 139861676046336 gin_utils.py:85] 
I0512 01:44:53.775256 139861676046336 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:53.775288 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775320 139861676046336 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:53.775352 139861676046336 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:53.775384 139861676046336 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:53.775416 139861676046336 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:53.775447 139861676046336 gin_utils.py:85] 
I0512 01:44:53.775480 139861676046336 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:53.775514 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775546 139861676046336 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:53.775578 139861676046336 gin_utils.py:85] 
I0512 01:44:53.775610 139861676046336 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:53.775642 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775673 139861676046336 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:53.775705 139861676046336 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:53.775737 139861676046336 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:53.775768 139861676046336 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:53.775800 139861676046336 gin_utils.py:85] 
I0512 01:44:53.775832 139861676046336 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:53.775864 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.775895 139861676046336 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:53.775927 139861676046336 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:53.775959 139861676046336 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:53.775991 139861676046336 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:53.776022 139861676046336 gin_utils.py:85] 
I0512 01:44:53.776054 139861676046336 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:53.776086 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.776118 139861676046336 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:53.776149 139861676046336 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:53.776181 139861676046336 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:53.776213 139861676046336 gin_utils.py:85] 
I0512 01:44:53.776251 139861676046336 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:53.776283 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.776315 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.776347 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.776379 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:53.776411 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.776442 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:53.776475 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:53.776508 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:53.776540 139861676046336 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:53.776572 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:53.776604 139861676046336 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:53.776635 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
E0512 01:44:53.953610 139867290535936 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.776668 139861676046336 gin_utils.py:85] 
I0512 01:44:53.776703 139861676046336 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:53.776735 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.776767 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.776799 139861676046336 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.776830 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.776863 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.776895 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:53.776927 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.776959 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:53.776991 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:53.777023 139861676046336 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:53.777054 139861676046336 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.777086 139861676046336 gin_utils.py:85] 
I0512 01:44:53.777118 139861676046336 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:53.777150 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.777182 139861676046336 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.777214 139861676046336 gin_utils.py:85] 
I0512 01:44:53.777252 139861676046336 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:53.777285 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.777317 139861676046336 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:53.777348 139861676046336 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:53.777380 139861676046336 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:53.777412 139861676046336 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:53.777443 139861676046336 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:53.777476 139861676046336 gin_utils.py:85] 
I0512 01:44:53.777509 139861676046336 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.777541 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.777573 139861676046336 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.777605 139861676046336 gin_utils.py:85] 
I0512 01:44:53.777637 139861676046336 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.777668 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.777700 139861676046336 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.777732 139861676046336 gin_utils.py:85] 
I0512 01:44:53.777764 139861676046336 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:53.777797 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.777828 139861676046336 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:53.959381 140600233973760 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.959868 140600233973760 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.959933 140600233973760 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
E0512 01:44:53.959985 140600233973760 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/st_moe_decoder_only_large.gin
I0512 01:44:53.970409 139836023437312 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.820529 139933136287744 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.820588 139933136287744 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.820621 139933136287744 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:53.820653 139933136287744 gin_utils.py:85] 
I0512 01:44:53.820685 139933136287744 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:53.820718 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.820750 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:53.820782 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:53.820813 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:53.820844 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:53.820876 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.820911 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:53.820943 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:53.820975 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:53.821006 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:53.821038 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:53.821069 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:53.821100 139933136287744 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.821132 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.827119 140497406441472 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:53.827154 140497406441472 gin_utils.py:85] 
I0512 01:44:53.827186 140497406441472 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:53.827217 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.827249 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:53.827287 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:53.827319 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:53.827350 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:53.827381 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:53.827413 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:53.827444 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:53.827476 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:53.827508 140497406441472 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:53.827542 140497406441472 gin_utils.py:85] 
I0512 01:44:53.827573 140497406441472 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:53.827605 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.827636 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.827667 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.827698 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:53.827729 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:53.827760 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.827791 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:53.827822 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:53.827853 140497406441472 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:53.827884 140497406441472 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.827916 140497406441472 gin_utils.py:85] 
I0512 01:44:53.827946 140497406441472 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:53.827977 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.828009 140497406441472 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:53.828040 140497406441472 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:53.828071 140497406441472 gin_utils.py:85] 
I0512 01:44:53.828102 140497406441472 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:53.828133 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.828164 140497406441472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:53.828195 140497406441472 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.828226 140497406441472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.828263 140497406441472 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:53.828296 140497406441472 gin_utils.py:85] 
I0512 01:44:53.828327 140497406441472 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:53.828359 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.828390 140497406441472 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.828421 140497406441472 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.828453 140497406441472 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:53.828484 140497406441472 gin_utils.py:85] 
I0512 01:44:53.828517 140497406441472 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:53.828549 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.828581 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:53.828613 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:53.828644 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:53.828675 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.828707 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:53.828738 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:53.828769 140497406441472 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.828801 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:53.828832 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:53.828864 140497406441472 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:53.828895 140497406441472 gin_utils.py:85] 
I0512 01:44:53.828926 140497406441472 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:53.828957 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.828988 140497406441472 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:53.829021 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829052 140497406441472 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:53.829084 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829115 140497406441472 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:53.829146 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829177 140497406441472 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:53.829209 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829240 140497406441472 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:53.829277 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829308 140497406441472 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:53.829339 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829370 140497406441472 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:53.829401 140497406441472 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:53.829432 140497406441472 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:54.010713 139867290535936 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:54.010890 139867290535936 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:54.010948 139867290535936 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:53.829463 140497406441472 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:53.829494 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829527 140497406441472 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:53.829560 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829591 140497406441472 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:53.829622 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829653 140497406441472 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:53.829684 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829715 140497406441472 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:53.829746 140497406441472 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:53.829777 140497406441472 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:53.829808 140497406441472 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:53.829839 140497406441472 gin_utils.py:85] 
I0512 01:44:53.829870 140497406441472 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:53.829902 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.829932 140497406441472 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:53.829963 140497406441472 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:53.829994 140497406441472 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:53.830025 140497406441472 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:53.830056 140497406441472 gin_utils.py:85] 
I0512 01:44:53.830087 140497406441472 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:53.830118 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.830149 140497406441472 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:53.830180 140497406441472 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:53.830211 140497406441472 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:53.830242 140497406441472 gin_utils.py:85] 
I0512 01:44:53.830279 140497406441472 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:53.830311 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.830342 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.830374 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.830405 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:53.830436 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.830467 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:53.830498 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:53.830532 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:53.830563 140497406441472 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:53.830595 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:53.830626 140497406441472 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:53.830657 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:53.830688 140497406441472 gin_utils.py:85] 
I0512 01:44:53.830719 140497406441472 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:53.830750 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.830781 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.830812 140497406441472 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.830843 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.830875 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.830907 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:53.830938 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.830970 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:53.831001 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:53.831032 140497406441472 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:53.831062 140497406441472 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.831118 140497406441472 gin_utils.py:85] 
I0512 01:44:53.831154 140497406441472 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:53.831186 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.831217 140497406441472 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.831248 140497406441472 gin_utils.py:85] 
I0512 01:44:53.831285 140497406441472 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:53.831316 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.831347 140497406441472 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:53.831379 140497406441472 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:53.831410 140497406441472 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:53.831441 140497406441472 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:53.831473 140497406441472 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:53.831504 140497406441472 gin_utils.py:85] 
I0512 01:44:53.831537 140497406441472 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.831569 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.831600 140497406441472 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.831631 140497406441472 gin_utils.py:85] 
I0512 01:44:53.831662 140497406441472 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.831692 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.831723 140497406441472 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.831755 140497406441472 gin_utils.py:85] 
I0512 01:44:53.831787 140497406441472 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:53.831819 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.831850 140497406441472 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
E0512 01:44:54.010992 139867290535936 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:54.011881 139867290535936 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:54.012017 139867290535936 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:54.012069 139867290535936 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:54.012110 139867290535936 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:54.016429 140600233973760 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:54.016604 140600233973760 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:54.016670 140600233973760 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
E0512 01:44:54.016716 140600233973760 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/models/tokens_choose_decoder_only_large.gin
I0512 01:44:54.017597 140600233973760 resource_reader.py:50] system_path_file_exists:flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:54.017740 140600233973760 resource_reader.py:55] Path not found: flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:54.017795 140600233973760 resource_reader.py:50] system_path_file_exists:./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
E0512 01:44:54.017837 140600233973760 resource_reader.py:55] Path not found: ./flaxformer/flaxformer/t5x/configs/moe/architectures/moe_decoder_only.gin
I0512 01:44:53.970778 139836023437312 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.971295 139836023437312 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:53.971608 139836023437312 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:53.986006 139836023437312 gin_utils.py:83] Gin Configuration:
I0512 01:44:54.001954 139836023437312 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:54.002026 139836023437312 gin_utils.py:85] import __main__ as train_script
I0512 01:44:54.002069 139836023437312 gin_utils.py:85] from flax import linen
I0512 01:44:54.002105 139836023437312 gin_utils.py:85] import flaxformer
I0512 01:44:54.002138 139836023437312 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:54.002172 139836023437312 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:54.002205 139836023437312 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:54.002238 139836023437312 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:54.002270 139836023437312 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:54.002302 139836023437312 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:54.002335 139836023437312 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:54.002368 139836023437312 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:54.002400 139836023437312 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:54.002433 139836023437312 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:54.002465 139836023437312 gin_utils.py:85] from gin import config
I0512 01:44:54.002498 139836023437312 gin_utils.py:85] import seqio
I0512 01:44:54.002530 139836023437312 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:54.002562 139836023437312 gin_utils.py:85] from t5x import adafactor
I0512 01:44:54.002594 139836023437312 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:54.002627 139836023437312 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:54.002660 139836023437312 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:54.002695 139836023437312 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:54.002728 139836023437312 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:54.002761 139836023437312 gin_utils.py:85] from t5x import partitioning
I0512 01:44:54.002794 139836023437312 gin_utils.py:85] from t5x import trainer
I0512 01:44:54.002825 139836023437312 gin_utils.py:85] from t5x import utils
I0512 01:44:54.002858 139836023437312 gin_utils.py:85] 
I0512 01:44:54.002891 139836023437312 gin_utils.py:85] # Macros:
I0512 01:44:54.002930 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.002964 139836023437312 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:54.002997 139836023437312 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:54.003030 139836023437312 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:54.003062 139836023437312 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:54.003095 139836023437312 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:54.003127 139836023437312 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:54.003159 139836023437312 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:54.003191 139836023437312 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:54.003224 139836023437312 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:54.003256 139836023437312 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:54.003288 139836023437312 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:54.003320 139836023437312 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:54.003352 139836023437312 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:54.003385 139836023437312 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:54.003417 139836023437312 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:54.003449 139836023437312 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:54.003481 139836023437312 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:54.003514 139836023437312 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:54.003546 139836023437312 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:54.003578 139836023437312 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:54.003610 139836023437312 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:54.003642 139836023437312 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:54.003677 139836023437312 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:54.003710 139836023437312 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:54.003742 139836023437312 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:54.003775 139836023437312 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:54.003807 139836023437312 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:54.003839 139836023437312 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:54.003871 139836023437312 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:54.003904 139836023437312 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:54.003945 139836023437312 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:54.003978 139836023437312 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:54.004010 139836023437312 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:54.004043 139836023437312 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:54.004075 139836023437312 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:54.004107 139836023437312 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:54.004139 139836023437312 gin_utils.py:85] SCALE = 0.1
I0512 01:44:54.004171 139836023437312 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:54.004204 139836023437312 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:54.004236 139836023437312 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:54.004269 139836023437312 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:54.004301 139836023437312 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:54.004333 139836023437312 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:54.004366 139836023437312 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:54.004398 139836023437312 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:54.004430 139836023437312 gin_utils.py:85] 
I0512 01:44:54.004463 139836023437312 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:54.004495 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.004528 139836023437312 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:54.004560 139836023437312 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:54.004593 139836023437312 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:54.004625 139836023437312 gin_utils.py:85] 
I0512 01:44:54.057508 139867290535936 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.777860 139861676046336 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:53.777892 139861676046336 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:53.777923 139861676046336 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:53.777955 139861676046336 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:53.777986 139861676046336 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:53.778017 139861676046336 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:53.778049 139861676046336 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:53.778080 139861676046336 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:53.778112 139861676046336 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:53.778143 139861676046336 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:53.778175 139861676046336 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:53.778206 139861676046336 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:53.778244 139861676046336 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:53.778277 139861676046336 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:53.778308 139861676046336 gin_utils.py:85] 
I0512 01:44:53.778340 139861676046336 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:53.778372 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.778403 139861676046336 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.778435 139861676046336 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:53.778466 139861676046336 gin_utils.py:85] 
I0512 01:44:53.778501 139861676046336 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.778533 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.778565 139861676046336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.778597 139861676046336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.778628 139861676046336 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.778660 139861676046336 gin_utils.py:85] 
I0512 01:44:53.778692 139861676046336 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.778724 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.778755 139861676046336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.778787 139861676046336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.778818 139861676046336 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.778850 139861676046336 gin_utils.py:85] 
I0512 01:44:53.778881 139861676046336 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.778913 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.778944 139861676046336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.778976 139861676046336 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.779007 139861676046336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.779039 139861676046336 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.779071 139861676046336 gin_utils.py:85] 
I0512 01:44:53.779125 139861676046336 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.779160 139861676046336 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.779191 139861676046336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.779228 139861676046336 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.779261 139861676046336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.779294 139861676046336 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.780204 139861676046336 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478293.841433  264564 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478293.841482  264564 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478293.841484  264564 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:53.821164 139933136287744 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:53.821195 139933136287744 gin_utils.py:85] 
I0512 01:44:53.821226 139933136287744 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:53.821257 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.821289 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:53.821320 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:53.821351 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:53.821383 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:53.821414 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:53.821452 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:53.821485 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:53.821517 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:53.821548 139933136287744 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:53.821579 139933136287744 gin_utils.py:85] 
I0512 01:44:53.821611 139933136287744 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:53.821642 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.821673 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:53.821706 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.821738 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:53.821769 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:53.821800 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.821832 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:53.821863 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:53.821894 139933136287744 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:53.821925 139933136287744 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:53.821956 139933136287744 gin_utils.py:85] 
I0512 01:44:53.821988 139933136287744 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:53.822019 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.822050 139933136287744 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:53.822081 139933136287744 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:53.822112 139933136287744 gin_utils.py:85] 
I0512 01:44:53.822143 139933136287744 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:53.822174 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.822205 139933136287744 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:53.822237 139933136287744 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:53.822268 139933136287744 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.822299 139933136287744 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:53.822331 139933136287744 gin_utils.py:85] 
I0512 01:44:53.822362 139933136287744 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:53.822393 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.822425 139933136287744 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.822463 139933136287744 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:53.822495 139933136287744 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:53.822526 139933136287744 gin_utils.py:85] 
I0512 01:44:53.822558 139933136287744 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:53.822589 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.822620 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:53.822651 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:53.822683 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:53.822716 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.822748 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:53.822779 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:53.822811 139933136287744 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:53.822842 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:53.822873 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:54.062185 140600233973760 resource_reader.py:50] system_path_file_exists:t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:53.822904 139933136287744 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:53.822936 139933136287744 gin_utils.py:85] 
I0512 01:44:53.822967 139933136287744 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:53.822998 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823029 139933136287744 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:53.823061 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823093 139933136287744 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:53.823124 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823155 139933136287744 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:53.823186 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823217 139933136287744 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:53.823248 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823279 139933136287744 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:53.823310 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823341 139933136287744 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:53.823371 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823402 139933136287744 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:53.823433 139933136287744 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:53.823471 139933136287744 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:53.823502 139933136287744 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:53.823533 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823564 139933136287744 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:53.823595 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823626 139933136287744 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:53.823657 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823689 139933136287744 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:53.823721 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823753 139933136287744 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:53.823784 139933136287744 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:53.823814 139933136287744 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:53.823845 139933136287744 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:53.823876 139933136287744 gin_utils.py:85] 
I0512 01:44:53.823907 139933136287744 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:53.823938 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.823968 139933136287744 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:53.823999 139933136287744 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:53.824030 139933136287744 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:53.824061 139933136287744 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:53.824091 139933136287744 gin_utils.py:85] 
I0512 01:44:53.824122 139933136287744 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:53.824153 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824184 139933136287744 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:53.824215 139933136287744 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:53.824245 139933136287744 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:53.824276 139933136287744 gin_utils.py:85] 
I0512 01:44:53.824307 139933136287744 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:53.824338 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824369 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.824399 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.824430 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:53.824468 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.824499 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:53.824551 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:53.824589 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:53.824621 139933136287744 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:53.824652 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:53.824683 139933136287744 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:53.824716 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:53.831882 140497406441472 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:53.831913 140497406441472 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:53.831944 140497406441472 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:53.831976 140497406441472 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:53.832007 140497406441472 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:53.832038 140497406441472 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:53.832069 140497406441472 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:53.832100 140497406441472 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:53.832132 140497406441472 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:53.832163 140497406441472 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:53.832194 140497406441472 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:53.832225 140497406441472 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:53.832261 140497406441472 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:53.832294 140497406441472 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:53.832325 140497406441472 gin_utils.py:85] 
I0512 01:44:53.832377 140497406441472 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:53.832409 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.832441 140497406441472 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.832472 140497406441472 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:53.832504 140497406441472 gin_utils.py:85] 
I0512 01:44:53.832538 140497406441472 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.832569 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.832601 140497406441472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.832637 140497406441472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.832669 140497406441472 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.832700 140497406441472 gin_utils.py:85] 
I0512 01:44:53.832730 140497406441472 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.832761 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.832792 140497406441472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.832823 140497406441472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.832855 140497406441472 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.832886 140497406441472 gin_utils.py:85] 
I0512 01:44:53.832917 140497406441472 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.832949 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.832980 140497406441472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.833011 140497406441472 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.833043 140497406441472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.833074 140497406441472 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.833105 140497406441472 gin_utils.py:85] 
I0512 01:44:53.833136 140497406441472 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.833167 140497406441472 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.833198 140497406441472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.833230 140497406441472 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.833267 140497406441472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.833299 140497406441472 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.834213 140497406441472 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478293.881680  264715 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478293.881741  264715 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478293.881744  264715 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:53.824748 139933136287744 gin_utils.py:85] 
I0512 01:44:53.824779 139933136287744 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:53.824810 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.824841 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:53.824872 139933136287744 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:53.824902 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:53.824934 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:53.824965 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:53.824996 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:53.825027 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:53.825058 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:53.825089 139933136287744 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:53.825119 139933136287744 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:53.825151 139933136287744 gin_utils.py:85] 
I0512 01:44:53.825182 139933136287744 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:53.825214 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825245 139933136287744 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:53.825276 139933136287744 gin_utils.py:85] 
I0512 01:44:53.825307 139933136287744 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:53.825338 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825369 139933136287744 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:53.825400 139933136287744 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:53.825432 139933136287744 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:53.825469 139933136287744 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:53.825501 139933136287744 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:53.825532 139933136287744 gin_utils.py:85] 
I0512 01:44:53.825563 139933136287744 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.825594 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825625 139933136287744 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.825656 139933136287744 gin_utils.py:85] 
I0512 01:44:53.825687 139933136287744 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:53.825721 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825752 139933136287744 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:53.825783 139933136287744 gin_utils.py:85] 
I0512 01:44:53.825815 139933136287744 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:53.825847 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.825878 139933136287744 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:54.004657 139836023437312 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:54.004692 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.004725 139836023437312 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:54.004757 139836023437312 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:54.004789 139836023437312 gin_utils.py:85] 
I0512 01:44:54.004821 139836023437312 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:54.004853 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.004885 139836023437312 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:54.004923 139836023437312 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.004957 139836023437312 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:54.004990 139836023437312 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:54.005022 139836023437312 gin_utils.py:85] 
I0512 01:44:54.005054 139836023437312 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:54.005086 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.005119 139836023437312 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:54.005151 139836023437312 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:54.005183 139836023437312 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:54.005215 139836023437312 gin_utils.py:85] 
I0512 01:44:54.005247 139836023437312 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:54.005279 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.005311 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:54.005343 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.005376 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.005408 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:54.005440 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:54.005472 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:54.005504 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:54.005537 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.005568 139836023437312 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.005601 139836023437312 gin_utils.py:85] 
I0512 01:44:54.005633 139836023437312 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:54.005666 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.005718 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:54.005758 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.005792 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.005825 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:54.005857 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:54.005889 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:54.005927 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:54.005960 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.005992 139836023437312 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.006024 139836023437312 gin_utils.py:85] 
I0512 01:44:54.006057 139836023437312 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:54.006088 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.006121 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.006153 139836023437312 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.006186 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.006218 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.006249 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.006282 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:54.006314 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:54.006346 139836023437312 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:54.006378 139836023437312 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.006410 139836023437312 gin_utils.py:85] 
I0512 01:44:54.006442 139836023437312 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:54.006475 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.006507 139836023437312 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:54.006539 139836023437312 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.006571 139836023437312 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:54.006603 139836023437312 gin_utils.py:85] 
I0512 01:44:54.006635 139836023437312 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:54.006669 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.006703 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:54.006735 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:54.006768 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:54.006800 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:54.006832 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:54.006864 139836023437312 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.006896 139836023437312 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:54.006933 139836023437312 gin_utils.py:85] 
I0512 01:44:54.006967 139836023437312 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:54.006999 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.007031 139836023437312 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:54.007063 139836023437312 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:54.007095 139836023437312 gin_utils.py:85] 
I0512 01:44:54.007127 139836023437312 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:54.007159 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.007191 139836023437312 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:54.007223 139836023437312 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:54.007255 139836023437312 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.007286 139836023437312 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:54.007318 139836023437312 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:54.007351 139836023437312 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:54.007383 139836023437312 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:54.007415 139836023437312 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:54.007447 139836023437312 gin_utils.py:85] 
I0512 01:44:54.007480 139836023437312 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:54.007512 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.007544 139836023437312 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.007576 139836023437312 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.007609 139836023437312 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.007641 139836023437312 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:54.007675 139836023437312 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.007709 139836023437312 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:54.007741 139836023437312 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:54.057876 139867290535936 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:54.058407 139867290535936 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:54.058753 139867290535936 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:54.073201 139867290535936 gin_utils.py:83] Gin Configuration:
I0512 01:44:54.089381 139867290535936 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:54.089451 139867290535936 gin_utils.py:85] import __main__ as train_script
I0512 01:44:54.089501 139867290535936 gin_utils.py:85] from flax import linen
I0512 01:44:54.089536 139867290535936 gin_utils.py:85] import flaxformer
I0512 01:44:54.089570 139867290535936 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:54.089603 139867290535936 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:54.089636 139867290535936 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:54.089668 139867290535936 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:54.089700 139867290535936 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:54.089732 139867290535936 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:54.089764 139867290535936 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:54.089795 139867290535936 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:54.089827 139867290535936 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:54.089859 139867290535936 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:54.089891 139867290535936 gin_utils.py:85] from gin import config
I0512 01:44:54.089923 139867290535936 gin_utils.py:85] import seqio
I0512 01:44:54.089955 139867290535936 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:54.089987 139867290535936 gin_utils.py:85] from t5x import adafactor
I0512 01:44:54.090018 139867290535936 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:54.090050 139867290535936 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:54.090082 139867290535936 gin_utils.py:85] from t5x.contrib.moe import partitioning
I0512 01:44:54.090114 139867290535936 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:54.090145 139867290535936 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:54.090177 139867290535936 gin_utils.py:85] from t5x import partitioning as partitioning2
I0512 01:44:54.090209 139867290535936 gin_utils.py:85] from t5x import trainer
I0512 01:44:54.090244 139867290535936 gin_utils.py:85] from t5x import utils
I0512 01:44:54.090276 139867290535936 gin_utils.py:85] 
I0512 01:44:54.090309 139867290535936 gin_utils.py:85] # Macros:
I0512 01:44:54.090341 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.090373 139867290535936 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:54.090405 139867290535936 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:54.090437 139867290535936 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:54.090476 139867290535936 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:54.090509 139867290535936 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:54.090541 139867290535936 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:54.090573 139867290535936 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:54.090605 139867290535936 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:54.090636 139867290535936 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:54.090668 139867290535936 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:54.090725 139867290535936 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:54.090762 139867290535936 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:54.090794 139867290535936 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:54.090826 139867290535936 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:54.090858 139867290535936 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:54.090889 139867290535936 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:54.090921 139867290535936 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:54.090952 139867290535936 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:54.090984 139867290535936 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:54.091016 139867290535936 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:54.091047 139867290535936 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:54.091079 139867290535936 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:54.091110 139867290535936 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:54.091142 139867290535936 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:54.091173 139867290535936 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:54.091205 139867290535936 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:54.091239 139867290535936 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:54.091271 139867290535936 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:54.091303 139867290535936 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:54.091335 139867290535936 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:54.091367 139867290535936 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:54.091398 139867290535936 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:54.091430 139867290535936 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:54.091468 139867290535936 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:54.091502 139867290535936 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:54.091534 139867290535936 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:54.091565 139867290535936 gin_utils.py:85] SCALE = 0.1
I0512 01:44:54.091597 139867290535936 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:54.091629 139867290535936 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:54.091660 139867290535936 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:54.091693 139867290535936 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:54.091724 139867290535936 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:54.091756 139867290535936 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:54.091787 139867290535936 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:54.091819 139867290535936 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:54.091850 139867290535936 gin_utils.py:85] 
I0512 01:44:54.091882 139867290535936 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:54.091914 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.091946 139867290535936 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:54.091977 139867290535936 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:54.092009 139867290535936 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:54.092040 139867290535936 gin_utils.py:85] 
I0512 01:44:54.062551 140600233973760 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/contrib/moe/configs/runs/pretrain.gin
I0512 01:44:54.063065 140600233973760 resource_reader.py:50] system_path_file_exists:t5x/configs/runs/pretrain.gin
I0512 01:44:54.063372 140600233973760 resource_reader.py:37] gin-config opened resource file:/home/rosinality/t5x/t5x/configs/runs/pretrain.gin
I0512 01:44:54.077502 140600233973760 gin_utils.py:83] Gin Configuration:
I0512 01:44:54.093399 140600233973760 gin_utils.py:85] from __gin__ import dynamic_registration
I0512 01:44:54.093470 140600233973760 gin_utils.py:85] import __main__ as train_script
I0512 01:44:54.093512 140600233973760 gin_utils.py:85] from flax import linen
I0512 01:44:54.093548 140600233973760 gin_utils.py:85] import flaxformer
I0512 01:44:54.093582 140600233973760 gin_utils.py:85] from flaxformer.architectures.moe import moe_architecture
I0512 01:44:54.093614 140600233973760 gin_utils.py:85] from flaxformer.architectures.moe import moe_enums
I0512 01:44:54.093647 140600233973760 gin_utils.py:85] from flaxformer.architectures.moe import moe_layers
I0512 01:44:54.093688 140600233973760 gin_utils.py:85] from flaxformer.architectures.moe import routing
I0512 01:44:54.093720 140600233973760 gin_utils.py:85] from flaxformer.architectures.t5 import t5_architecture
I0512 01:44:54.093752 140600233973760 gin_utils.py:85] from flaxformer.components.attention import dense_attention
I0512 01:44:54.093784 140600233973760 gin_utils.py:85] from flaxformer.components.attention import memory_efficient_attention
I0512 01:44:54.093816 140600233973760 gin_utils.py:85] from flaxformer.components import dense
I0512 01:44:54.093848 140600233973760 gin_utils.py:85] from flaxformer.components import embedding
I0512 01:44:54.093880 140600233973760 gin_utils.py:85] from flaxformer.components import layer_norm
I0512 01:44:54.093914 140600233973760 gin_utils.py:85] from gin import config
I0512 01:44:54.093948 140600233973760 gin_utils.py:85] import seqio
I0512 01:44:54.093980 140600233973760 gin_utils.py:85] import t5.data.mixtures
I0512 01:44:54.094012 140600233973760 gin_utils.py:85] from t5x import adafactor
I0512 01:44:54.094044 140600233973760 gin_utils.py:85] from t5x.contrib.moe import adafactor_utils
I0512 01:44:54.094076 140600233973760 gin_utils.py:85] from t5x.contrib.moe import models
I0512 01:44:54.094108 140600233973760 gin_utils.py:85] from t5x.contrib.moe import partitioning as moe_partitioning
I0512 01:44:54.094140 140600233973760 gin_utils.py:85] from t5x.contrib.moe import trainer as moe_trainer
I0512 01:44:54.094172 140600233973760 gin_utils.py:85] from t5x import gin_utils
I0512 01:44:54.094203 140600233973760 gin_utils.py:85] from t5x import partitioning
I0512 01:44:54.094235 140600233973760 gin_utils.py:85] from t5x import trainer
I0512 01:44:54.094267 140600233973760 gin_utils.py:85] from t5x import utils
I0512 01:44:54.094298 140600233973760 gin_utils.py:85] 
I0512 01:44:54.094330 140600233973760 gin_utils.py:85] # Macros:
I0512 01:44:54.094363 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.094395 140600233973760 gin_utils.py:85] ACTIVATION_DTYPE = 'bfloat16'
I0512 01:44:54.094427 140600233973760 gin_utils.py:85] ACTIVATION_PARTITIONING_DIMS = 1
I0512 01:44:54.094459 140600233973760 gin_utils.py:85] ARCHITECTURE = @t5_architecture.DecoderOnly()
I0512 01:44:54.094491 140600233973760 gin_utils.py:85] AUX_LOSS_FACTOR = 0.01
I0512 01:44:54.094523 140600233973760 gin_utils.py:85] BATCH_SIZE = 384
I0512 01:44:54.094555 140600233973760 gin_utils.py:85] BIAS_INIT = @bias_init/linen.initializers.normal()
I0512 01:44:54.094586 140600233973760 gin_utils.py:85] DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED
I0512 01:44:54.094618 140600233973760 gin_utils.py:85] DROPOUT_FACTORY = @dropout_factory/linen.Dropout
I0512 01:44:54.094655 140600233973760 gin_utils.py:85] DROPOUT_RATE = 0.0
I0512 01:44:54.094690 140600233973760 gin_utils.py:85] EMBED_DIM = 2048
I0512 01:44:54.094722 140600233973760 gin_utils.py:85] EVAL_EXPERT_CAPACITY_FACTOR = 2.0
I0512 01:44:54.094753 140600233973760 gin_utils.py:85] EXPERT_DROPOUT_RATE = %DROPOUT_RATE
I0512 01:44:54.094785 140600233973760 gin_utils.py:85] EXPERT_MLP_DIM = %MLP_DIM
I0512 01:44:54.094817 140600233973760 gin_utils.py:85] GROUP_SIZE = 4096
I0512 01:44:54.094849 140600233973760 gin_utils.py:85] HEAD_DIM = 128
I0512 01:44:54.094881 140600233973760 gin_utils.py:85] JITTER_NOISE = 0.0
I0512 01:44:54.094915 140600233973760 gin_utils.py:85] LABEL_SMOOTHING = 0.0
I0512 01:44:54.094948 140600233973760 gin_utils.py:85] LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'
I0512 01:44:54.094980 140600233973760 gin_utils.py:85] MIXTURE_OR_TASK_MODULE = None
I0512 01:44:54.095012 140600233973760 gin_utils.py:85] MIXTURE_OR_TASK_NAME = 'mix_ul2_test'
I0512 01:44:54.095044 140600233973760 gin_utils.py:85] MLP_DIM = 8192
I0512 01:44:54.095076 140600233973760 gin_utils.py:85] MODEL = @models.MoeDecoderOnlyModel()
I0512 01:44:54.095138 140600233973760 gin_utils.py:85] MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'
I0512 01:44:54.095172 140600233973760 gin_utils.py:85] MODEL_PARALLEL_SUBMESH = None
I0512 01:44:54.095204 140600233973760 gin_utils.py:85] MOE_TRUNCATED_DTYPE = 'bfloat16'
I0512 01:44:54.095236 140600233973760 gin_utils.py:85] NUM_DECODER_LAYERS = 24
I0512 01:44:54.095267 140600233973760 gin_utils.py:85] NUM_DECODER_SPARSE_LAYERS = 4
I0512 01:44:54.095299 140600233973760 gin_utils.py:85] NUM_EMBEDDINGS = 256384
I0512 01:44:54.095330 140600233973760 gin_utils.py:85] NUM_EXPERT_PARTITIONS = 8
I0512 01:44:54.095362 140600233973760 gin_utils.py:85] NUM_EXPERTS = 8
I0512 01:44:54.095394 140600233973760 gin_utils.py:85] NUM_HEADS = 24
I0512 01:44:54.095425 140600233973760 gin_utils.py:85] NUM_MODEL_PARTITIONS = 4
I0512 01:44:54.095457 140600233973760 gin_utils.py:85] NUM_SELECTED_EXPERTS = 2
I0512 01:44:54.095489 140600233973760 gin_utils.py:85] OPTIMIZER = @adafactor.Adafactor()
I0512 01:44:54.095521 140600233973760 gin_utils.py:85] RANDOM_SEED = None
I0512 01:44:54.095552 140600233973760 gin_utils.py:85] ROUTER_Z_LOSS_FACTOR = 0.0001
I0512 01:44:54.095609 140600233973760 gin_utils.py:85] SCALE = 0.1
I0512 01:44:54.095644 140600233973760 gin_utils.py:85] SHUFFLE_TRAIN_EXAMPLES = True
I0512 01:44:54.095683 140600233973760 gin_utils.py:85] TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}
I0512 01:44:54.095715 140600233973760 gin_utils.py:85] TRAIN_EXPERT_CAPACITY_FACTOR = 1.25
I0512 01:44:54.095748 140600233973760 gin_utils.py:85] TRAIN_STEPS = 500000
I0512 01:44:54.095780 140600233973760 gin_utils.py:85] USE_CACHED_TASKS = False
I0512 01:44:54.095811 140600233973760 gin_utils.py:85] USE_HARDWARE_RNG = False
I0512 01:44:54.095843 140600233973760 gin_utils.py:85] VOCABULARY = @seqio.SentencePieceVocabulary()
I0512 01:44:54.095875 140600233973760 gin_utils.py:85] Z_LOSS = 0.0001
I0512 01:44:54.095909 140600233973760 gin_utils.py:85] 
I0512 01:44:54.095943 140600233973760 gin_utils.py:85] # Parameters for adafactor.Adafactor:
I0512 01:44:54.095975 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096007 140600233973760 gin_utils.py:85] adafactor.Adafactor.decay_rate = 0.8
I0512 01:44:54.096039 140600233973760 gin_utils.py:85] adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()
I0512 01:44:54.096071 140600233973760 gin_utils.py:85] adafactor.Adafactor.step_offset = 0
I0512 01:44:54.096103 140600233973760 gin_utils.py:85] 
I0512 01:44:54.007774 139836023437312 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.007806 139836023437312 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:54.007839 139836023437312 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:54.007871 139836023437312 gin_utils.py:85] 
I0512 01:44:54.007903 139836023437312 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:54.007943 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.007977 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:54.008009 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.008042 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.008074 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:54.008106 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.008138 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:54.008171 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.008203 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:54.008235 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:54.008268 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:54.008300 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:54.008332 139836023437312 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.008364 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.825910 139933136287744 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:53.825941 139933136287744 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:53.825972 139933136287744 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:53.826003 139933136287744 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:53.826034 139933136287744 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:53.826065 139933136287744 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:53.826096 139933136287744 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:53.826127 139933136287744 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:53.826158 139933136287744 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:53.826189 139933136287744 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:53.826220 139933136287744 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:53.826251 139933136287744 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:53.826283 139933136287744 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:53.826314 139933136287744 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:53.826344 139933136287744 gin_utils.py:85] 
I0512 01:44:53.826375 139933136287744 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:53.826406 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826443 139933136287744 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.826476 139933136287744 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:53.826507 139933136287744 gin_utils.py:85] 
I0512 01:44:53.826539 139933136287744 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.826570 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826601 139933136287744 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.826632 139933136287744 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.826663 139933136287744 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.826695 139933136287744 gin_utils.py:85] 
I0512 01:44:53.826727 139933136287744 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.826759 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826790 139933136287744 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.826821 139933136287744 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.826852 139933136287744 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.826883 139933136287744 gin_utils.py:85] 
I0512 01:44:53.826914 139933136287744 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.826945 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.826976 139933136287744 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.827007 139933136287744 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.827038 139933136287744 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.827069 139933136287744 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.827099 139933136287744 gin_utils.py:85] 
I0512 01:44:53.827130 139933136287744 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.827161 139933136287744 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.827192 139933136287744 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.827223 139933136287744 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.827254 139933136287744 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.827285 139933136287744 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.828174 139933136287744 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478293.875016  291751 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478293.875072  291751 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478293.875074  291751 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:53.714552 139995375843328 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:53.714584 139995375843328 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:53.714617 139995375843328 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:53.714649 139995375843328 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:53.714682 139995375843328 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:53.714714 139995375843328 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:53.714746 139995375843328 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:53.714779 139995375843328 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:53.714811 139995375843328 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:53.714844 139995375843328 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:54.092072 139867290535936 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:54.092103 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.092135 139867290535936 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:54.092167 139867290535936 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:54.092200 139867290535936 gin_utils.py:85] 
I0512 01:44:54.092233 139867290535936 gin_utils.py:85] # Parameters for partitioning.compute_num_model_partitions:
I0512 01:44:54.092267 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.092299 139867290535936 gin_utils.py:85] partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:54.092331 139867290535936 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.092363 139867290535936 gin_utils.py:85] partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:54.092395 139867290535936 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:54.092427 139867290535936 gin_utils.py:85] 
I0512 01:44:54.092464 139867290535936 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:54.092497 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.092529 139867290535936 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:54.092561 139867290535936 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:54.092593 139867290535936 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:54.092624 139867290535936 gin_utils.py:85] 
I0512 01:44:54.092656 139867290535936 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:54.092688 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.092719 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:54.092751 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.092783 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.092814 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:54.092846 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:54.092878 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:54.092909 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:54.092941 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.092972 139867290535936 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.093004 139867290535936 gin_utils.py:85] 
I0512 01:44:54.093036 139867290535936 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:54.093068 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.093100 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:54.093132 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.093163 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.093195 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:54.093229 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:54.093262 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:54.093293 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:54.093325 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.093356 139867290535936 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.093388 139867290535936 gin_utils.py:85] 
I0512 01:44:54.093420 139867290535936 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:54.093451 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.093490 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.093522 139867290535936 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.093554 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.093586 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.093617 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.093649 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:54.093681 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:54.093712 139867290535936 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:54.093744 139867290535936 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.093776 139867290535936 gin_utils.py:85] 
I0512 01:44:54.093807 139867290535936 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:54.093839 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.093870 139867290535936 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:54.093902 139867290535936 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.093934 139867290535936 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:54.093965 139867290535936 gin_utils.py:85] 
I0512 01:44:54.093997 139867290535936 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:54.094028 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.094060 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:54.094092 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:54.094123 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:54.094155 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:54.094187 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:54.094220 139867290535936 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.094253 139867290535936 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:54.094307 139867290535936 gin_utils.py:85] 
I0512 01:44:54.094341 139867290535936 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:54.094373 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.094405 139867290535936 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:54.094437 139867290535936 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:54.094474 139867290535936 gin_utils.py:85] 
I0512 01:44:54.094507 139867290535936 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:54.094539 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.094571 139867290535936 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:54.094602 139867290535936 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:54.094634 139867290535936 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.094666 139867290535936 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:54.094716 139867290535936 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:54.094753 139867290535936 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:54.094786 139867290535936 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:54.094818 139867290535936 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:54.094850 139867290535936 gin_utils.py:85] 
I0512 01:44:54.094882 139867290535936 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:54.094914 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.094947 139867290535936 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.094979 139867290535936 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.095011 139867290535936 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.095043 139867290535936 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:54.095075 139867290535936 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.095107 139867290535936 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:54.095139 139867290535936 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:54.096135 140600233973760 gin_utils.py:85] # Parameters for utils.CheckpointConfig:
I0512 01:44:54.096167 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096198 140600233973760 gin_utils.py:85] utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()
I0512 01:44:54.096231 140600233973760 gin_utils.py:85] utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()
I0512 01:44:54.096263 140600233973760 gin_utils.py:85] 
I0512 01:44:54.096294 140600233973760 gin_utils.py:85] # Parameters for moe_partitioning.compute_num_model_partitions:
I0512 01:44:54.096326 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096358 140600233973760 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \
I0512 01:44:54.096390 140600233973760 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.096422 140600233973760 gin_utils.py:85] moe_partitioning.compute_num_model_partitions.num_model_partitions = \
I0512 01:44:54.096454 140600233973760 gin_utils.py:85]     %NUM_MODEL_PARTITIONS
I0512 01:44:54.096486 140600233973760 gin_utils.py:85] 
I0512 01:44:54.096517 140600233973760 gin_utils.py:85] # Parameters for utils.create_learning_rate_scheduler:
I0512 01:44:54.096549 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096581 140600233973760 gin_utils.py:85] utils.create_learning_rate_scheduler.base_learning_rate = 1.0
I0512 01:44:54.096613 140600233973760 gin_utils.py:85] utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'
I0512 01:44:54.096645 140600233973760 gin_utils.py:85] utils.create_learning_rate_scheduler.warmup_steps = 10000
I0512 01:44:54.096686 140600233973760 gin_utils.py:85] 
I0512 01:44:54.096718 140600233973760 gin_utils.py:85] # Parameters for train/utils.DatasetConfig:
I0512 01:44:54.096750 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096782 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.batch_size = %BATCH_SIZE
I0512 01:44:54.096814 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.096846 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.096878 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.pack = True
I0512 01:44:54.096911 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.seed = 42
I0512 01:44:54.096944 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES
I0512 01:44:54.096977 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.split = 'train'
I0512 01:44:54.097009 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.097041 140600233973760 gin_utils.py:85] train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.097073 140600233973760 gin_utils.py:85] 
I0512 01:44:54.097104 140600233973760 gin_utils.py:85] # Parameters for train_eval/utils.DatasetConfig:
I0512 01:44:54.097136 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097168 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.batch_size = 128
I0512 01:44:54.097200 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME
I0512 01:44:54.097232 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE
I0512 01:44:54.097264 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.pack = True
I0512 01:44:54.097296 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.seed = 42
I0512 01:44:54.097328 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.shuffle = False
I0512 01:44:54.097359 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.split = 'validation'
I0512 01:44:54.097391 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS
I0512 01:44:54.097423 140600233973760 gin_utils.py:85] train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS
I0512 01:44:54.097455 140600233973760 gin_utils.py:85] 
I0512 01:44:54.097487 140600233973760 gin_utils.py:85] # Parameters for t5_architecture.DecoderLayer:
I0512 01:44:54.097519 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097551 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.097582 140600233973760 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.097614 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.097646 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.097686 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.097718 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()
I0512 01:44:54.097750 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.scanned = False
I0512 01:44:54.097782 140600233973760 gin_utils.py:85] t5_architecture.DecoderLayer.self_attention = \
I0512 01:44:54.097814 140600233973760 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.097846 140600233973760 gin_utils.py:85] 
I0512 01:44:54.097877 140600233973760 gin_utils.py:85] # Parameters for t5_architecture.DecoderOnly:
I0512 01:44:54.097910 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097944 140600233973760 gin_utils.py:85] t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder
I0512 01:44:54.097976 140600233973760 gin_utils.py:85] t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.098008 140600233973760 gin_utils.py:85] t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed
I0512 01:44:54.098040 140600233973760 gin_utils.py:85] 
I0512 01:44:54.098072 140600233973760 gin_utils.py:85] # Parameters for output_logits/dense.DenseGeneral:
I0512 01:44:54.098104 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098136 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT
I0512 01:44:54.098168 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.dtype = 'float32'
I0512 01:44:54.098200 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS
I0512 01:44:54.098231 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']
I0512 01:44:54.098263 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.kernel_init = \
I0512 01:44:54.098296 140600233973760 gin_utils.py:85]     @output_logits_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.098328 140600233973760 gin_utils.py:85] output_logits/dense.DenseGeneral.use_bias = False
I0512 01:44:54.098359 140600233973760 gin_utils.py:85] 
I0512 01:44:54.098391 140600233973760 gin_utils.py:85] # Parameters for dropout_factory/linen.Dropout:
I0512 01:44:54.098423 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098454 140600233973760 gin_utils.py:85] dropout_factory/linen.Dropout.broadcast_dims = (-2,)
I0512 01:44:54.098486 140600233973760 gin_utils.py:85] dropout_factory/linen.Dropout.rate = %DROPOUT_RATE
I0512 01:44:54.098518 140600233973760 gin_utils.py:85] 
I0512 01:44:54.098550 140600233973760 gin_utils.py:85] # Parameters for embedding.Embed:
I0512 01:44:54.098582 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098613 140600233973760 gin_utils.py:85] embedding.Embed.attend_dtype = 'float32'
I0512 01:44:54.098645 140600233973760 gin_utils.py:85] embedding.Embed.cast_input_dtype = 'int32'
I0512 01:44:54.098684 140600233973760 gin_utils.py:85] embedding.Embed.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.098716 140600233973760 gin_utils.py:85] embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()
I0512 01:44:54.098748 140600233973760 gin_utils.py:85] embedding.Embed.features = %EMBED_DIM
I0512 01:44:54.098780 140600233973760 gin_utils.py:85] embedding.Embed.name = 'token_embedder'
I0512 01:44:54.098811 140600233973760 gin_utils.py:85] embedding.Embed.num_embeddings = %NUM_EMBEDDINGS
I0512 01:44:54.098843 140600233973760 gin_utils.py:85] embedding.Embed.one_hot = True
I0512 01:44:54.098875 140600233973760 gin_utils.py:85] 
I0512 01:44:54.098908 140600233973760 gin_utils.py:85] # Parameters for dense.MlpBlock:
I0512 01:44:54.098942 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098975 140600233973760 gin_utils.py:85] dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.099007 140600233973760 gin_utils.py:85] dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.099040 140600233973760 gin_utils.py:85] dense.MlpBlock.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.099071 140600233973760 gin_utils.py:85] dense.MlpBlock.final_dropout_rate = 0
I0512 01:44:54.099104 140600233973760 gin_utils.py:85] dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.099136 140600233973760 gin_utils.py:85] dense.MlpBlock.intermediate_dim = %MLP_DIM
I0512 01:44:54.099168 140600233973760 gin_utils.py:85] dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE
I0512 01:44:54.008396 139836023437312 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:54.008429 139836023437312 gin_utils.py:85] 
I0512 01:44:54.008460 139836023437312 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:54.008493 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.008525 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:54.008558 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:54.008590 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:54.008623 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:54.008655 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:54.008690 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:54.008724 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:54.008756 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:54.008789 139836023437312 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:54.008821 139836023437312 gin_utils.py:85] 
I0512 01:44:54.008854 139836023437312 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:54.008886 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.008925 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.008959 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.008991 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:54.009024 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:54.009056 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.009088 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:54.009121 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:54.009153 139836023437312 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:54.009186 139836023437312 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.009218 139836023437312 gin_utils.py:85] 
I0512 01:44:54.009251 139836023437312 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:54.009306 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.009344 139836023437312 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:54.009377 139836023437312 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:54.009410 139836023437312 gin_utils.py:85] 
I0512 01:44:54.009442 139836023437312 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:54.009474 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.009507 139836023437312 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:54.009540 139836023437312 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.009572 139836023437312 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.009605 139836023437312 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:54.009638 139836023437312 gin_utils.py:85] 
I0512 01:44:54.009672 139836023437312 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:54.009724 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.009763 139836023437312 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.009796 139836023437312 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.009829 139836023437312 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:54.009862 139836023437312 gin_utils.py:85] 
I0512 01:44:54.009895 139836023437312 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:54.009934 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.009967 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:54.010000 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:54.010033 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:54.010066 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.010098 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:54.010132 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:54.010164 139836023437312 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.010196 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:54.010229 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:54.010262 139836023437312 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:54.010294 139836023437312 gin_utils.py:85] 
I0512 01:44:54.010327 139836023437312 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:54.010360 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.010392 139836023437312 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:54.010425 139836023437312 gin_utils.py:85] 
I0512 01:44:54.010458 139836023437312 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:54.010491 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.010523 139836023437312 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:54.010556 139836023437312 gin_utils.py:85] 
I0512 01:44:54.010588 139836023437312 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:54.010621 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.010653 139836023437312 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:54.010689 139836023437312 gin_utils.py:85] 
I0512 01:44:54.010722 139836023437312 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:54.010755 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.010788 139836023437312 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:54.010820 139836023437312 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:54.010853 139836023437312 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:54.010885 139836023437312 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:54.010923 139836023437312 gin_utils.py:85] 
I0512 01:44:54.010957 139836023437312 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:54.010989 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.011022 139836023437312 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:54.011054 139836023437312 gin_utils.py:85] 
I0512 01:44:54.011087 139836023437312 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:54.011119 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.011152 139836023437312 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:54.011184 139836023437312 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:54.011217 139836023437312 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:54.011250 139836023437312 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:54.011282 139836023437312 gin_utils.py:85] 
I0512 01:44:54.011314 139836023437312 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:54.011347 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.011379 139836023437312 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:54.011411 139836023437312 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:54.011444 139836023437312 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:54.011477 139836023437312 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:54.011509 139836023437312 gin_utils.py:85] 
I0512 01:44:54.011542 139836023437312 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:54.011574 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.011607 139836023437312 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:54.011640 139836023437312 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:54.011674 139836023437312 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:54.011708 139836023437312 gin_utils.py:85] 
I0512 01:44:54.011741 139836023437312 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:54.011774 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.011806 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.011839 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.011872 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:54.011909 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.011943 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:54.011977 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:54.012009 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:54.012042 139836023437312 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:54.012074 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:54.012107 139836023437312 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:54.012140 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:54.012172 139836023437312 gin_utils.py:85] 
I0512 01:44:54.012205 139836023437312 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:54.012237 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.012270 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.012303 139836023437312 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.012335 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.012375 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.012412 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:54.012446 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.012483 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:54.012518 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:54.012552 139836023437312 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:54.012587 139836023437312 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.012624 139836023437312 gin_utils.py:85] 
I0512 01:44:54.012659 139836023437312 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:54.012696 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.012733 139836023437312 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.012769 139836023437312 gin_utils.py:85] 
I0512 01:44:54.012802 139836023437312 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:54.012838 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.012873 139836023437312 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:54.012914 139836023437312 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:54.012949 139836023437312 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:54.012985 139836023437312 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:54.013021 139836023437312 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:54.013054 139836023437312 gin_utils.py:85] 
I0512 01:44:54.013090 139836023437312 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.013125 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.013158 139836023437312 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.013194 139836023437312 gin_utils.py:85] 
I0512 01:44:54.013229 139836023437312 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.013262 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.013299 139836023437312 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.013335 139836023437312 gin_utils.py:85] 
I0512 01:44:54.013369 139836023437312 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:54.013405 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.013441 139836023437312 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:54.095170 139867290535936 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.095202 139867290535936 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:54.095237 139867290535936 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:54.095269 139867290535936 gin_utils.py:85] 
I0512 01:44:54.095301 139867290535936 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:54.095333 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.095365 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:54.095397 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.095429 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.095467 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:54.095500 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.095533 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:54.095565 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.095597 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:54.095628 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:54.095661 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:54.095693 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:54.095725 139867290535936 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.095757 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:54.099200 140600233973760 gin_utils.py:85] dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.099232 140600233973760 gin_utils.py:85] dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:54.099264 140600233973760 gin_utils.py:85] dense.MlpBlock.use_bias = False
I0512 01:44:54.099296 140600233973760 gin_utils.py:85] 
I0512 01:44:54.099328 140600233973760 gin_utils.py:85] # Parameters for expert/dense.MlpBlock:
I0512 01:44:54.099360 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.099392 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.activation_partitioning_dims = 1
I0512 01:44:54.099423 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.activations = ('swiglu', 'linear')
I0512 01:44:54.099456 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.bias_init = %BIAS_INIT
I0512 01:44:54.099488 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')
I0512 01:44:54.099520 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.099552 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.final_dropout_rate = 0.0
I0512 01:44:54.099605 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.input_axis_name = 'mlp_embed'
I0512 01:44:54.099640 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'
I0512 01:44:54.099678 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM
I0512 01:44:54.099711 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE
I0512 01:44:54.099743 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.kernel_init = \
I0512 01:44:54.099775 140600233973760 gin_utils.py:85]     @expert_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.099807 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.output_axis_name = 'mlp_embed'
I0512 01:44:53.765775 140139156215808 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:53.765805 140139156215808 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:53.765836 140139156215808 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:53.765867 140139156215808 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:53.765897 140139156215808 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:53.765928 140139156215808 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:53.765959 140139156215808 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:53.765989 140139156215808 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:53.766020 140139156215808 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:54.013475 139836023437312 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:54.013512 139836023437312 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:54.013548 139836023437312 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:54.013581 139836023437312 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:54.013617 139836023437312 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:54.013652 139836023437312 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:54.013688 139836023437312 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:54.013749 139836023437312 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:54.013788 139836023437312 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:54.013825 139836023437312 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:54.013859 139836023437312 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:54.013895 139836023437312 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:54.013936 139836023437312 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:54.013972 139836023437312 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:54.014006 139836023437312 gin_utils.py:85] 
I0512 01:44:54.014042 139836023437312 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:54.014078 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.014111 139836023437312 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.014147 139836023437312 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:54.014182 139836023437312 gin_utils.py:85] 
I0512 01:44:54.014215 139836023437312 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.014251 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.014288 139836023437312 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.014323 139836023437312 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.014357 139836023437312 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.014392 139836023437312 gin_utils.py:85] 
I0512 01:44:54.014428 139836023437312 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.014461 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.014497 139836023437312 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.014532 139836023437312 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.014566 139836023437312 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.014601 139836023437312 gin_utils.py:85] 
I0512 01:44:54.014637 139836023437312 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.014672 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.014709 139836023437312 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.014745 139836023437312 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.014778 139836023437312 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.014814 139836023437312 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.014849 139836023437312 gin_utils.py:85] 
I0512 01:44:54.014883 139836023437312 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.014923 139836023437312 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.014961 139836023437312 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.014997 139836023437312 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.015030 139836023437312 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.015066 139836023437312 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.016043 139836023437312 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478294.064319  284980 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478294.064386  284980 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478294.064389  284980 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:54.095788 139867290535936 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:54.095821 139867290535936 gin_utils.py:85] 
I0512 01:44:54.095853 139867290535936 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:54.095885 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.095916 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:54.095949 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:54.095981 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:54.096013 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:54.096045 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:54.096077 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:54.096109 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:54.096141 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:54.096173 139867290535936 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:54.096204 139867290535936 gin_utils.py:85] 
I0512 01:44:54.096238 139867290535936 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:54.096271 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096303 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.096334 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.096366 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:54.096399 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:54.096430 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.096467 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:54.096500 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:54.096532 139867290535936 gin_utils.py:85]     @partitioning.compute_num_model_partitions()
I0512 01:44:54.096564 139867290535936 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.096595 139867290535936 gin_utils.py:85] 
I0512 01:44:54.096627 139867290535936 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:54.096658 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096689 139867290535936 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:54.096721 139867290535936 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:54.096753 139867290535936 gin_utils.py:85] 
I0512 01:44:54.096785 139867290535936 gin_utils.py:85] # Parameters for partitioning.MoePjitPartitioner:
I0512 01:44:54.096816 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.096848 139867290535936 gin_utils.py:85] partitioning.MoePjitPartitioner.model_parallel_submesh = %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.096880 139867290535936 gin_utils.py:85] partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.096912 139867290535936 gin_utils.py:85] partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:54.096944 139867290535936 gin_utils.py:85] 
I0512 01:44:54.096975 139867290535936 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:54.097007 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097039 139867290535936 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.097070 139867290535936 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.097102 139867290535936 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:54.097134 139867290535936 gin_utils.py:85] 
I0512 01:44:54.097165 139867290535936 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:54.097197 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097231 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:54.097264 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:54.097296 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:54.097328 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.097359 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:54.097391 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:54.097423 139867290535936 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.097455 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:54.097493 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:54.097525 139867290535936 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:54.097557 139867290535936 gin_utils.py:85] 
I0512 01:44:54.097589 139867290535936 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:54.097620 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097652 139867290535936 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:54.097684 139867290535936 gin_utils.py:85] 
I0512 01:44:54.097717 139867290535936 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:54.097748 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097780 139867290535936 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:54.097812 139867290535936 gin_utils.py:85] 
I0512 01:44:54.097843 139867290535936 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:54.097875 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.097907 139867290535936 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:54.097939 139867290535936 gin_utils.py:85] 
I0512 01:44:54.097971 139867290535936 gin_utils.py:85] # Parameters for partitioning2.PjitPartitioner:
I0512 01:44:54.098003 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098035 139867290535936 gin_utils.py:85] partitioning2.PjitPartitioner.logical_axis_rules = \
I0512 01:44:54.098067 139867290535936 gin_utils.py:85]     @partitioning2.standard_logical_axis_rules()
I0512 01:44:54.098098 139867290535936 gin_utils.py:85] partitioning2.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:54.098129 139867290535936 gin_utils.py:85] partitioning2.PjitPartitioner.num_partitions = 1
I0512 01:44:54.098161 139867290535936 gin_utils.py:85] 
I0512 01:44:54.098193 139867290535936 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:54.098226 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098259 139867290535936 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:54.098291 139867290535936 gin_utils.py:85] 
I0512 01:44:54.098324 139867290535936 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:54.098356 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098388 139867290535936 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:54.098420 139867290535936 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:54.098451 139867290535936 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:54.098489 139867290535936 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:54.098522 139867290535936 gin_utils.py:85] 
I0512 01:44:54.098554 139867290535936 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:54.098586 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098617 139867290535936 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:54.098649 139867290535936 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:54.098681 139867290535936 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:54.098736 139867290535936 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:54.098769 139867290535936 gin_utils.py:85] 
I0512 01:44:54.098801 139867290535936 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:54.098833 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.098865 139867290535936 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:54.098897 139867290535936 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:54.098928 139867290535936 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:54.098960 139867290535936 gin_utils.py:85] 
I0512 01:44:54.098992 139867290535936 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:54.099024 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.099056 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.099087 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.099119 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:54.099151 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.099183 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:54.099216 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:54.099249 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:54.099282 139867290535936 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:54.099314 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:54.099346 139867290535936 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:54.099378 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:54.099410 139867290535936 gin_utils.py:85] 
I0512 01:44:54.099442 139867290535936 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:54.099480 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.099513 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.099545 139867290535936 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.099577 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.099609 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.099642 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:54.099675 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.099708 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:54.099740 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:54.099773 139867290535936 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:54.099805 139867290535936 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.099837 139867290535936 gin_utils.py:85] 
I0512 01:44:54.099871 139867290535936 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:54.099904 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.099936 139867290535936 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.099968 139867290535936 gin_utils.py:85] 
I0512 01:44:54.100001 139867290535936 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:54.100033 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100066 139867290535936 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:54.100098 139867290535936 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:54.100131 139867290535936 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:54.100163 139867290535936 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:54.100196 139867290535936 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:54.100230 139867290535936 gin_utils.py:85] 
I0512 01:44:54.100264 139867290535936 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.100297 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100329 139867290535936 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.100362 139867290535936 gin_utils.py:85] 
I0512 01:44:54.100394 139867290535936 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.100427 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100464 139867290535936 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.100498 139867290535936 gin_utils.py:85] 
I0512 01:44:54.100532 139867290535936 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:54.100565 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100598 139867290535936 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:54.100630 139867290535936 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:54.099838 140600233973760 gin_utils.py:85] expert/dense.MlpBlock.use_bias = False
I0512 01:44:54.099870 140600233973760 gin_utils.py:85] 
I0512 01:44:54.099902 140600233973760 gin_utils.py:85] # Parameters for models.MoeDecoderOnlyModel:
I0512 01:44:54.099936 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.099969 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR
I0512 01:44:54.100001 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True
I0512 01:44:54.100033 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING
I0512 01:44:54.100065 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR
I0512 01:44:54.100097 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.module = %ARCHITECTURE
I0512 01:44:54.100129 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER
I0512 01:44:54.100161 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR
I0512 01:44:54.100193 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY
I0512 01:44:54.100225 140600233973760 gin_utils.py:85] models.MoeDecoderOnlyModel.z_loss = %Z_LOSS
I0512 01:44:54.100257 140600233973760 gin_utils.py:85] 
I0512 01:44:54.100289 140600233973760 gin_utils.py:85] # Parameters for moe_layers.MoeLayer:
I0512 01:44:54.100321 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100353 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE
I0512 01:44:54.100384 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.100417 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()
I0512 01:44:54.100449 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.max_group_size = %GROUP_SIZE
I0512 01:44:54.100480 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.100512 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.num_experts = %NUM_EXPERTS
I0512 01:44:54.100544 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.num_model_partitions = \
I0512 01:44:54.100577 140600233973760 gin_utils.py:85]     @moe_partitioning.compute_num_model_partitions()
I0512 01:44:54.100608 140600233973760 gin_utils.py:85] moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR
I0512 01:44:54.100640 140600233973760 gin_utils.py:85] 
I0512 01:44:54.100678 140600233973760 gin_utils.py:85] # Parameters for sparse_decoder/moe_layers.MoeLayer:
I0512 01:44:54.100711 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100743 140600233973760 gin_utils.py:85] sparse_decoder/moe_layers.MoeLayer.router = \
I0512 01:44:54.100775 140600233973760 gin_utils.py:85]     @sparse_decoder/routing.TokensChooseMaskedRouter()
I0512 01:44:54.100807 140600233973760 gin_utils.py:85] 
I0512 01:44:54.100839 140600233973760 gin_utils.py:85] # Parameters for moe_partitioning.MoePjitPartitioner:
I0512 01:44:54.100871 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.100904 140600233973760 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \
I0512 01:44:54.100938 140600233973760 gin_utils.py:85]     %MODEL_PARALLEL_SUBMESH
I0512 01:44:54.100970 140600233973760 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.101002 140600233973760 gin_utils.py:85] moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS
I0512 01:44:54.101035 140600233973760 gin_utils.py:85] 
I0512 01:44:54.101066 140600233973760 gin_utils.py:85] # Parameters for moe_trainer.MoeTrainer:
I0512 01:44:54.101098 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101130 140600233973760 gin_utils.py:85] moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.101162 140600233973760 gin_utils.py:85] moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS
I0512 01:44:54.101194 140600233973760 gin_utils.py:85] moe_trainer.MoeTrainer.num_microbatches = 8
I0512 01:44:54.101226 140600233973760 gin_utils.py:85] 
I0512 01:44:54.101258 140600233973760 gin_utils.py:85] # Parameters for dense_attention.MultiHeadDotProductAttention:
I0512 01:44:54.101290 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101322 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT
I0512 01:44:54.101354 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True
I0512 01:44:54.101386 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE
I0512 01:44:54.101418 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.101450 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM
I0512 01:44:54.101481 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.kernel_init = \
I0512 01:44:54.101513 140600233973760 gin_utils.py:85]     @attention_kernel_init/linen.initializers.variance_scaling()
I0512 01:44:54.101545 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS
I0512 01:44:54.101577 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_bias = False
I0512 01:44:54.101608 140600233973760 gin_utils.py:85] dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True
I0512 01:44:54.101640 140600233973760 gin_utils.py:85] 
I0512 01:44:54.101679 140600233973760 gin_utils.py:85] # Parameters for bias_init/linen.initializers.normal:
I0512 01:44:54.101712 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101744 140600233973760 gin_utils.py:85] bias_init/linen.initializers.normal.stddev = 1e-06
I0512 01:44:54.101778 140600233973760 gin_utils.py:85] 
I0512 01:44:54.101810 140600233973760 gin_utils.py:85] # Parameters for router_init/linen.initializers.normal:
I0512 01:44:54.101842 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101874 140600233973760 gin_utils.py:85] router_init/linen.initializers.normal.stddev = 0.02
I0512 01:44:54.101907 140600233973760 gin_utils.py:85] 
I0512 01:44:54.101942 140600233973760 gin_utils.py:85] # Parameters for token_embedder_init/linen.initializers.normal:
I0512 01:44:54.101978 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102012 140600233973760 gin_utils.py:85] token_embedder_init/linen.initializers.normal.stddev = 1.0
I0512 01:44:54.102044 140600233973760 gin_utils.py:85] 
I0512 01:44:54.102075 140600233973760 gin_utils.py:85] # Parameters for partitioning.PjitPartitioner:
I0512 01:44:54.102108 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102139 140600233973760 gin_utils.py:85] partitioning.PjitPartitioner.logical_axis_rules = \
I0512 01:44:54.102171 140600233973760 gin_utils.py:85]     @partitioning.standard_logical_axis_rules()
I0512 01:44:54.102203 140600233973760 gin_utils.py:85] partitioning.PjitPartitioner.model_parallel_submesh = None
I0512 01:44:54.102235 140600233973760 gin_utils.py:85] partitioning.PjitPartitioner.num_partitions = 1
I0512 01:44:54.102267 140600233973760 gin_utils.py:85] 
I0512 01:44:54.102298 140600233973760 gin_utils.py:85] # Parameters for utils.RestoreCheckpointConfig:
I0512 01:44:54.102330 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102362 140600233973760 gin_utils.py:85] utils.RestoreCheckpointConfig.path = []
I0512 01:44:54.102394 140600233973760 gin_utils.py:85] 
I0512 01:44:54.102426 140600233973760 gin_utils.py:85] # Parameters for routing.RouterWeights:
I0512 01:44:54.102457 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102490 140600233973760 gin_utils.py:85] routing.RouterWeights.bias_init = %BIAS_INIT
I0512 01:44:54.102521 140600233973760 gin_utils.py:85] routing.RouterWeights.dtype = 'float32'
I0512 01:44:54.102553 140600233973760 gin_utils.py:85] routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()
I0512 01:44:54.102585 140600233973760 gin_utils.py:85] routing.RouterWeights.use_bias = False
I0512 01:44:54.102617 140600233973760 gin_utils.py:85] 
I0512 01:44:54.102654 140600233973760 gin_utils.py:85] # Parameters for utils.SaveCheckpointConfig:
I0512 01:44:54.102688 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102721 140600233973760 gin_utils.py:85] utils.SaveCheckpointConfig.dtype = 'float32'
I0512 01:44:54.102753 140600233973760 gin_utils.py:85] utils.SaveCheckpointConfig.keep = 20
I0512 01:44:54.102785 140600233973760 gin_utils.py:85] utils.SaveCheckpointConfig.period = 2500
I0512 01:44:54.102817 140600233973760 gin_utils.py:85] utils.SaveCheckpointConfig.save_dataset = True
I0512 01:44:54.102849 140600233973760 gin_utils.py:85] 
I0512 01:44:54.102880 140600233973760 gin_utils.py:85] # Parameters for seqio.SentencePieceVocabulary:
I0512 01:44:54.102914 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.102948 140600233973760 gin_utils.py:85] seqio.SentencePieceVocabulary.extra_ids = 300
I0512 01:44:54.102980 140600233973760 gin_utils.py:85] seqio.SentencePieceVocabulary.sentencepiece_model_file = \
I0512 01:44:54.103012 140600233973760 gin_utils.py:85]     'gs://rosinality-tpu-bucket/sentencepiece.model'
I0512 01:44:54.103044 140600233973760 gin_utils.py:85] 
I0512 01:44:54.103075 140600233973760 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoder:
I0512 01:44:54.103107 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.103139 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.103171 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.103203 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer
I0512 01:44:54.103235 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.103267 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS
I0512 01:44:54.103299 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS
I0512 01:44:54.103331 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.output_logits_factory = \
I0512 01:44:54.103363 140600233973760 gin_utils.py:85]     @output_logits/dense.DenseGeneral
I0512 01:44:54.103394 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layer_factory = \
I0512 01:44:54.103426 140600233973760 gin_utils.py:85]     @moe_architecture.SparseDecoderLayer
I0512 01:44:54.103458 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT
I0512 01:44:54.103490 140600233973760 gin_utils.py:85] 
I0512 01:44:54.103522 140600233973760 gin_utils.py:85] # Parameters for moe_architecture.SparseDecoderLayer:
I0512 01:44:54.103554 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.103610 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \
I0512 01:44:54.103645 140600233973760 gin_utils.py:85]     %ACTIVATION_PARTITIONING_DIMS
I0512 01:44:54.103684 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY
I0512 01:44:54.103717 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None
I0512 01:44:54.103749 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()
I0512 01:44:54.103782 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm
I0512 01:44:54.103814 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()
I0512 01:44:54.103847 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.scanned = False
I0512 01:44:54.103879 140600233973760 gin_utils.py:85] moe_architecture.SparseDecoderLayer.self_attention = \
I0512 01:44:54.103913 140600233973760 gin_utils.py:85]     @dense_attention.MultiHeadDotProductAttention()
I0512 01:44:54.103947 140600233973760 gin_utils.py:85] 
I0512 01:44:54.103980 140600233973760 gin_utils.py:85] # Parameters for layer_norm.T5LayerNorm:
I0512 01:44:54.104012 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.104044 140600233973760 gin_utils.py:85] layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE
I0512 01:44:54.104076 140600233973760 gin_utils.py:85] 
I0512 01:44:54.104108 140600233973760 gin_utils.py:85] # Parameters for routing.TokensChooseMaskedRouter:
I0512 01:44:54.104140 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.104172 140600233973760 gin_utils.py:85] routing.TokensChooseMaskedRouter.dtype = 'float32'
I0512 01:44:54.104204 140600233973760 gin_utils.py:85] routing.TokensChooseMaskedRouter.ignore_padding_tokens = False
I0512 01:44:54.104237 140600233973760 gin_utils.py:85] routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE
I0512 01:44:54.104269 140600233973760 gin_utils.py:85] routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS
I0512 01:44:54.104301 140600233973760 gin_utils.py:85] routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()
I0512 01:44:54.104333 140600233973760 gin_utils.py:85] 
I0512 01:44:54.104364 140600233973760 gin_utils.py:85] # Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.104396 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.104428 140600233973760 gin_utils.py:85] sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.104460 140600233973760 gin_utils.py:85] 
I0512 01:44:54.104493 140600233973760 gin_utils.py:85] # Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:
I0512 01:44:54.104525 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.104556 140600233973760 gin_utils.py:85] sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False
I0512 01:44:54.104589 140600233973760 gin_utils.py:85] 
I0512 01:44:54.104622 140600233973760 gin_utils.py:85] # Parameters for train_script.train:
I0512 01:44:54.104659 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.104693 140600233973760 gin_utils.py:85] train_script.train.checkpoint_cfg = @utils.CheckpointConfig()
I0512 01:44:53.714876 139995375843328 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:53.714909 139995375843328 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:53.714942 139995375843328 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:53.714974 139995375843328 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:53.715007 139995375843328 gin_utils.py:85] 
I0512 01:44:53.715039 139995375843328 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:53.715071 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.715104 139995375843328 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.715137 139995375843328 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:53.715174 139995375843328 gin_utils.py:85] 
I0512 01:44:53.715208 139995375843328 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.715241 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.715274 139995375843328 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.715306 139995375843328 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.715339 139995375843328 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.715372 139995375843328 gin_utils.py:85] 
I0512 01:44:53.715404 139995375843328 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.715439 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.715473 139995375843328 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.715505 139995375843328 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.715538 139995375843328 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.715571 139995375843328 gin_utils.py:85] 
I0512 01:44:53.715603 139995375843328 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.715636 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.715668 139995375843328 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.715701 139995375843328 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.715733 139995375843328 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.715765 139995375843328 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.715798 139995375843328 gin_utils.py:85] 
I0512 01:44:53.715830 139995375843328 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.715862 139995375843328 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.715895 139995375843328 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.715927 139995375843328 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.715960 139995375843328 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.715992 139995375843328 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.716919 139995375843328 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478293.758611  268433 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478293.758663  268433 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478293.758667  268433 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:53.766050 140139156215808 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:53.766081 140139156215808 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:53.766112 140139156215808 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:53.766144 140139156215808 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:53.766176 140139156215808 gin_utils.py:85] 
I0512 01:44:53.766207 140139156215808 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:53.766237 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.766268 140139156215808 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:53.766298 140139156215808 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:53.766329 140139156215808 gin_utils.py:85] 
I0512 01:44:53.766359 140139156215808 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.766395 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.766427 140139156215808 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.766457 140139156215808 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.766488 140139156215808 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.766518 140139156215808 gin_utils.py:85] 
I0512 01:44:53.766549 140139156215808 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.766579 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.766610 140139156215808 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:53.766640 140139156215808 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.766671 140139156215808 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.766701 140139156215808 gin_utils.py:85] 
I0512 01:44:53.766732 140139156215808 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.766762 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.766793 140139156215808 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.766824 140139156215808 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.766855 140139156215808 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.766885 140139156215808 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.766915 140139156215808 gin_utils.py:85] 
I0512 01:44:53.766946 140139156215808 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:53.766976 140139156215808 gin_utils.py:85] # ==============================================================================
I0512 01:44:53.767007 140139156215808 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:53.767037 140139156215808 gin_utils.py:85]     'truncated_normal'
I0512 01:44:53.767068 140139156215808 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:53.767098 140139156215808 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:53.768011 140139156215808 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478293.823319  265473 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478293.823388  265473 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478293.823390  265473 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:54.100662 139867290535936 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:54.100695 139867290535936 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:54.100728 139867290535936 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:54.100760 139867290535936 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:54.100793 139867290535936 gin_utils.py:85] train_script.train.partitioner = @partitioning.MoePjitPartitioner()
I0512 01:44:54.100826 139867290535936 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:54.100858 139867290535936 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:54.100890 139867290535936 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:54.100923 139867290535936 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:54.100955 139867290535936 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:54.100987 139867290535936 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:54.101019 139867290535936 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:54.101051 139867290535936 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:54.101083 139867290535936 gin_utils.py:85] 
I0512 01:44:54.101115 139867290535936 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:54.101147 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101179 139867290535936 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.101212 139867290535936 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:54.101247 139867290535936 gin_utils.py:85] 
I0512 01:44:54.101279 139867290535936 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.101312 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101344 139867290535936 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.101377 139867290535936 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.101409 139867290535936 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.101441 139867290535936 gin_utils.py:85] 
I0512 01:44:54.101480 139867290535936 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.101513 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101545 139867290535936 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.101577 139867290535936 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.101610 139867290535936 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.101642 139867290535936 gin_utils.py:85] 
I0512 01:44:54.101675 139867290535936 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.101707 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101740 139867290535936 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.101772 139867290535936 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.101804 139867290535936 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.101836 139867290535936 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.101868 139867290535936 gin_utils.py:85] 
I0512 01:44:54.101900 139867290535936 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.101933 139867290535936 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.101965 139867290535936 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.101997 139867290535936 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.102030 139867290535936 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.102061 139867290535936 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.103026 139867290535936 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478294.169248  280635 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478294.169309  280635 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478294.169311  280635 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0512 01:44:54.104725 140600233973760 gin_utils.py:85] train_script.train.eval_period = 2500
I0512 01:44:54.104758 140600233973760 gin_utils.py:85] train_script.train.eval_steps = 20
I0512 01:44:54.104789 140600233973760 gin_utils.py:85] train_script.train.infer_eval_dataset_cfg = None
I0512 01:44:54.104822 140600233973760 gin_utils.py:85] train_script.train.model = %MODEL
I0512 01:44:54.104854 140600233973760 gin_utils.py:85] train_script.train.model_dir = %MODEL_DIR
I0512 01:44:54.104885 140600233973760 gin_utils.py:85] train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()
I0512 01:44:54.104919 140600233973760 gin_utils.py:85] train_script.train.random_seed = 42
I0512 01:44:54.104953 140600233973760 gin_utils.py:85] train_script.train.stats_period = 10
I0512 01:44:54.104984 140600233973760 gin_utils.py:85] train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config
I0512 01:44:54.105017 140600233973760 gin_utils.py:85] train_script.train.total_steps = %TRAIN_STEPS
I0512 01:44:54.105049 140600233973760 gin_utils.py:85] train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()
I0512 01:44:54.105081 140600233973760 gin_utils.py:85] train_script.train.train_eval_dataset_cfg = None
I0512 01:44:54.105113 140600233973760 gin_utils.py:85] train_script.train.trainer_cls = @moe_trainer.MoeTrainer
I0512 01:44:54.105144 140600233973760 gin_utils.py:85] train_script.train.use_hardware_rng = %USE_HARDWARE_RNG
I0512 01:44:54.105176 140600233973760 gin_utils.py:85] 
I0512 01:44:54.105208 140600233973760 gin_utils.py:85] # Parameters for trainer.Trainer:
I0512 01:44:54.105239 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.105271 140600233973760 gin_utils.py:85] trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()
I0512 01:44:54.105303 140600233973760 gin_utils.py:85] trainer.Trainer.num_microbatches = None
I0512 01:44:54.105335 140600233973760 gin_utils.py:85] 
I0512 01:44:54.105367 140600233973760 gin_utils.py:85] # Parameters for attention_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.105399 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.105431 140600233973760 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.105463 140600233973760 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.105495 140600233973760 gin_utils.py:85] attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.105527 140600233973760 gin_utils.py:85] 
I0512 01:44:54.105559 140600233973760 gin_utils.py:85] # Parameters for expert_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.105590 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.105622 140600233973760 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'
I0512 01:44:54.105660 140600233973760 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.105694 140600233973760 gin_utils.py:85] expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.105726 140600233973760 gin_utils.py:85] 
I0512 01:44:54.105758 140600233973760 gin_utils.py:85] # Parameters for mlp_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.105790 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.105822 140600233973760 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.105854 140600233973760 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.105885 140600233973760 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.105920 140600233973760 gin_utils.py:85] mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.105953 140600233973760 gin_utils.py:85] 
I0512 01:44:54.105985 140600233973760 gin_utils.py:85] # Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:
I0512 01:44:54.106017 140600233973760 gin_utils.py:85] # ==============================================================================
I0512 01:44:54.106049 140600233973760 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \
I0512 01:44:54.106081 140600233973760 gin_utils.py:85]     'truncated_normal'
I0512 01:44:54.106113 140600233973760 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'
I0512 01:44:54.106144 140600233973760 gin_utils.py:85] output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE
I0512 01:44:54.107063 140600233973760 partitioning.py:559] `activation_partitioning_dims` = 2, `parameter_partitioning_dims` = 1
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715478294.169922  265344 pjrt_api.cc:98] GetPjrtApi was found for tpu at /home/rosinality/openmoe_venv/lib/python3.10/site-packages/libtpu/libtpu.so
I0000 00:00:1715478294.169978  265344 pjrt_api.cc:67] PJRT_Api is set for device type tpu
I0000 00:00:1715478294.169981  265344 pjrt_api.cc:72] PJRT plugin for tpu has PJRT API version 0.30. The framework PJRT API version is 0.30.
I0000 00:00:1715478297.528768  265473 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0000 00:00:1715478297.529714  264715 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.529009 140139156215808 train.py:196] Process ID: 5
I0512 01:44:57.529978 140497406441472 train.py:196] Process ID: 3
I0000 00:00:1715478297.518508  284980 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.518760 139836023437312 train.py:196] Process ID: 7
I0000 00:00:1715478297.541871  264564 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.542132 139861676046336 train.py:196] Process ID: 2
I0000 00:00:1715478297.525645  280635 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.525884 139867290535936 train.py:196] Process ID: 6
I0000 00:00:1715478297.546777  268433 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.546995 139995375843328 train.py:196] Process ID: 4
I0000 00:00:1715478297.550873  291751 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.551118 139933136287744 train.py:196] Process ID: 0
I0000 00:00:1715478297.691126  265344 tfrt_cpu_pjrt_client.cc:349] TfrtCpuClient created.
I0512 01:44:57.691371 140600233973760 train.py:196] Process ID: 1
I0512 01:44:57.862001 140497406441472 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.866972 140139156215808 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.853026 139836023437312 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.876811 139861676046336 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.862225 139867290535936 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.887937 139995375843328 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.869256 139933136287744 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:57.875646 140600233973760 train.py:266] Using seed for initialization and dropout RNG: 42
I0512 01:44:58.165061 140139156215808 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.165437 140139156215808 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.165640 140139156215808 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.165812 140139156215808 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.165868 140139156215808 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.166460 140139156215808 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.166527 140139156215808 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.166572 140139156215808 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.166754 140139156215808 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.176029 140497406441472 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.176437 140497406441472 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.176658 140497406441472 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.178259 139861676046336 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.176854 140497406441472 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.176908 140497406441472 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.178646 139861676046336 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.178849 139861676046336 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.161878 139836023437312 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.162313 139836023437312 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
I0512 01:44:58.177616 140497406441472 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.177683 140497406441472 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.177725 140497406441472 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
W0512 01:44:58.162526 139836023437312 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.179028 139861676046336 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.179110 139861676046336 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.179720 139861676046336 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.179788 139861676046336 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.179830 139861676046336 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.177911 140497406441472 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.162719 139836023437312 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.162775 139836023437312 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.163474 139836023437312 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.163541 139836023437312 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.163584 139836023437312 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.180014 139861676046336 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.163769 139836023437312 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.190452 139995375843328 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.190835 139995375843328 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.191033 139995375843328 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.191210 139995375843328 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.191268 139995375843328 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.191749 139995375843328 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.191813 139995375843328 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.191856 139995375843328 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.192036 139995375843328 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.176217 139933136287744 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.176652 139933136287744 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.176865 139933136287744 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.177036 139933136287744 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.177089 139933136287744 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.177740 139933136287744 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.177806 139933136287744 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.177850 139933136287744 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.179184 140600233973760 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.178034 139933136287744 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:58.180484 140600233973760 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
W0512 01:44:58.180829 140600233973760 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.181072 140600233973760 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.181138 140600233973760 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.181871 140600233973760 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.181941 140600233973760 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.181985 140600233973760 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.184640 139867290535936 partitioning.py:94] For MoE, first construct vanilla T5X (data, model) mesh.
I0512 01:44:58.185111 139867290535936 partitioning.py:151] last device coords : (1, 3, 3, 0)
last local device coords: (1, 1, 0, 0)
I0512 01:44:58.182169 140600233973760 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
W0512 01:44:58.185339 139867290535936 partitioning.py:197] Tiling device assignment mesh by hosts, which may lead to reduced XLA collective performance. To avoid this, modify the model parallel submesh or run with more tasks per host.
I0512 01:44:58.185555 139867290535936 partitioning.py:255] global_mesh axis_names: ('data', 'model')
I0512 01:44:58.185614 139867290535936 partitioning.py:256] global_mesh devices: [[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
  TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
  TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
  TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
 [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
  TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
  TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
  TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
 [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
  TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
  TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
  TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
 [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
  TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
  TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
  TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
 [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
  TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
  TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
  TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
 [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
  TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
  TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
  TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
 [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
  TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
  TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
  TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
 [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
  TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
  TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
  TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]
I0512 01:44:58.186341 139867290535936 partitioning.py:257] global_mesh devices shape: (8, 4)
I0512 01:44:58.186417 139867290535936 partitioning.py:111] Overridden MoE global_mesh axes_names: ('data', 'expert', 'model')
I0512 01:44:58.186461 139867290535936 partitioning.py:114] Overridden MoE global_mesh devices: [[[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)
   TpuDevice(id=2, process_index=0, coords=(0,1,0), core_on_chip=0)
   TpuDevice(id=1, process_index=0, coords=(1,0,0), core_on_chip=0)
   TpuDevice(id=3, process_index=0, coords=(1,1,0), core_on_chip=0)]
  [TpuDevice(id=8, process_index=2, coords=(0,0,1), core_on_chip=0)
   TpuDevice(id=10, process_index=2, coords=(0,1,1), core_on_chip=0)
   TpuDevice(id=9, process_index=2, coords=(1,0,1), core_on_chip=0)
   TpuDevice(id=11, process_index=2, coords=(1,1,1), core_on_chip=0)]
  [TpuDevice(id=16, process_index=4, coords=(0,0,2), core_on_chip=0)
   TpuDevice(id=18, process_index=4, coords=(0,1,2), core_on_chip=0)
   TpuDevice(id=17, process_index=4, coords=(1,0,2), core_on_chip=0)
   TpuDevice(id=19, process_index=4, coords=(1,1,2), core_on_chip=0)]
  [TpuDevice(id=24, process_index=6, coords=(0,0,3), core_on_chip=0)
   TpuDevice(id=26, process_index=6, coords=(0,1,3), core_on_chip=0)
   TpuDevice(id=25, process_index=6, coords=(1,0,3), core_on_chip=0)
   TpuDevice(id=27, process_index=6, coords=(1,1,3), core_on_chip=0)]
  [TpuDevice(id=4, process_index=1, coords=(0,2,0), core_on_chip=0)
   TpuDevice(id=6, process_index=1, coords=(0,3,0), core_on_chip=0)
   TpuDevice(id=5, process_index=1, coords=(1,2,0), core_on_chip=0)
   TpuDevice(id=7, process_index=1, coords=(1,3,0), core_on_chip=0)]
  [TpuDevice(id=12, process_index=3, coords=(0,2,1), core_on_chip=0)
   TpuDevice(id=14, process_index=3, coords=(0,3,1), core_on_chip=0)
   TpuDevice(id=13, process_index=3, coords=(1,2,1), core_on_chip=0)
   TpuDevice(id=15, process_index=3, coords=(1,3,1), core_on_chip=0)]
  [TpuDevice(id=20, process_index=5, coords=(0,2,2), core_on_chip=0)
   TpuDevice(id=22, process_index=5, coords=(0,3,2), core_on_chip=0)
   TpuDevice(id=21, process_index=5, coords=(1,2,2), core_on_chip=0)
   TpuDevice(id=23, process_index=5, coords=(1,3,2), core_on_chip=0)]
  [TpuDevice(id=28, process_index=7, coords=(0,2,3), core_on_chip=0)
   TpuDevice(id=30, process_index=7, coords=(0,3,3), core_on_chip=0)
   TpuDevice(id=29, process_index=7, coords=(1,2,3), core_on_chip=0)
   TpuDevice(id=31, process_index=7, coords=(1,3,3), core_on_chip=0)]]]
I0512 01:44:58.186649 139867290535936 partitioning.py:115] Overridden MoE global_mesh shape: (1, 8, 4)
I0512 01:44:59.061434 139995375843328 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061534 139861676046336 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061510 140497406441472 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061473 140139156215808 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061615 139867290535936 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061586 139836023437312 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061496 140600233973760 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.061503 139933136287744 utils.py:1979] Initializing dataset for task 'mix_ul2_test' with a replica batch size of 48 and a seed of 42
I0512 01:44:59.383780 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.384449 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.393836 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.397645 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.403179 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.401355 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.420079 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.450450 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.597849 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:44:59.624906 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:44:59.629502 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:44:59.643732 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:44:59.667404 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.655470 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:44:59.661950 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:44:59.690598 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.693417 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.679247 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:44:59.699855 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:44:59.720292 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.725322 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.722405 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.742677 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.764815 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:44:59.887734 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:59.949465 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:59.954502 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:59.955517 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:59.956706 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:44:59.980022 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:45:00.005676 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:45:00.056869 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.042477 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_arxiv (gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0)
I0512 01:45:00.123424 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.123379 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.127590 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.156438 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.167300 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.192481 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
I0512 01:45:00.219606 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_arxiv for split train, from gs://rosinality-tpu-bucket/redpajama_arxiv/1.0.0
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.077208 139861676046336 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.093436 139933136287744 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.138032 139836023437312 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.153952 140139156215808 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.180851 140497406441472 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.212148 139995375843328 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.200188 139867290535936 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
WARNING:tensorflow:From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
W0512 01:45:01.255014 140600233973760 deprecation.py:50] From /home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1105: _RandomDataset.__init__ (from tensorflow.python.data.experimental.ops.random_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.random(...)`.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.581258 139933136287744 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.669716 140139156215808 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.713582 139861676046336 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.700114 139836023437312 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.831825 140497406441472 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.840835 139995375843328 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.933706 140600233973760 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
WARNING:tensorflow:From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
W0512 01:45:05.960478 139867290535936 deprecation.py:50] From /home/rosinality/t5x/text-to-text-transfer-transformer/t5/data/preprocessors.py:2902: sample_from_datasets_v2 (from tensorflow.python.data.experimental.ops.interleave_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use `tf.data.Dataset.sample_from_datasets(...)`.
I0512 01:45:06.129783 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.229155 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.233214 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.260693 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.370429 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.377434 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.359850 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:06.425312 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.461838 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:06.463839 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:06.479719 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.521919 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.511144 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.522966 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.567291 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:06.582145 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:06.597351 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:06.626590 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.649461 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.669406 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.653984 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.716546 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:06.729205 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.737900 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:06.746953 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.782461 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.776412 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.794883 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.822720 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.853021 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.858136 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.867983 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.900386 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:06.928301 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.943840 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:06.981582 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:07.003693 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:07.007669 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_c4 (gs://rosinality-tpu-bucket/redpajama_c4/1.0.0)
I0512 01:45:07.081502 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:07.084654 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_c4 for split train[:-2000], from gs://rosinality-tpu-bucket/redpajama_c4/1.0.0
I0512 01:45:08.046817 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.158818 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.176698 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.258756 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.259176 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.339065 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.364089 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:08.426169 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.441392 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.451706 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.496072 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:08.489727 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:08.558090 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:08.561530 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.554773 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.588579 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:08.622120 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.647278 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.647604 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:08.715777 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.711844 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:08.725369 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:08.736748 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:08.773969 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.803044 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.819945 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.838307 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:08.855152 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:08.929981 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.951831 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:08.958645 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:08.969317 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:08.997273 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:09.053205 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:09.065264 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:09.064530 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:09.093666 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:09.095312 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_common_crawl (gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0)
I0512 01:45:09.165431 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:09.197363 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_common_crawl for split train, from gs://rosinality-tpu-bucket/redpajama_common_crawl/1.0.0
I0512 01:45:10.103020 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.235088 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.250476 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.344741 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.335564 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:10.374391 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.392623 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.395556 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.440038 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:10.457708 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:10.485268 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.503153 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.509247 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.526224 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.602602 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:10.613737 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:10.636792 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:10.617227 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.662476 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.673229 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.705792 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.694221 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:10.692574 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.733530 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.728912 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:10.739448 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.755114 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.806759 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.799486 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.813326 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.880292 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.909057 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.923425 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.953184 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.982488 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:10.975284 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:10.997183 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:11.028236 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_github (gs://rosinality-tpu-bucket/redpajama_github/1.0.0)
I0512 01:45:11.048637 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:11.102458 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_github for split train, from gs://rosinality-tpu-bucket/redpajama_github/1.0.0
I0512 01:45:11.980708 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.093820 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.112749 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.193924 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:12.263818 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.259356 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.285222 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.307750 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.308212 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:12.333100 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:12.368461 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.373870 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.396683 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.425167 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.484288 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:12.484966 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.518572 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:12.551270 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.560703 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:12.575542 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.559444 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.589033 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:12.614255 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.630180 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.619917 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.650222 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.652218 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:12.702514 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.695245 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.713973 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.782721 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.797774 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.859029 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.868742 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:12.913969 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.926878 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.955356 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_stackexchange (gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0)
I0512 01:45:12.988328 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:13.004603 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:13.030949 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_stackexchange for split train, from gs://rosinality-tpu-bucket/redpajama_stackexchange/1.0.0
I0512 01:45:13.884906 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:13.978067 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.001660 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.112066 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:14.168635 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.169581 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.190356 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.224150 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:14.230976 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:14.282865 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.310377 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.300779 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.331682 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.379833 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:14.369278 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.411268 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:14.444784 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.426003 139933136287744 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.473029 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.496190 140139156215808 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.497566 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.513437 139836023437312 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.558203 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:14.553666 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:14.576452 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.563484 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:14.624640 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.610493 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.621013 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.631382 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.667251 139995375843328 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.703466 140497406441472 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.740514 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.778918 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.860862 139861676046336 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.847072 139867290535936 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.855881 140600233973760 dataset_builder.py:421] Reusing dataset redpajama_wikipedia (gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0)
I0512 01:45:14.921674 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.938082 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:14.932195 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset redpajama_wikipedia for split train, from gs://rosinality-tpu-bucket/redpajama_wikipedia/1.0.0
I0512 01:45:15.823925 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:15.844973 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:15.910608 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.057962 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.135366 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.166477 139933136287744 dataset_providers.py:1597] Sharding at the data source: 1 of 8
I0512 01:45:16.202959 140139156215808 dataset_providers.py:1597] Sharding at the data source: 7 of 8
I0512 01:45:16.239587 139933136287744 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.273160 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.256738 139836023437312 dataset_providers.py:1597] Sharding at the data source: 8 of 8
I0512 01:45:16.272553 140139156215808 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.270077 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.298103 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.325552 139836023437312 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.410807 139995375843328 dataset_providers.py:1597] Sharding at the data source: 3 of 8
I0512 01:45:16.476239 139995375843328 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.528385 140497406441472 dataset_providers.py:1597] Sharding at the data source: 6 of 8
I0512 01:45:16.592390 140497406441472 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.578392 139933136287744 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:16.601188 139861676046336 dataset_providers.py:1597] Sharding at the data source: 2 of 8
I0512 01:45:16.606714 140139156215808 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:16.610507 140600233973760 dataset_providers.py:1597] Sharding at the data source: 5 of 8
I0512 01:45:16.641958 139836023437312 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:16.649565 139867290535936 dataset_providers.py:1597] Sharding at the data source: 4 of 8
I0512 01:45:16.671736 139861676046336 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.678404 140600233973760 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.693330 139933136287744 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.719698 140139156215808 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.713683 139867290535936 dataset_info.py:540] Load dataset info from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.756888 139836023437312 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.788914 139995375843328 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:16.905594 139995375843328 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:16.919575 140497406441472 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:17.016647 139861676046336 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:17.018784 140600233973760 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:17.039465 140497406441472 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:17.076741 139867290535936 dataset_builder.py:421] Reusing dataset the_stack_dedup (gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0)
I0512 01:45:17.133932 139861676046336 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:17.133137 140600233973760 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:17.197283 139867290535936 logging_logger.py:49] Constructing tf.data.Dataset the_stack_dedup for split train, from gs://rosinality-tpu-bucket/the_stack_dedup/1.0.0
I0512 01:45:18.133361 140139156215808 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.119839 139933136287744 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.243189 139836023437312 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.327293 139995375843328 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.595786 139861676046336 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.602352 140497406441472 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.662434 140600233973760 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:18.758239 139867290535936 feature_converters.py:1406] prefix <_DirectedInterleaveDataset element_spec={'inputs': TensorSpec(shape=(None,), dtype=tf.int32, name=None), 'targets': TensorSpec(shape=(None,), dtype=tf.int32, name=None)}> {'inputs': 1024, 'targets': 1024}
I0512 01:45:19.888152 140139156215808 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:19.888323 140139156215808 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.888386 140139156215808 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.888425 140139156215808 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.888462 140139156215808 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.888497 140139156215808 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.888533 140139156215808 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.928904 139933136287744 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:19.929074 139933136287744 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.929127 139933136287744 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.929167 139933136287744 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.929205 139933136287744 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.929242 139933136287744 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:19.929280 139933136287744 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.039659 139836023437312 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.039968 139836023437312 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.040027 139836023437312 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.040068 139836023437312 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.040105 139836023437312 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.040142 139836023437312 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.040178 139836023437312 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.099786 139995375843328 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.099971 139995375843328 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.100022 139995375843328 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.100061 139995375843328 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.100097 139995375843328 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.100134 139995375843328 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.100172 139995375843328 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386509 139861676046336 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.386712 139861676046336 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386765 139861676046336 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386802 139861676046336 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386839 139861676046336 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386874 139861676046336 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.386910 139861676046336 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449479 140497406441472 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.449678 140497406441472 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449728 140497406441472 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449766 140497406441472 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449801 140497406441472 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449836 140497406441472 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.449877 140497406441472 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.521970 140600233973760 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.522262 140600233973760 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.522321 140600233973760 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.522361 140600233973760 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.522397 140600233973760 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.522432 140600233973760 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.522474 140600233973760 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.675880 139867290535936 dataset_providers.py:2432] The output dataset from seqio.get_dataset has the following features
I0512 01:45:20.676177 139867290535936 dataset_providers.py:2438] feature: decoder_target_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.676238 139867290535936 dataset_providers.py:2438] feature: decoder_input_tokens 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.676277 139867290535936 dataset_providers.py:2438] feature: decoder_loss_weights 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.676313 139867290535936 dataset_providers.py:2438] feature: decoder_segment_ids 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.676349 139867290535936 dataset_providers.py:2438] feature: decoder_positions 	 shape: [48, 2048] 	 dtype: int32
I0512 01:45:20.676391 139867290535936 dataset_providers.py:2438] feature: decoder_causal_attention 	 shape: [48, 2048] 	 dtype: int32
W0512 01:45:21.797181 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.804190 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.816880 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.824379 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.855463 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.858761 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.876639 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.880054 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.904059 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.908365 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.911559 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.933896 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.937043 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.925668 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.930017 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.950279 140139156215808 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.933220 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.955314 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.958473 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:21.971584 139933136287744 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.007328 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.014389 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.004056 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.011268 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.065662 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.068931 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.063248 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.066542 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.114326 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.118620 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.121798 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.121043 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.112297 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.116594 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.119795 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.143821 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.146952 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.160139 139995375843328 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.141968 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.145103 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.143406 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.158374 139836023437312 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.329346 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.331127 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.336573 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.330852 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.388679 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.392030 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.439338 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.443918 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.447232 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.469997 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.473227 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.486773 139861676046336 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.504162 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.511559 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.519638 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.526968 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.564502 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.567868 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.579621 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.582941 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.614616 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.619053 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.622327 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.644924 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.629374 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.648146 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.633776 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.637074 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.661730 140497406441472 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.662740 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.658498 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.659729 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.662950 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.666138 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.676569 140600233973760 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.720171 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.723602 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.770960 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.775518 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:520, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.778826 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:586, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.795221 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:22.801759 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:623, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.805060 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:663, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
W0512 01:45:22.819028 139867290535936 activation_partitioning.py:145] In /home/rosinality/t5x/flaxformer/flaxformer/architectures/t5/t5_architecture.py:678, activation_partitioning_dims was set, but it is deprecated and will be removed soon.
I0512 01:45:22.837544 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:22.835039 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:22.852767 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.014053 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:22.999837 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.048130 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.115309 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.151904 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.334092 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.364260 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.371982 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.433911 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:23.467155 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.531945 140139156215808 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.532913 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.534067 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.535146 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.536334 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.537268 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.538155 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.539251 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.540335 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.541524 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.542605 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.543675 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.544785 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.545948 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.546823 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.547700 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.548761 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.549923 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.551009 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.552079 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.553179 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.554346 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.555422 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.556280 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.557189 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.558297 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.559364 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.560438 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.561534 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.562673 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.563736 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.567805 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.568693 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.569617 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.570508 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.571451 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.572556 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.573664 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.574756 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.576638 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.577758 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.578845 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.579920 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.580862 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:23.582459 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.581773 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.564502 139933136287744 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.582864 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.565578 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.583952 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.566779 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.585139 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.567907 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.586221 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.569164 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.587306 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.570092 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.570976 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.589196 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.572056 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.590322 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.573162 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.591219 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.574331 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.592090 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.593189 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.575401 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.594308 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.576472 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.577581 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.578759 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.579649 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.580525 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.581626 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.582772 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.583836 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.584942 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.586000 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.595385 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.596442 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.597541 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.598666 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.599738 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.600618 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.601526 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.602598 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.587154 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.588245 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:23.589474 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.589174 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.590073 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.591192 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.592260 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.593375 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.594477 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.595609 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.596748 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.600829 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.601724 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.602612 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.603486 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.604433 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.605532 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.606613 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.607688 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.609613 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.610697 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.611768 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.612867 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.613804 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.614711 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.615779 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.616891 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.618010 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.619075 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.620150 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.621755 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.622877 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.623748 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.624640 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.625728 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.626873 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.627964 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.629097 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.630210 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.631350 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:23.652918 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.632485 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.633428 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.634336 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.635409 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.636529 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.637624 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.638687 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.639754 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.640898 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:23.692942 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.603711 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.604812 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.605917 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.606984 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.608091 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.609182 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.610061 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.610931 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.612052 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.613164 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.614233 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.615288 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.616418 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.617518 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.618593 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.619469 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.620401 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.621489 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.622569 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.623636 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.624792 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.625911 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.626973 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.631478 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.632380 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.633330 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.634199 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.635064 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.636137 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.637233 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.638375 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.639445 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.640510 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.641633 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.642754 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.643632 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.644534 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.645640 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.646776 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.647850 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.648926 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.650017 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.651158 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.652225 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.653124 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.653998 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.655127 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.656201 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.657312 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.658393 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.659523 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.660598 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.661701 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.662574 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.663500 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.664621 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.665731 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.666813 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.667925 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.668995 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.670086 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.671157 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.672040 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.672963 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.674047 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.675121 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.676181 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.677346 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.678417 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.679489 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:23.695467 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.680565 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.681902 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.682781 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.683843 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.684954 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.686092 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.687178 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.688250 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.689352 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.690495 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.691366 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.692229 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.693332 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.694469 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.695533 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.696603 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.697699 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.698825 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.702854 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.703759 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.704659 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.705563 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.706446 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.707554 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.708786 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.709876 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.710943 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.712060 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.713148 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.714203 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.715063 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.715996 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.717068 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.718139 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.719206 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.720310 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.721434 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.722492 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.723548 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.724492 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.725411 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.726472 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.727534 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.728664 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.729749 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.730808 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.731874 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.736269 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.737236 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.738106 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.738982 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.739847 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.740923 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.742066 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.743123 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.744211 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.745324 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.746445 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.750896 139995375843328 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.747502 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.748378 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.751869 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.749274 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.753026 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.750406 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.754130 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.751502 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.755308 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.752584 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.756201 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.757101 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.753683 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.758200 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.754792 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.759296 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.755851 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.760480 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.756903 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.757797 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.761555 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.758708 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.762630 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.759767 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.763736 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:23.746167 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.760825 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.764902 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.761932 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.765788 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.766659 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.763044 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.767737 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.764091 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.768910 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.765201 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.769991 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.766268 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.767180 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.771081 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.768045 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.772156 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.769115 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.773341 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.770176 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.642003 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.642885 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.643759 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.644907 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.645986 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.647049 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.648125 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.649266 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.650335 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.774430 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.771280 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.651409 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.652281 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.653230 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.654333 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.655411 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.656469 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.657611 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.658674 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.775306 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.772372 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.659728 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.664348 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.665270 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.666203 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.667078 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.776201 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.667951 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.669036 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.670095 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.671248 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.672358 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.673465 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.674589 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.675721 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.676625 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.677486 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.678539 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.679659 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.680736 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.773458 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.777351 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.774516 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.778437 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.681813 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.682875 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.684009 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.685135 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.686022 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.686895 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.688019 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.689116 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.690196 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.691263 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.692391 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.693481 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.694578 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.695467 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.696394 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.697517 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.698587 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.775622 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.779522 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.776496 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.699663 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.700798 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.701873 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.702949 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.704007 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.704925 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.705840 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.706894 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.707950 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.780628 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.777405 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.709036 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.710159 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.711222 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.712282 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.713376 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.714834 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.715709 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.716839 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.781775 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.778463 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.717943 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.719093 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.720206 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.721302 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.722360 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.723498 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.724361 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.725250 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.726324 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.782896 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.779521 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.727440 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.728506 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.729595 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.730663 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.731775 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.735845 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.736757 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.737596 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.738462 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.780631 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.739307 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.740409 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.741665 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.742718 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.743773 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.744937 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.746025 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.747112 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.781722 140139156215808 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.748009 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.748952 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.750001 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.751054 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.752118 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.782793 140139156215808 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.786999 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.753259 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.754374 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.755433 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.756489 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.757424 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.758284 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.759331 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.760391 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.787890 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.766757 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.788824 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.767962 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.789711 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.769086 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.790673 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.770156 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.791773 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.792884 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.793983 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.795857 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.774823 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.775771 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.796971 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.776670 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.798058 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.777536 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.799144 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.778404 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.800081 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.779459 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.800998 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.780579 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.802086 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.781636 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.803210 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.782678 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.804376 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.783737 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.805473 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.784856 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.806549 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.785911 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.786780 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.808178 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.787651 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.809336 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.788904 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.810230 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.789971 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.811101 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.812198 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.791030 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.794418 139836023437312 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.792075 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.813353 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.795414 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.793209 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.796591 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.794337 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.797693 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.795449 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.798919 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.796332 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.799821 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.797276 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.800722 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.814444 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.815522 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.816639 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.817762 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.818856 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.798333 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.801908 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.799375 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.803018 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.800431 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.804181 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.801592 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.805260 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.802651 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.806379 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.803713 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.807481 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.804793 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.805708 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.808612 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.806553 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.809498 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.810551 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.807612 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.808694 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.811637 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.812781 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.809803 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.813915 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.810909 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.815004 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.811959 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.813039 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.816082 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.817257 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.814141 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.815033 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.818371 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.815906 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.819260 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.816991 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.820156 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.818026 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.821287 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.819130 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.822437 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.820179 139933136287744 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.823531 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.821257 139933136287744 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.824607 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.825783 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.826887 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.831015 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.831933 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.832825 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.833747 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.834719 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.835819 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.836927 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.838068 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.839962 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.841052 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.842217 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.843317 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.844262 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.845160 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.846293 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.847383 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.848527 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.849611 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.850735 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.852457 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.853600 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.854523 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.855436 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.856513 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.857644 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.858760 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.859856 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.860957 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.862152 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.863248 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.864140 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.865024 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.866165 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.867310 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.868410 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.869494 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.870608 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.871745 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.872829 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:23.912705 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.819740 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.820664 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.821762 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.822940 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.824034 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.825147 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.826213 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.827347 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.828476 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.829352 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.830227 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.831361 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.832484 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.833573 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.834669 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.835799 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.836907 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.837986 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.838878 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.839828 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.840929 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.842025 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.843145 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.844282 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.845396 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.846470 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.851000 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.851895 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.852861 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.853754 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.854638 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.855732 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.856845 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.857981 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.859061 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.860145 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.861268 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.862401 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.863318 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.864195 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.865307 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.866453 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.867536 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.868655 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.869731 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.870869 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.871968 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.872884 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.873769 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.874891 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.875966 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.877089 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.878175 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.879307 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.880416 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.881517 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.882382 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.883346 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.884453 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.885533 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.886614 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.887733 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.888836 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.889923 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.890996 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.891891 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.892865 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.893934 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.894994 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.896060 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.897214 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.898280 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.899352 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.900462 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.901774 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.902664 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.903770 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.904871 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.905990 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.907054 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.908123 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.909223 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.910377 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.911257 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.912113 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.913229 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.914333 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.915410 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.916516 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.917589 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.918722 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.922837 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.923732 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.924633 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.925523 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.926598 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.927736 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.929031 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.930095 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.931184 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.932334 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.933414 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.934488 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.935369 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.936318 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.937391 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.938461 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:23.918423 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:23.939520 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.940672 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.941756 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.942856 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.943954 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.944923 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.945804 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.946868 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.947951 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.949095 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.950156 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.951225 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.952280 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.956809 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.957766 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.958643 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.959528 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.960430 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.961505 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.962628 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.963742 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.964835 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.965905 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.967023 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.968085 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.968985 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.969873 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.971006 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.972074 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.973193 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.974266 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.975385 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.976485 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.977551 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.978431 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.979350 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.980472 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.981535 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.982600 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.983742 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.984853 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.985914 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.986968 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.987889 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.988803 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.989883 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.990961 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.992062 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.993197 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.994259 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.995320 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.996469 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.997341 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.873739 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.874635 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.875770 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.876858 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.877976 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.879066 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.880210 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.881289 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.882424 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.998229 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.883319 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.884261 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.885331 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.886483 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.887563 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.888690 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.889818 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.890902 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.999303 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.895443 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.896366 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.897302 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.898231 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.899125 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.900216 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.901300 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.902499 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.903583 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.000396 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.904670 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.905811 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.906951 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.907833 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.908729 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.909841 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.910986 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.912092 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.913177 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.001515 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.914280 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.915413 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.916482 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.917359 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.918282 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.919403 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.920472 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.921602 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.002575 139995375843328 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.922740 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.923878 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.924977 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.926093 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.926987 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.927927 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.929025 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.930132 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.931231 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.003687 139995375843328 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.932360 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.933442 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.934556 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.935620 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.936520 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.937443 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.938545 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.939628 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.940708 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.941908 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.943006 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.944083 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.945165 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.946541 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.947440 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.948523 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.949605 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.950779 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.951885 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.952991 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.954101 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.955254 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.956144 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.957026 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.958148 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.959287 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.960365 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.961445 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.962586 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.963719 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.967801 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:23.968720 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.969609 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.970669 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.971566 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.972687 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.973970 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.975055 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.976137 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.977277 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.978392 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.979470 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.980359 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.981310 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.982447 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.983520 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.984593 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.985737 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.986817 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.987898 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.988975 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.989944 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:23.990822 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.991905 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:23.992981 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.994145 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.995211 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.996299 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:23.997368 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:24.017902 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:24.001903 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.002884 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.003755 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.004654 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.005533 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.006661 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.007796 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.008864 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.009979 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.011050 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.012173 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.013254 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.014168 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.015053 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.016174 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.017254 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.018365 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.019438 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.020557 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.021633 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.022763 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.023638 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.024572 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.025646 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.026733 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.027820 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.028961 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.030063 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.031137 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.032218 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.033168 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.034094 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.035167 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.036236 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.037349 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.038483 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.039556 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.040633 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.041780 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.042695 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.043586 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.044660 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.048858 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.050292 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.051388 139836023437312 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.052471 139836023437312 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:24.079370 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:24.107931 140139156215808 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
W0512 01:45:24.118218 139861676046336 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.119302 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.120489 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.121592 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.122816 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.123779 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.124733 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.125849 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.126969 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.128192 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.129299 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.130391 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.131543 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.132708 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.133614 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.134527 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.135652 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.136826 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.137925 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.139035 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.140156 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.141346 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.142450 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.143368 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.144315 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.145467 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.146567 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.147702 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.148790 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.149966 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.151079 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.155277 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.156203 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.157113 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.158036 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.159018 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.160173 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.161281 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.162406 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.164453 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.165579 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.166683 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.167828 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.168790 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.169697 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.170807 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.171961 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.173124 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.174275 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.175421 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.177394 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.178556 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.179488 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.180386 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:24.161167 139933136287744 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
W0512 01:45:24.181495 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.182637 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.183768 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.184906 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.186028 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.187212 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.188354 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.189266 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.190170 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:24.241040 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:24.247138 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:24.191303 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.192463 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.193573 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.194670 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.195793 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.196942 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.198030 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.198931 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.199863 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.201004 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.202114 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.203250 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.204388 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.205545 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.206636 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.207762 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.208669 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.209617 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.210704 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.211865 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.212962 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.214118 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.215276 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.216381 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.220993 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.221927 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.222892 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.223830 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.224782 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.225891 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.226985 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.228175 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.229258 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.230350 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.231502 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.232656 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.233555 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.234453 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.235571 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.236725 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.237821 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.238919 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.240027 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.241196 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.242301 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.243366 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.244337 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.245486 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.246597 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.247743 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.248848 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.250012 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.251146 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.252255 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.253148 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.254085 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.255239 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.256345 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.257457 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.258605 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.259761 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.260868 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.261974 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.262905 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.263918 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.265017 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.266122 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.267283 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.268451 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.269551 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.270654 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.271786 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.273163 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.274047 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.275214 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.276313 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.277454 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.278549 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.279677 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.280802 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.282055 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.283038 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.284056 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.285261 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.286506 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.287736 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.288938 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.290115 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.291393 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.295879 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.296846 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.297821 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.298802 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.299816 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.301065 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.302445 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.303631 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.304785 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.305933 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.307026 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.308164 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.309046 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.310005 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.311118 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.312220 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.313318 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.314456 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.315578 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.316668 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.317768 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.318715 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.319638 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.320724 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.321815 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.322945 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.324090 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.325188 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.326278 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.330859 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.331874 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.332776 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.333678 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.334570 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.335708 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.336858 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.337934 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.339021 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.340123 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.341292 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.342380 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.341955 140497406441472 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.343307 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.342997 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.344249 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.344187 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.345389 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.345309 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.346484 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.346515 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.347615 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.347469 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.348711 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.348374 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.349855 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.349495 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.350954 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.350628 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.352082 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.351843 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.352971 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.353916 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.352988 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:24.355678 139995375843328 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
W0512 01:45:24.355010 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.354089 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.356114 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.355235 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.357215 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.356395 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.357305 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.358368 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.358205 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.359490 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.359332 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.360589 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.360509 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.361696 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.362632 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.361623 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.363552 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.362732 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.364671 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.363855 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.365763 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.365323 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.348185 140600233973760 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.366890 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.366432 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.349236 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.368043 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.367361 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.350419 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.369138 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.368273 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.351545 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.369422 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.370233 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.352784 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.371394 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.370530 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.353700 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.372291 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.371671 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.373194 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.354613 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.372791 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.374292 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.355793 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.373936 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.375449 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.356914 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.375026 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.358090 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.376591 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.377669 139861676046336 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.359197 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.378764 139861676046336 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.360339 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.361440 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.379214 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.380131 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.362586 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.363487 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.381047 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.364404 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.381953 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.382938 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.365498 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.384083 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.366664 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.385192 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.367821 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.386344 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.368957 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.370064 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.388599 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.371259 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.389812 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.372393 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.373301 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.391015 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.374223 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.392234 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.375412 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.393327 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.376548 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.394316 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.377669 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.395575 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.378773 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.396769 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.380003 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.397940 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.381127 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.399062 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.400189 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.402079 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.385337 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.403270 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.386285 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.404162 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.405070 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.387219 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.388168 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.406160 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.389157 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.407487 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:24.391881 139836023437312 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
W0512 01:45:24.390276 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.408591 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.391384 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.409686 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.392533 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.410791 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.412005 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.394646 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.413150 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.395841 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.414047 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.396959 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.414950 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.398111 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.416075 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.399126 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.400089 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.401219 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.402353 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.403524 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.404669 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.405779 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.407938 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.409138 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.410053 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:24.412340 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:24.410968 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.412103 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.413250 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.414368 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.415521 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.416669 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.417839 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.418957 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.419889 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.420782 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.421902 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.417252 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.418365 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.419501 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.420614 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.421781 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.422875 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.423812 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.424719 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.425864 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.426975 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.428098 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.429198 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.430354 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.431493 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.432633 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.433539 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.434503 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.435635 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.436759 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.437863 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.439027 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.440172 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.441277 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.445838 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.446769 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.447758 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.448671 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.449568 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.450680 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.451806 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.453012 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.454112 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.455242 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.456353 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.457505 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.458400 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.459321 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.460416 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.461590 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.462685 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.463819 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.464909 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.466059 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.467170 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.468063 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.468960 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.470102 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.471214 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.472328 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.473466 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.474616 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.475754 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.476847 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.477749 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.478686 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.479822 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.480912 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.482026 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.483212 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.484308 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.485406 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.486491 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.487437 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.488390 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.489478 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.490560 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.491673 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.492851 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.493936 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.495030 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.496175 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.497526 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.498428 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.499568 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.500676 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.501829 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.502928 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.504068 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.505171 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.506328 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.507254 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.508144 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.509264 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.510411 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.511566 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.512699 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.513812 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.514965 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.519122 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.520051 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.520950 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.521852 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.522746 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.523921 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.525213 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.526301 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.527426 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.528570 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.529661 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.514380 139867290535936 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.530747 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.515495 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.531673 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.516713 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.532657 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.517854 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.533739 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.519129 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.534820 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.520065 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.535942 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.521001 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.537082 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.522150 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.538176 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.523333 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.539292 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.524549 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.540374 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.525687 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.541314 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.542202 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.526862 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.543329 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.528013 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.544426 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:24.543553 140139156215808 utils.py:1065] Initializing parameters from scratch.
W0512 01:45:24.529217 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.530138 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.545580 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.531102 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.546656 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.532243 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.547764 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.533434 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.548849 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.534596 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.535772 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.536908 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.553376 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.538129 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.423053 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.424203 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.425516 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.426756 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.427957 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.429063 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.429980 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.430886 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.432055 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.554343 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.539323 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.433165 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.434265 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.435412 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.436599 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.437701 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.438817 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.439745 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.440717 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.441809 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.555274 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.540240 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.442924 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.444048 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.445208 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.446318 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.447411 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.452070 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.453012 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.453983 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.556163 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.541166 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.454897 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.455862 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.457005 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.458095 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.459263 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.460382 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.461480 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.462594 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.463782 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.557047 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.542352 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.558136 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.464679 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.465571 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.466675 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.467855 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.468964 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.470064 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.471161 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.472347 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.473463 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.543522 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.474364 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.475306 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.476494 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.477610 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.478740 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.479887 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.481055 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.482161 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.559309 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.483263 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.484184 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.485136 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.486244 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.544668 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.487347 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.488492 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.489656 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.490764 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.491921 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.560400 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.493039 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.493965 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.494929 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.496090 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.497202 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.498316 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.499459 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.500580 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.501676 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.545792 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.561502 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.547014 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.502790 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.504165 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.505074 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.506174 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.507268 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.508464 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.509575 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.510679 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.562593 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.548170 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.563749 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.511821 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.512984 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.513897 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.514801 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.515996 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.517146 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.518250 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.519351 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.520463 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.564848 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.521649 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.525815 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.526723 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.527643 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.528532 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.529417 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.530565 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.531850 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.532966 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.534068 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.535266 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.565731 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.566619 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.536388 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.537496 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.538401 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.539351 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.540487 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.541586 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.542677 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.543884 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.544986 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.546082 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.547179 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.548158 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.549054 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.567799 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.552462 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.550158 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.553416 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.568883 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.551255 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.554380 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.552435 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.569965 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.555352 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.553519 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.571055 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.556350 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.554632 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.572204 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.557505 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.555813 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.573326 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.558652 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.574410 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.575331 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.559970 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.576266 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.577352 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.561936 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.560599 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.578427 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.563107 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.561599 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.564247 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.579567 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.562486 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.565377 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.580703 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.563386 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.566365 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.564308 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.581782 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.567335 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.565426 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.582873 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.568482 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.566581 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.583993 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.584934 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.567694 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.569623 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.585824 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.570846 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.568814 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.586901 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.571990 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.569900 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.588006 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.573123 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.571039 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.589133 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.572167 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.590250 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.575177 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.573065 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.591362 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.573976 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.576359 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.577283 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.592483 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.575161 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.578191 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.593614 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.576276 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.594498 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.579347 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.577369 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.595426 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.578464 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.596519 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.579630 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.597596 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.580728 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.598743 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.581815 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.599860 140497406441472 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.582703 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.600943 140497406441472 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.583671 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.584782 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.585863 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.586961 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.588145 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.589263 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.590353 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.591452 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.592419 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.593309 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.594397 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.595512 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.580501 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.581629 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.582774 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.583906 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.585092 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.586242 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.587205 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.588135 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.596678 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.597806 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.598891 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.601643 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.602831 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.603750 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.604642 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.605737 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.606836 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.608023 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.609113 140600233973760 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.610206 140600233973760 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:24.609995 139933136287744 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:24.716420 139861676046336 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
W0512 01:45:24.589302 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.590494 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.591675 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.592804 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.593928 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.595169 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.596307 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.597235 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.598166 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.599374 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.600530 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.601657 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.602816 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.604010 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.605133 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.606287 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.607246 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.608224 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.609339 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.610490 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.611658 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.612841 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.613980 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.615177 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.619866 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.620826 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.621804 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.622772 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.623722 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.624862 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.626002 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.627239 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.628367 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.629492 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.630636 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.631841 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.632767 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.633689 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.634874 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.636066 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.637205 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.638330 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.639491 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.640678 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.641823 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.642773 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.643720 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.644916 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.646050 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.647242 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.648370 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.649557 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.650713 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.651853 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.652783 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.653769 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.655126 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.656260 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.657405 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.658572 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.659730 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.660883 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.662024 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.663011 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.663994 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.665131 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.666271 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.667443 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.668631 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.669760 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.670908 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.672044 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.673457 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.674419 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.675587 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.676742 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.677931 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.679106 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.680246 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.681379 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.682923 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.683867 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.684794 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.685929 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.687135 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.688280 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.689424 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.690572 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.691800 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.696154 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.697104 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.698022 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.698990 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.699920 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.701098 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.702419 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.703583 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.704717 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.705895 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.707052 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.708186 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.709103 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.710083 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.712585 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.713735 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.714941 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.716113 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.717247 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.718367 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.719531 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.720499 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.721418 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.722557 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.723706 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.724879 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.725991 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.727175 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.728294 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.733038 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:24.734056 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.735048 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.735984 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.736903 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.738040 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.739261 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.740368 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.741487 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.742605 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.743807 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.744933 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.745855 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.746819 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.748009 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.749119 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.750248 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.751409 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.752583 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.753698 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.754877 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.755801 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.756764 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.757903 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.759048 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.760201 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.761378 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.762514 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.763675 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.764825 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.765794 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.766745 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.767874 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.769028 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.770218 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.771414 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.772540 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.773681 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.774935 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.775862 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:24.776801 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.777936 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:24.779095 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.780299 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.781420 139867290535936 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:24.782544 139867290535936 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:24.805419 139995375843328 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:24.850699 139836023437312 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:24.953905 140497406441472 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
I0512 01:45:24.953319 140600233973760 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
I0512 01:45:25.135614 139867290535936 utils.py:947] No checkpoints found in specified directory: gs://rosinality-tpu-bucket/openmoe_8b_ul2/training
I0512 01:45:25.165412 139861676046336 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:25.294290 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.350597 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.397665 140497406441472 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:25.407389 140600233973760 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:25.561914 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.590379 139867290535936 utils.py:1065] Initializing parameters from scratch.
I0512 01:45:25.618082 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.777552 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.799058 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:25.942656 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.050373 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.125708 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.132508 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.184072 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.191477 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.206763 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:26.382012 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.412158 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:26.468240 140139156215808 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.470678 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
I0512 01:45:26.491712 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:45:26.545957 139933136287744 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.572999 140139156215808 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.574022 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.575139 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:26.576228 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.577457 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.578336 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.579218 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.580307 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.581438 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.582607 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.583675 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.584763 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.585863 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.587004 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.587882 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.588762 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.589862 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.591002 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.592083 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.593183 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.594260 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.595405 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.596482 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.597380 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.598255 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.599366 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.600433 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.601536 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.602599 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.604265 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.605410 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.609489 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.610375 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.611254 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.612129 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.613082 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.614154 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.615216 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.616286 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.617513 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.618588 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.619662 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.620733 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.621706 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.622584 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.623658 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.624748 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.625908 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.626981 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.628044 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.629129 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.630252 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.631119 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.631981 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.633073 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.634187 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.635257 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.636321 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.638277 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.639389 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.640444 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.641333 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.642203 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.643248 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.644357 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.645476 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.646531 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.647584 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.648697 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.649790 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.650680 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.651565 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.652683 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.653774 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.654840 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.655891 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.657520 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.658579 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.659630 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.660494 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.661432 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:26.662503 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.663767 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.665115 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.666430 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.668046 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.650451 139933136287744 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.651520 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.652743 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.671192 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.653879 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.655085 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.655980 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.656904 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.657994 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.675940 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.659077 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.677003 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.660227 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.678509 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.661375 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.679800 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.662491 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.681376 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.663597 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.664801 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.682746 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.665714 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.684026 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.666624 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.667742 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.685896 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.668951 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.687106 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.670067 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.688436 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.671184 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.689864 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.672303 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.691072 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.673527 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.692075 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.674665 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.693014 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.675572 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.694282 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.676481 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.677678 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.695429 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.678798 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.696578 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.679901 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.697694 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:26.680961 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.699011 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.700140 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.701419 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.683954 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.702289 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.703166 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.685966 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:26.704305 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.704315 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.687090 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.705474 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.706576 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.707636 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.708762 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.691306 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.692232 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.709867 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.693142 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.710947 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.694026 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.711840 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.694989 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.712780 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.696054 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.713884 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.697164 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.715032 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.698232 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.716127 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.699470 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.717913 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.700636 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.719008 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.701756 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.720090 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.702858 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.721220 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.703819 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.722129 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.704746 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.723113 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.705850 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.724198 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.706947 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.725425 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.708093 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.726528 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.727993 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.709217 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.710336 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.729156 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.711451 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.730235 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.712623 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.713514 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.731467 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.714444 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.732409 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.715545 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.733332 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.716709 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.734575 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.717813 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.735650 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.718926 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.736779 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.720017 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.737883 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.721333 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.738954 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.722461 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.740031 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.723372 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.741246 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.724260 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.742125 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.743002 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.725347 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.744106 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.726480 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.727546 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.745334 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.728640 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.746416 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.747512 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.729706 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.730825 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.748574 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.749759 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.731898 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.732828 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.733733 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.734917 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.736017 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.753882 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.737157 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.754802 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.755702 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.738268 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.756595 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.739862 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.757507 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.741009 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.758651 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.742106 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.759738 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.743009 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.760806 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.743956 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.761929 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.745092 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.763072 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.746207 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.764174 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.747340 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.765349 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.766239 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.748489 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.767172 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.749616 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.768234 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.750749 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.769354 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.770428 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.772302 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.754988 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.773428 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.755913 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.774494 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.756895 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.757803 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.775556 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.758709 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.776494 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.777396 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.759809 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.760958 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.778453 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.779515 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.762064 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.780628 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.763123 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:26.766703 139836023437312 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.764177 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.781716 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.765267 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.782777 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.766366 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.783829 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.767225 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.768094 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.769186 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.787883 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.770300 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.788873 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.771369 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.789787 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.772429 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.790668 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.773503 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.791535 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.774631 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.792639 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.775695 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.793778 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.776592 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.794837 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.777623 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.795898 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.778836 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.796988 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.779909 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.798119 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.780996 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.799170 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.782052 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.800030 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.800892 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.783148 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.802026 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.784188 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.803082 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.785256 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.786102 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.804129 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.786998 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.805244 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.788038 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.806353 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.789119 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.807424 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.790171 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.808479 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.791670 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.809372 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.810286 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.792744 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.811332 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.793796 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.812386 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.794868 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.795731 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.813479 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.796665 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.814579 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:26.797706 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.815629 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.798763 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.816696 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.799823 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.817776 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.818693 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.801127 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.819559 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.802924 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.820611 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.804008 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.821688 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.805237 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.823165 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.824226 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.806996 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.825364 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.807962 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.826464 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.809485 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.827663 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.828541 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.810892 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.829526 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.812310 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.830615 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.813667 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.831787 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.832928 140139156215808 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.815265 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.834228 140139156215808 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.816928 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.835332 140139156215808 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:26.839068 139995375843328 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.818333 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.819433 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.820466 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.821654 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.822853 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.824026 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.825240 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.826658 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.827872 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.832778 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
I0512 01:45:26.853233 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.833941 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.834971 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.835947 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.836969 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.838227 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.839304 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.840386 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.841484 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.842598 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.843651 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.844743 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.845757 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.846735 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.847805 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.848911 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.849968 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.851561 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:26.854790 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:26.852646 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.853705 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.854788 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.855699 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.856574 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.857870 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.859307 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.860733 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.861984 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.863215 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.864604 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.869407 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.870346 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.873537 139836023437312 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.871215 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.874591 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.872079 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.875741 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.872982 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.876843 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.874201 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.878082 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.875327 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.878999 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.876400 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.879888 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.877477 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.880988 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.878544 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.882171 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.879644 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.883346 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.880728 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.881577 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.884446 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.882446 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.885578 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.883538 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.886690 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.884601 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.887853 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.888747 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.885635 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.889641 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.886680 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.887770 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.890755 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.891929 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.888841 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.889878 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.893027 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.890745 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.894167 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.891663 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.895270 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.896441 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.W0512 01:45:26.897545 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.

W0512 01:45:26.895593 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.898474 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.899364 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.896734 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.900507 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.897818 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.901612 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.898959 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.902777 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.900041 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.901149 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.903879 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.902244 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.905608 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.903176 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.906749 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.904057 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.905184 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.906261 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.907788 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.910894 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.911789 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.908915 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.912687 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.910002 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.913581 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.911085 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.914599 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.912220 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.915706 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.913138 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.916800 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.914036 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.917926 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.915163 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.919198 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.916249 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.920285 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.917411 139933136287744 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.921379 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.918487 139933136287744 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.922534 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.919571 139933136287744 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.923486 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.924375 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.925464 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.926578 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.927713 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.946407 139995375843328 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.928798 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.947461 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.929915 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.948651 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.930998 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.949762 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.932137 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.950996 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.933022 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.952068 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.933940 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.953035 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.935043 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.954407 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.936174 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.955603 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.937257 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.938371 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.956839 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.939465 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.957962 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.940585 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.959066 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.960164 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.941669 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.942610 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.961366 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.943493 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.962260 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.944562 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.963182 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:26.945697 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.964348 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.946809 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.965597 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.947915 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.966693 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.949003 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.967780 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.968912 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.970065 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.952353 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.971177 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.953573 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.972112 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.973044 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.955243 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.974205 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.956147 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.975294 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.957318 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.976426 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.958925 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.977541 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.960036 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.961425 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.979612 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.980778 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.963518 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.965299 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.984996 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:26.966709 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.985916 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.968599 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.986812 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.987704 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.969647 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.988693 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.971041 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.989788 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.972262 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.990863 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.973739 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.991936 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.975057 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.993419 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.994615 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.976599 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.995713 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.977834 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.996874 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.997817 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.998720 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:26.999808 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.982475 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.000922 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.983392 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.002050 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.984418 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.003168 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.985324 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.004245 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.986299 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.005358 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.987402 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.006498 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.988516 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.007357 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.989670 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.008242 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.990814 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.009348 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.991985 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.010478 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.993104 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.011548 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.994287 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.012646 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.995203 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.013714 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.996083 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.014838 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:26.997197 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.015910 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:26.998397 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.016817 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.017698 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:26.999490 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.018767 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.000609 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.019895 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.001726 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.021008 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.002917 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.022115 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.004016 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.004905 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.023240 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.005815 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.024392 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.006972 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.025480 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.008058 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.026350 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.027234 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.009151 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.028401 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.010300 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.029473 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.011498 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.030542 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.012595 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.031623 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.013676 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.014728 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.033403 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.015658 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.034478 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.017032 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.035554 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.018180 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.036468 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.037382 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.019364 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.038455 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.020984 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.039520 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.022222 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.040630 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.023334 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.041756 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.024423 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.042855 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.025326 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.043922 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.026308 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.027397 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.028506 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.029659 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.048008 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.049040 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.031156 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.050209 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.032282 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.051348 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.033616 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.052537 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.034765 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.035728 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.053958 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.036621 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.055466 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.037957 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.056952 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.039068 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.040236 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.058393 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.041327 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.059800 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:27.039721 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.042582 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.061152 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.043697 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.062433 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.044848 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.063477 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.045774 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:27.061341 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.064667 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.046661 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.047782 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.066104 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.048940 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.067532 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.050069 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.068816 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.051178 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.069934 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.052291 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.071007 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.053442 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.072321 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.073448 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.074326 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.075214 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.057576 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.058515 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.076655 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.059404 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.078191 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.060308 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.079284 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.061196 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.080430 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.062431 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.081570 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.063529 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.064600 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.083032 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.065677 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.084316 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.066845 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.085779 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.067939 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.086803 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.069023 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.069940 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.088441 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.070881 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.089617 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.071966 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.090930 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.073051 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.074180 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.093229 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.075697 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.094415 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.076779 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.095736 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.077884 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.096858 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.078963 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.097949 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.079911 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.098901 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.080783 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.100049 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.081942 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.101190 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.083033 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.084185 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.102294 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.085272 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.103718 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.086515 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.104901 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.087705 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.105982 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.107071 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.108020 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.108940 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.110030 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.092051 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.093029 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.111098 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.112225 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.093988 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.094863 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.113528 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.096073 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.114789 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.097205 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.116054 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.098392 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.099479 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.117883 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.118804 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.100589 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.119692 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.101694 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.103549 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.122542 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.104638 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.105512 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.123729 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.106472 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.125309 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.107599 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.126488 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.108685 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.127706 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.109794 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.128936 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.110911 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.112135 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.113226 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.114366 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.115268 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.134194 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.116798 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.135880 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.118464 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.140068 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.122374 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.141149 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.123471 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.142161 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.124619 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.143584 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.125736 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.144903 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.127004 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.146128 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.128500 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.147383 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.129634 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.148731 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.130723 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.149964 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.132080 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.151198 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.152192 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.153276 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.154514 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.136967 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.155742 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.138514 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.157000 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.139628 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.140730 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.159126 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.141880 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.160380 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.143069 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.161607 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.143966 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.162862 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.144862 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.163937 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.146039 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.164995 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.147120 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.166230 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.148282 139836023437312 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.167492 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.149378 139836023437312 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.168838 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.150522 139836023437312 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.170098 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.171358 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.172673 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.177755 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.178875 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.179878 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.180917 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.181933 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.183241 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.184569 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.185798 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.187010 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.188236 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.189556 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.190774 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.191768 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.192809 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.194070 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.195303 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.196579 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.197802 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.199075 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.200346 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.201604 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.202627 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.203772 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.205103 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.206865 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.208094 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.209470 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.210697 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.211925 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.213185 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.214373 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.215364 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:27.214555 139861676046336 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.216797 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.218125 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.220815 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.222512 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.223867 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.225362 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.226739 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.228226 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.229263 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.230777 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.231993 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.233920 139995375843328 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.235140 139995375843328 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.237212 139995375843328 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:27.226972 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.320984 139861676046336 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.322033 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.323223 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.324400 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.325639 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.326570 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.327526 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.328768 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.329994 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.331346 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.332576 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.333790 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.335009 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.336318 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.337296 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.338279 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.339532 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.340796 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.342025 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.343262 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.344427 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.345601 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.346711 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.347684 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.348598 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.349766 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.350869 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.352008 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.353124 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.354884 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.356045 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.360272 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.361198 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.362113 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.363025 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.364267 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.365403 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.366519 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.367651 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.368905 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.370015 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.371128 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.372248 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.373214 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.374121 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.375256 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.376363 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.377529 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.378644 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.379777 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.380869 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.382022 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.382911 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.383838 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.384972 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.386107 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.387228 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.388314 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.389409 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.390557 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.391688 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.392595 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.393501 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.394614 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.395807 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.396916 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.398033 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.399155 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.400306 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.401403 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.402292 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.403215 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.404409 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.405502 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.406598 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.407731 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.409379 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.410489 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.411625 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.412519 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
I0512 01:45:27.393864 140600233973760 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.413462 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.414552 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.415676 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.416940 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.418081 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
I0512 01:45:27.417209 140497406441472 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.419202 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.420318 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.424594 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.425518 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.426462 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.427378 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.428277 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.429371 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.430470 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.431641 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.432747 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.433846 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.434959 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.436135 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.437034 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.437922 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.439001 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.440178 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.441267 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.442357 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.443493 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.444683 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.445776 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.446684 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.447633 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.448800 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.449918 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.451031 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.452182 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.453368 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.454516 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.455703 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.456650 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.457647 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.458890 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.460301 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.462266 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.464422 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.465793 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.467204 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.468433 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.469514 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.470535 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.472013 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.473265 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.474581 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.477604 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.479080 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.480511 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.481749 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.483059 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.484063 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.485241 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.486380 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.487609 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.488760 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.489928 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.491041 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.492297 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.493251 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.494314 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.495493 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.496665 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.497820 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.498969 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.500141 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.501331 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.505638 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.506593 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.507561 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.508540 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.509460 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.510640 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.511854 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.512972 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.514124 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.515374 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.516501 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.517636 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.518575 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.499721 140600233973760 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.500760 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.519600 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.501912 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.520732 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.503044 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.521843 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.504303 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.522961 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.505247 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.506173 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.524796 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.524179 140497406441472 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.507294 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.525928 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.525252 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.508442 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.527058 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.526427 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.528199 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.509622 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.527595 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.529174 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.510725 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.528844 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.530087 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.511880 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.529761 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.531232 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.512983 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.530681 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.532336 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.514151 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.531841 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.533507 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.515069 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.532988 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.516034 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.534620 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.534175 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.517123 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.535773 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.535323 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.518282 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.536871 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.536432 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.519384 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.537541 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.520499 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.538708 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.521589 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.539647 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.540968 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.522750 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.540552 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.541958 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.523884 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.541657 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.542867 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.524776 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.543807 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.542840 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.525668 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.544751 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.543977 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.526813 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.545939 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.545076 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.527955 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.547115 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.546169 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.529071 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.548238 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.547362 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.530169 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.549344 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.548471 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.550459 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.531980 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.549383 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.550302 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.551653 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.533145 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.551532 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.552760 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.553660 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.552712 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.554565 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.553847 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.555748 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.537513 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.555074 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.556854 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.538468 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.557947 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.556973 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.539398 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.540369 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.559043 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.558221 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.541362 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.560231 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.542480 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.561338 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.543612 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.562444 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.544724 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.563390 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.563171 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.564380 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.545944 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.564240 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.565486 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.547060 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.565341 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.566580 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.567707 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.567011 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.549794 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.568853 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.550985 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.568735 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.569952 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.552022 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.571061 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.570144 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.552971 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.572205 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.554133 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.571683 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.573159 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.555323 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.572925 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.574056 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.556575 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.575203 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.574332 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.557735 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.576307 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.575615 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.558846 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.577850 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.576847 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.559992 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.578951 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.578094 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.561151 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.580098 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.579357 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.562061 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.581228 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.580394 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.562978 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.582381 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.564121 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.581686 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.583337 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.565273 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.582970 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.584444 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.566391 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.584374 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.585590 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.567515 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.586708 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.585687 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.568649 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.588084 139861676046336 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.586971 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.569814 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.588117 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.589327 139861676046336 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.570938 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.571880 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.589522 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.590561 139861676046336 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.572788 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.590530 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.573888 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.591647 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.575048 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.592813 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.576205 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.593975 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.577300 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.595120 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.578397 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.596235 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.579542 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.597333 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.580668 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.598473 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.581561 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.599699 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.582453 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.600828 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.583619 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.601920 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.584723 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.585817 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.603281 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
I0512 01:45:27.588124 139867290535936 moe_layers.py:777] Selected group_size=4096 and num_groups=192 for input num_tokens=786432, max_group_size=4096, num_experts=8 and num_expert_replicas=1
W0512 01:45:27.586928 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.604658 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.605993 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.588630 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.607200 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.589726 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.608291 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.590828 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.591772 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.609426 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.592722 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.610525 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.593809 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.611465 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.594913 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.612397 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.596088 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.613545 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.597226 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.614629 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.615755 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.598321 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.599418 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.616835 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.618791 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.619914 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.620993 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.603596 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.621877 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.604513 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.622813 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.605457 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.606368 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.623925 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.607268 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.625011 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.608399 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.626111 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.609498 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.627367 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.610651 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.628458 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.611784 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.629541 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.612883 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.613989 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.615157 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.616090 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.633673 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.616984 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.634593 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.635566 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.618093 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.636465 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.619237 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.637348 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.620380 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.638436 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.621476 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.639566 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.622581 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.640729 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.623818 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.641833 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.624966 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.625896 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.642942 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.626837 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.644071 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.645213 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.628088 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.646120 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.629239 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.647014 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.630359 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.648148 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.631453 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.649296 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.632617 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.650384 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.633729 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.651509 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.634814 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.652651 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.635752 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.653808 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.636708 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.654908 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.637790 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.655857 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.638907 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.656770 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.640040 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.657931 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.659021 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.641575 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.642682 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.660153 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.661259 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.643830 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.644943 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.662416 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.645836 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.663557 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.646799 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.664664 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.647931 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.665590 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.666547 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.649025 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.667681 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.650141 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.668800 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.651504 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.669909 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.652639 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.653719 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.671498 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.654811 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.672648 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.655801 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.673768 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.656694 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.674890 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.657779 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.675832 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.658880 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.676795 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.660059 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.677899 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.661151 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.679011 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.662251 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.680139 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.663345 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.681548 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.664508 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.682663 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.665412 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.683806 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.666304 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.684914 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.667396 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.685861 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.668590 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.686777 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.669690 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.687905 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.670811 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.689015 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.671954 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.690161 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.673111 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.691349 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.692500 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.693610 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.677280 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.694979 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.678198 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.695919 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.696815 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.679100 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.680015 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.697911 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.680913 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.699054 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.682058 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.700173 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.683153 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.701302 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.684273 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.702386 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.685367 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.703581 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.686529 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.687661 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.689019 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.689913 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.707690 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.690859 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.708609 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.692003 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.709504 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.710399 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.693096 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.711324 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.694211 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.712503 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.695819 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.713606 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.698659 139867290535936 adafactor.py:358] Since rank of parameter decoder/decoder_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.696925 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.714705 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.699771 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.698022 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.715827 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.700959 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.699118 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.717293 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.702100 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.700105 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.701007 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.718636 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.703388 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.702098 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.704338 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.719980 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.705276 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.703199 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.721076 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.706434 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.704361 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.722253 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.707622 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.705457 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.723627 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.706542 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.708843 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_0/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.724815 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.707657 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.709983 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.725942 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.711159 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.727684 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.712289 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.728817 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.713490 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.711774 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.729917 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.714463 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.712741 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.715464 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.731017 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.713638 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.716614 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.732012 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.714533 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.732948 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.715461 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.717841 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.734033 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.716602 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.719023 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_1/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.735325 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.717745 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.720156 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.718839 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.721299 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.736676 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.719961 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.722496 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.737941 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.721066 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.739170 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.723718 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.722197 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.724705 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.740259 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.723292 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.725854 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.724217 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.725109 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.727313 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.726238 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.728805 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.744494 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.727333 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.729966 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_10/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.745511 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.728446 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.746404 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.731276 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.729531 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.747344 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.748256 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.730652 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.733637 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.749462 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.731782 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.732881 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.735081 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.750631 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.733777 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.751796 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.734726 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.752925 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.735881 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.754026 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.736969 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.739573 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.755198 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.738058 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.740554 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.756303 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.739202 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.741491 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.757192 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.742450 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.740326 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.758084 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.743582 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.741420 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.759277 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.744736 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.742514 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.760375 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.743454 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.745882 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.761481 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.744381 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.747055 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_11/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.762581 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.745477 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.748334 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.763785 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.746571 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.749492 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.764879 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.748126 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.750635 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.765972 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.766874 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.749219 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.767866 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.750313 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.769042 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.751418 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.752579 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.770138 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.753472 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.771271 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.754362 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.772459 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.755488 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.773561 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.756600 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.774683 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.757734 140600233973760 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.775832 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.758844 140600233973760 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.776788 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.777702 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.760667 140600233973760 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.778798 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.779954 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.781603 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.782754 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.783927 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.785036 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.786188 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.787125 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.788066 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.789334 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.777123 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.792922 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.778258 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.794374 140497406441472 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.779241 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.795741 140497406441472 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.780402 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.796903 140497406441472 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.781549 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.782775 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_12/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.783910 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.785044 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.786187 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.787461 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.788379 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.789330 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.790486 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.791814 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.792963 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_13/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.794144 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.795384 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.796602 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.797749 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.798683 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.799658 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.800790 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.801987 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.803145 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_14/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.804274 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.805414 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.806605 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.807749 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.808668 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.809601 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.811002 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.813281 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.816413 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_15/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.817792 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.819755 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.820882 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.822061 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.823032 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.824107 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.825277 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.826930 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.828276 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_16/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.832811 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.834261 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.835761 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.840858 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.841935 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.843147 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.844227 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.845328 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.846613 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.847941 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.849289 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_17/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.850568 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.851878 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.853172 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.854540 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.855621 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.856672 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.857946 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.859302 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.860585 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_18/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.861868 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.863198 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.864543 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.865833 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.866909 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.867970 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.869328 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.870587 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.871923 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_19/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.873185 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.874591 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.875919 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.877181 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.878232 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.879369 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.880653 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.881959 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.883274 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_2/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.885429 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.886885 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.888180 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.889461 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.890492 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.891828 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.893095 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.894471 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.895775 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_20/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.897372 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.898632 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.899919 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.901186 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.902262 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.903345 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.904619 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.905929 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.907259 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_21/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.908440 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.909601 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.910799 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.911997 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.912945 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.913891 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.915120 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.916335 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.917513 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_22/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.918736 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.919910 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.921145 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.925684 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.926672 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.927723 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.928710 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.929781 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.931385 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
W0512 01:45:27.932857 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.934507 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_23/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.935859 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.937115 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.938445 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.939910 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.940900 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.942010 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.943797 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.944957 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.946114 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_3/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.948047 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.949235 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.950401 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.951651 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.952666 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.953629 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.954883 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.956067 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.957241 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_4/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.958411 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.959598 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.960738 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/extra_mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.965064 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/mlp/router/router_weights/w/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>, <FactorDim.NONE: None>) is ignored.
W0512 01:45:27.966077 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_extra_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.967043 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.967969 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.968892 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.970064 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.971285 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.972434 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_5/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.973553 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.974758 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.975953 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.977081 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.977986 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.978945 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.980105 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.981220 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.982360 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_6/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.983531 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.984717 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.985847 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.987015 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.987952 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.988930 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.990036 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.991199 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.992328 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_7/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.993512 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.994678 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:27.995859 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:27.997002 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.997986 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:27.998990 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.000124 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:28.001277 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.003148 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_8/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.004436 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_0/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.005578 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wi_1/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.006742 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/mlp/wo/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:28.007945 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_mlp_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:28.008871 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/pre_self_attention_layer_norm/scale 1 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.NONE: None>,) is ignored.
W0512 01:45:28.009813 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/key/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.011040 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/out/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
W0512 01:45:28.012413 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/query/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.013692 139867290535936 adafactor.py:358] Since rank of parameter decoder/layers_9/self_attention/value/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.015106 139867290535936 adafactor.py:358] Since rank of parameter decoder/logits_dense/kernel 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.ROW: 2>, <FactorDim.COLUMN: 3>) is ignored.
W0512 01:45:28.016416 139867290535936 adafactor.py:358] Since rank of parameter token_embedder/embedding 2 is less than or equal to 2, the factorization method falls back to heuristics and the provided factor rule (<FactorDim.COLUMN: 3>, <FactorDim.ROW: 2>) is ignored.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
normalizer.cc(51) LOG(INFO) precompiled_charsmap is empty. use identity normalization.
I0512 01:55:34.592245 139836023437312 train.py:421] Initialize/restore complete (612.71 seconds).
I0512 01:55:47.956609 139995375843328 train.py:421] Initialize/restore complete (626.06 seconds).
I0512 01:56:23.857023 139867290535936 train.py:421] Initialize/restore complete (661.32 seconds).
I0512 01:56:25.860570 140497406441472 train.py:421] Initialize/restore complete (663.48 seconds).
I0512 01:56:26.413532 139861676046336 train.py:421] Initialize/restore complete (664.20 seconds).
I0512 01:56:33.681175 140600233973760 train.py:421] Initialize/restore complete (671.28 seconds).
I0512 01:56:46.561214 139933136287744 train.py:421] Initialize/restore complete (684.86 seconds).
I0512 01:56:46.789342 139933136287744 utils.py:1372] Variable decoder/decoder_norm/scale                                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.789886 139933136287744 utils.py:1372] Variable decoder/layers_0/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.789960 139933136287744 utils.py:1372] Variable decoder/layers_0/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.790014 139933136287744 utils.py:1372] Variable decoder/layers_0/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.790060 139933136287744 utils.py:1372] Variable decoder/layers_0/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790107 139933136287744 utils.py:1372] Variable decoder/layers_0/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790153 139933136287744 utils.py:1372] Variable decoder/layers_0/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790199 139933136287744 utils.py:1372] Variable decoder/layers_0/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.790244 139933136287744 utils.py:1372] Variable decoder/layers_0/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790287 139933136287744 utils.py:1372] Variable decoder/layers_0/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790332 139933136287744 utils.py:1372] Variable decoder/layers_1/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.790378 139933136287744 utils.py:1372] Variable decoder/layers_1/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.790425 139933136287744 utils.py:1372] Variable decoder/layers_1/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.790472 139933136287744 utils.py:1372] Variable decoder/layers_1/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790517 139933136287744 utils.py:1372] Variable decoder/layers_1/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790560 139933136287744 utils.py:1372] Variable decoder/layers_1/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790603 139933136287744 utils.py:1372] Variable decoder/layers_1/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.790654 139933136287744 utils.py:1372] Variable decoder/layers_1/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790701 139933136287744 utils.py:1372] Variable decoder/layers_1/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.790746 139933136287744 utils.py:1372] Variable decoder/layers_10/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.790791 139933136287744 utils.py:1372] Variable decoder/layers_10/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.790838 139933136287744 utils.py:1372] Variable decoder/layers_10/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.790884 139933136287744 utils.py:1372] Variable decoder/layers_10/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790927 139933136287744 utils.py:1372] Variable decoder/layers_10/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.790970 139933136287744 utils.py:1372] Variable decoder/layers_10/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791018 139933136287744 utils.py:1372] Variable decoder/layers_10/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.791062 139933136287744 utils.py:1372] Variable decoder/layers_10/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791106 139933136287744 utils.py:1372] Variable decoder/layers_10/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791149 139933136287744 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.791194 139933136287744 utils.py:1372] Variable decoder/layers_11/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.791240 139933136287744 utils.py:1372] Variable decoder/layers_11/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.791284 139933136287744 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:56:46.791330 139933136287744 utils.py:1372] Variable decoder/layers_11/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:56:46.791377 139933136287744 utils.py:1372] Variable decoder/layers_11/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:56:46.791429 139933136287744 utils.py:1372] Variable decoder/layers_11/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.791475 139933136287744 utils.py:1372] Variable decoder/layers_11/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.791519 139933136287744 utils.py:1372] Variable decoder/layers_11/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.791563 139933136287744 utils.py:1372] Variable decoder/layers_11/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.791607 139933136287744 utils.py:1372] Variable decoder/layers_11/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791663 139933136287744 utils.py:1372] Variable decoder/layers_11/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.791711 139933136287744 utils.py:1372] Variable decoder/layers_11/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791756 139933136287744 utils.py:1372] Variable decoder/layers_11/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.791800 139933136287744 utils.py:1372] Variable decoder/layers_12/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.791845 139933136287744 utils.py:1372] Variable decoder/layers_12/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.791888 139933136287744 utils.py:1372] Variable decoder/layers_12/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.791991 139933136287744 utils.py:1372] Variable decoder/layers_12/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792042 139933136287744 utils.py:1372] Variable decoder/layers_12/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792088 139933136287744 utils.py:1372] Variable decoder/layers_12/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792133 139933136287744 utils.py:1372] Variable decoder/layers_12/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.792177 139933136287744 utils.py:1372] Variable decoder/layers_12/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792220 139933136287744 utils.py:1372] Variable decoder/layers_12/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792265 139933136287744 utils.py:1372] Variable decoder/layers_13/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.792309 139933136287744 utils.py:1372] Variable decoder/layers_13/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.792354 139933136287744 utils.py:1372] Variable decoder/layers_13/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.792397 139933136287744 utils.py:1372] Variable decoder/layers_13/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792445 139933136287744 utils.py:1372] Variable decoder/layers_13/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792489 139933136287744 utils.py:1372] Variable decoder/layers_13/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792572 139933136287744 utils.py:1372] Variable decoder/layers_13/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.792629 139933136287744 utils.py:1372] Variable decoder/layers_13/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792685 139933136287744 utils.py:1372] Variable decoder/layers_13/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792730 139933136287744 utils.py:1372] Variable decoder/layers_14/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.792775 139933136287744 utils.py:1372] Variable decoder/layers_14/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.792822 139933136287744 utils.py:1372] Variable decoder/layers_14/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.792866 139933136287744 utils.py:1372] Variable decoder/layers_14/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792909 139933136287744 utils.py:1372] Variable decoder/layers_14/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.792952 139933136287744 utils.py:1372] Variable decoder/layers_14/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.792998 139933136287744 utils.py:1372] Variable decoder/layers_14/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.793042 139933136287744 utils.py:1372] Variable decoder/layers_14/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793088 139933136287744 utils.py:1372] Variable decoder/layers_14/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793135 139933136287744 utils.py:1372] Variable decoder/layers_15/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.793184 139933136287744 utils.py:1372] Variable decoder/layers_15/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.793228 139933136287744 utils.py:1372] Variable decoder/layers_15/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.793271 139933136287744 utils.py:1372] Variable decoder/layers_15/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.793314 139933136287744 utils.py:1372] Variable decoder/layers_15/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.793359 139933136287744 utils.py:1372] Variable decoder/layers_15/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793406 139933136287744 utils.py:1372] Variable decoder/layers_15/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.793454 139933136287744 utils.py:1372] Variable decoder/layers_15/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793502 139933136287744 utils.py:1372] Variable decoder/layers_15/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793549 139933136287744 utils.py:1372] Variable decoder/layers_16/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.793616 139933136287744 utils.py:1372] Variable decoder/layers_16/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.793672 139933136287744 utils.py:1372] Variable decoder/layers_16/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.793719 139933136287744 utils.py:1372] Variable decoder/layers_16/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.793795 139933136287744 utils.py:1372] Variable decoder/layers_16/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.793845 139933136287744 utils.py:1372] Variable decoder/layers_16/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793890 139933136287744 utils.py:1372] Variable decoder/layers_16/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.793934 139933136287744 utils.py:1372] Variable decoder/layers_16/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.793979 139933136287744 utils.py:1372] Variable decoder/layers_16/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.794022 139933136287744 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.794066 139933136287744 utils.py:1372] Variable decoder/layers_17/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.794114 139933136287744 utils.py:1372] Variable decoder/layers_17/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.794213 139933136287744 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:56:46.794268 139933136287744 utils.py:1372] Variable decoder/layers_17/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:56:46.794314 139933136287744 utils.py:1372] Variable decoder/layers_17/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:56:46.794358 139933136287744 utils.py:1372] Variable decoder/layers_17/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.794403 139933136287744 utils.py:1372] Variable decoder/layers_17/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.794449 139933136287744 utils.py:1372] Variable decoder/layers_17/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.794493 139933136287744 utils.py:1372] Variable decoder/layers_17/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.794538 139933136287744 utils.py:1372] Variable decoder/layers_17/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.794584 139933136287744 utils.py:1372] Variable decoder/layers_17/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.794629 139933136287744 utils.py:1372] Variable decoder/layers_17/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.794679 139933136287744 utils.py:1372] Variable decoder/layers_17/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.794723 139933136287744 utils.py:1372] Variable decoder/layers_18/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.794771 139933136287744 utils.py:1372] Variable decoder/layers_18/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.794817 139933136287744 utils.py:1372] Variable decoder/layers_18/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.794859 139933136287744 utils.py:1372] Variable decoder/layers_18/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.794903 139933136287744 utils.py:1372] Variable decoder/layers_18/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.794947 139933136287744 utils.py:1372] Variable decoder/layers_18/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.794990 139933136287744 utils.py:1372] Variable decoder/layers_18/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.795032 139933136287744 utils.py:1372] Variable decoder/layers_18/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795074 139933136287744 utils.py:1372] Variable decoder/layers_18/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795121 139933136287744 utils.py:1372] Variable decoder/layers_19/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.795165 139933136287744 utils.py:1372] Variable decoder/layers_19/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.795208 139933136287744 utils.py:1372] Variable decoder/layers_19/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.795255 139933136287744 utils.py:1372] Variable decoder/layers_19/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.795301 139933136287744 utils.py:1372] Variable decoder/layers_19/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.795344 139933136287744 utils.py:1372] Variable decoder/layers_19/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795387 139933136287744 utils.py:1372] Variable decoder/layers_19/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.795433 139933136287744 utils.py:1372] Variable decoder/layers_19/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795477 139933136287744 utils.py:1372] Variable decoder/layers_19/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795521 139933136287744 utils.py:1372] Variable decoder/layers_2/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.795565 139933136287744 utils.py:1372] Variable decoder/layers_2/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.795607 139933136287744 utils.py:1372] Variable decoder/layers_2/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.795658 139933136287744 utils.py:1372] Variable decoder/layers_2/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.795703 139933136287744 utils.py:1372] Variable decoder/layers_2/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.795747 139933136287744 utils.py:1372] Variable decoder/layers_2/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795790 139933136287744 utils.py:1372] Variable decoder/layers_2/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.795834 139933136287744 utils.py:1372] Variable decoder/layers_2/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795880 139933136287744 utils.py:1372] Variable decoder/layers_2/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.795923 139933136287744 utils.py:1372] Variable decoder/layers_20/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.795966 139933136287744 utils.py:1372] Variable decoder/layers_20/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.796007 139933136287744 utils.py:1372] Variable decoder/layers_20/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.796050 139933136287744 utils.py:1372] Variable decoder/layers_20/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796093 139933136287744 utils.py:1372] Variable decoder/layers_20/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796135 139933136287744 utils.py:1372] Variable decoder/layers_20/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796180 139933136287744 utils.py:1372] Variable decoder/layers_20/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.796240 139933136287744 utils.py:1372] Variable decoder/layers_20/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796289 139933136287744 utils.py:1372] Variable decoder/layers_20/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796333 139933136287744 utils.py:1372] Variable decoder/layers_21/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.796377 139933136287744 utils.py:1372] Variable decoder/layers_21/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.796424 139933136287744 utils.py:1372] Variable decoder/layers_21/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.796469 139933136287744 utils.py:1372] Variable decoder/layers_21/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796510 139933136287744 utils.py:1372] Variable decoder/layers_21/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796589 139933136287744 utils.py:1372] Variable decoder/layers_21/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796638 139933136287744 utils.py:1372] Variable decoder/layers_21/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.796690 139933136287744 utils.py:1372] Variable decoder/layers_21/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796735 139933136287744 utils.py:1372] Variable decoder/layers_21/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.796780 139933136287744 utils.py:1372] Variable decoder/layers_22/mlp/wi_0/kernel                                                size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.796825 139933136287744 utils.py:1372] Variable decoder/layers_22/mlp/wi_1/kernel                                                size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.796868 139933136287744 utils.py:1372] Variable decoder/layers_22/mlp/wo/kernel                                                  size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.796910 139933136287744 utils.py:1372] Variable decoder/layers_22/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796953 139933136287744 utils.py:1372] Variable decoder/layers_22/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.796995 139933136287744 utils.py:1372] Variable decoder/layers_22/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797039 139933136287744 utils.py:1372] Variable decoder/layers_22/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.797083 139933136287744 utils.py:1372] Variable decoder/layers_22/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797125 139933136287744 utils.py:1372] Variable decoder/layers_22/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797168 139933136287744 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_0/kernel                                          size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.797212 139933136287744 utils.py:1372] Variable decoder/layers_23/extra_mlp/wi_1/kernel                                          size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.797256 139933136287744 utils.py:1372] Variable decoder/layers_23/extra_mlp/wo/kernel                                            size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.797303 139933136287744 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_0/kernel                                         size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:56:46.797348 139933136287744 utils.py:1372] Variable decoder/layers_23/mlp/expert/wi_1/kernel                                         size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:56:46.797392 139933136287744 utils.py:1372] Variable decoder/layers_23/mlp/expert/wo/kernel                                           size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:56:46.797440 139933136287744 utils.py:1372] Variable decoder/layers_23/mlp/router/router_weights/w/kernel                             size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.797486 139933136287744 utils.py:1372] Variable decoder/layers_23/pre_extra_mlp_layer_norm/scale                                 size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.797533 139933136287744 utils.py:1372] Variable decoder/layers_23/pre_mlp_layer_norm/scale                                       size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.797576 139933136287744 utils.py:1372] Variable decoder/layers_23/pre_self_attention_layer_norm/scale                            size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.797619 139933136287744 utils.py:1372] Variable decoder/layers_23/self_attention/key/kernel                                      size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797672 139933136287744 utils.py:1372] Variable decoder/layers_23/self_attention/out/kernel                                      size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.797717 139933136287744 utils.py:1372] Variable decoder/layers_23/self_attention/query/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797759 139933136287744 utils.py:1372] Variable decoder/layers_23/self_attention/value/kernel                                    size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.797801 139933136287744 utils.py:1372] Variable decoder/layers_3/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.797845 139933136287744 utils.py:1372] Variable decoder/layers_3/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.797890 139933136287744 utils.py:1372] Variable decoder/layers_3/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.797934 139933136287744 utils.py:1372] Variable decoder/layers_3/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.797976 139933136287744 utils.py:1372] Variable decoder/layers_3/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.798020 139933136287744 utils.py:1372] Variable decoder/layers_3/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798063 139933136287744 utils.py:1372] Variable decoder/layers_3/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.798105 139933136287744 utils.py:1372] Variable decoder/layers_3/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798147 139933136287744 utils.py:1372] Variable decoder/layers_3/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798190 139933136287744 utils.py:1372] Variable decoder/layers_4/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.798233 139933136287744 utils.py:1372] Variable decoder/layers_4/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.798305 139933136287744 utils.py:1372] Variable decoder/layers_4/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.798356 139933136287744 utils.py:1372] Variable decoder/layers_4/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.798401 139933136287744 utils.py:1372] Variable decoder/layers_4/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.798447 139933136287744 utils.py:1372] Variable decoder/layers_4/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798492 139933136287744 utils.py:1372] Variable decoder/layers_4/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.798536 139933136287744 utils.py:1372] Variable decoder/layers_4/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798579 139933136287744 utils.py:1372] Variable decoder/layers_4/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.798621 139933136287744 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_0/kernel                                           size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.798674 139933136287744 utils.py:1372] Variable decoder/layers_5/extra_mlp/wi_1/kernel                                           size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.798721 139933136287744 utils.py:1372] Variable decoder/layers_5/extra_mlp/wo/kernel                                             size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.798767 139933136287744 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_0/kernel                                          size 268435456    shape (expert=8, mlp_embed=2048, expert_mlp=16384) partition spec ('expert', None, 'model')
I0512 01:56:46.798812 139933136287744 utils.py:1372] Variable decoder/layers_5/mlp/expert/wi_1/kernel                                          size 134217728    shape (expert=8, mlp_embed=2048, expert_mlp=8192) partition spec ('expert', None, 'model')
I0512 01:56:46.798856 139933136287744 utils.py:1372] Variable decoder/layers_5/mlp/expert/wo/kernel                                            size 134217728    shape (expert=8, expert_mlp=8192, mlp_embed=2048) partition spec ('expert', 'model', None)
I0512 01:56:46.798899 139933136287744 utils.py:1372] Variable decoder/layers_5/mlp/router/router_weights/w/kernel                              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.798941 139933136287744 utils.py:1372] Variable decoder/layers_5/pre_extra_mlp_layer_norm/scale                                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.798988 139933136287744 utils.py:1372] Variable decoder/layers_5/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799041 139933136287744 utils.py:1372] Variable decoder/layers_5/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799091 139933136287744 utils.py:1372] Variable decoder/layers_5/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799136 139933136287744 utils.py:1372] Variable decoder/layers_5/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.799180 139933136287744 utils.py:1372] Variable decoder/layers_5/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799223 139933136287744 utils.py:1372] Variable decoder/layers_5/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799266 139933136287744 utils.py:1372] Variable decoder/layers_6/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.799309 139933136287744 utils.py:1372] Variable decoder/layers_6/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.799351 139933136287744 utils.py:1372] Variable decoder/layers_6/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.799400 139933136287744 utils.py:1372] Variable decoder/layers_6/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799449 139933136287744 utils.py:1372] Variable decoder/layers_6/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799493 139933136287744 utils.py:1372] Variable decoder/layers_6/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799536 139933136287744 utils.py:1372] Variable decoder/layers_6/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.799579 139933136287744 utils.py:1372] Variable decoder/layers_6/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799622 139933136287744 utils.py:1372] Variable decoder/layers_6/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799671 139933136287744 utils.py:1372] Variable decoder/layers_7/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.799717 139933136287744 utils.py:1372] Variable decoder/layers_7/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.799760 139933136287744 utils.py:1372] Variable decoder/layers_7/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.799802 139933136287744 utils.py:1372] Variable decoder/layers_7/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799845 139933136287744 utils.py:1372] Variable decoder/layers_7/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.799887 139933136287744 utils.py:1372] Variable decoder/layers_7/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.799929 139933136287744 utils.py:1372] Variable decoder/layers_7/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.799972 139933136287744 utils.py:1372] Variable decoder/layers_7/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800015 139933136287744 utils.py:1372] Variable decoder/layers_7/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800058 139933136287744 utils.py:1372] Variable decoder/layers_8/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.800102 139933136287744 utils.py:1372] Variable decoder/layers_8/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.800144 139933136287744 utils.py:1372] Variable decoder/layers_8/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.800186 139933136287744 utils.py:1372] Variable decoder/layers_8/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.800233 139933136287744 utils.py:1372] Variable decoder/layers_8/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.800277 139933136287744 utils.py:1372] Variable decoder/layers_8/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800338 139933136287744 utils.py:1372] Variable decoder/layers_8/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.800386 139933136287744 utils.py:1372] Variable decoder/layers_8/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800433 139933136287744 utils.py:1372] Variable decoder/layers_8/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800476 139933136287744 utils.py:1372] Variable decoder/layers_9/mlp/wi_0/kernel                                                 size 33554432     shape (mlp_embed=2048, mlp=16384)              partition spec (None, 'model')
I0512 01:56:46.800519 139933136287744 utils.py:1372] Variable decoder/layers_9/mlp/wi_1/kernel                                                 size 16777216     shape (mlp_embed=2048, mlp=8192)               partition spec (None, 'model')
I0512 01:56:46.800603 139933136287744 utils.py:1372] Variable decoder/layers_9/mlp/wo/kernel                                                   size 16777216     shape (mlp=8192, mlp_embed=2048)               partition spec ('model', None)
I0512 01:56:46.800654 139933136287744 utils.py:1372] Variable decoder/layers_9/pre_mlp_layer_norm/scale                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.800703 139933136287744 utils.py:1372] Variable decoder/layers_9/pre_self_attention_layer_norm/scale                             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.800747 139933136287744 utils.py:1372] Variable decoder/layers_9/self_attention/key/kernel                                       size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800790 139933136287744 utils.py:1372] Variable decoder/layers_9/self_attention/out/kernel                                       size 6291456      shape (joined_kv=3072, embed=2048)             partition spec ('model', None)
I0512 01:56:46.800834 139933136287744 utils.py:1372] Variable decoder/layers_9/self_attention/query/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800879 139933136287744 utils.py:1372] Variable decoder/layers_9/self_attention/value/kernel                                     size 6291456      shape (embed=2048, joined_kv=3072)             partition spec (None, 'model')
I0512 01:56:46.800922 139933136287744 utils.py:1372] Variable decoder/logits_dense/kernel                                                      size 525074432    shape (embed=2048, vocab=256384)               partition spec (None, 'model')
I0512 01:56:46.800966 139933136287744 utils.py:1372] Variable token_embedder/embedding                                                         size 525074432    shape (vocab=256384, embed=2048)               partition spec ('model', None)
I0512 01:56:46.801102 139933136287744 utils.py:1372] Total number of parameters: 5412399104
I0512 01:56:46.801164 139933136287744 utils.py:1372] 
I0512 01:56:46.807975 139933136287744 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/m                                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808141 139933136287744 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v                                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.808202 139933136287744 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_col                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808252 139933136287744 utils.py:1372] Variable param_states/decoder/decoder_norm/scale/v_row                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808300 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808347 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808398 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.808444 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.808483 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808559 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808613 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.808666 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.808710 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808754 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808795 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.808837 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.808876 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808917 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.808959 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.808997 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809036 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809077 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.809117 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809156 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809194 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809233 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809271 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.809312 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.809351 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809390 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809433 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.809476 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.809540 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809585 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809627 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.809675 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.809716 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809761 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809803 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.809844 139933136287744 utils.py:1372] Variable param_states/decoder/layers_0/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.809882 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809921 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.809961 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.810001 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.810039 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810078 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810119 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.810158 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.810196 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810235 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810275 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.810317 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.810356 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810398 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.810442 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810484 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810522 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810562 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.810606 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810655 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810698 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810739 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810781 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.810822 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.810862 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810901 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.810939 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.810981 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811020 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811059 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811101 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.811141 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811179 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811220 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811261 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.811300 139933136287744 utils.py:1372] Variable param_states/decoder/layers_1/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811338 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811378 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811419 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.811480 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811525 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811567 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811606 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.811651 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811692 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811732 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811774 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.811813 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.811851 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811895 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.811938 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.811977 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812015 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812056 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.812096 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812134 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812174 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812213 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812256 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.812295 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.812334 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812372 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812415 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.812456 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.812494 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812572 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812621 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.812669 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.812710 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812749 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812792 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.812833 139933136287744 utils.py:1372] Variable param_states/decoder/layers_10/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.812872 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812911 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.812950 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.812990 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.813028 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813066 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813105 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.813144 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.813183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813222 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813263 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.813302 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.813341 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813380 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813440 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:56:46.813489 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.813534 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813575 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813614 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.813663 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.813705 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813745 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813786 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.813825 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.813864 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813908 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.813951 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.813992 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814038 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814085 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.814127 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814168 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814245 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814288 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.814330 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814372 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814415 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814460 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.814505 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814547 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814586 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814625 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814672 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.814715 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.814754 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814795 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814836 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.814875 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.814913 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814951 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.814990 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.815030 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.815068 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815107 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815145 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.815183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_11/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.815221 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815262 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815303 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.815343 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.815381 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815439 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815484 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.815523 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.815562 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815601 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815640 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.815691 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.815732 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815775 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.815819 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815858 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815897 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.815937 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.815978 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816019 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816057 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816099 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816142 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.816183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.816220 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816259 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816302 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.816342 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.816381 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816422 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816463 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.816501 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.816564 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816612 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816659 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.816700 139933136287744 utils.py:1372] Variable param_states/decoder/layers_12/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.816739 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816778 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816819 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.816860 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.816899 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816938 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.816977 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.817016 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.817054 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817093 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817131 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.817170 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.817209 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817253 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.817296 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817334 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817391 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817444 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.817489 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817529 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817568 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817607 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817656 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.817700 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.817742 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817781 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817823 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.817863 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.817902 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817940 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.817984 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.818025 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.818065 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818104 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818144 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.818183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_13/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.818222 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818263 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818304 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.818343 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.818382 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818423 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818464 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.818504 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.818545 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818584 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818623 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.818671 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.818712 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818755 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.818796 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818834 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818874 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818916 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.818958 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.818998 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819037 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819081 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819126 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.819164 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.819204 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819245 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819283 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.819339 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.819384 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819427 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819469 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.819508 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.819546 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819585 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819623 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.819670 139933136287744 utils.py:1372] Variable param_states/decoder/layers_14/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.819710 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819750 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819789 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.819827 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.819865 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819905 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.819947 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.819986 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.820025 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820063 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820101 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.820140 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.820178 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820221 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.820262 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820300 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820339 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820380 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.820423 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820466 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820505 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820576 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820624 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.820672 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.820712 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820751 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820791 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.820831 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.820870 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820909 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.820949 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.820995 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.821037 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821076 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821114 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.821152 139933136287744 utils.py:1372] Variable param_states/decoder/layers_15/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.821193 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821233 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821289 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.821331 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.821373 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821415 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821459 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.821502 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.821547 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821594 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821643 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.821706 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.821753 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821805 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.821854 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821901 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821946 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.821995 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.822043 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822088 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822133 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822178 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822224 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.822270 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.822316 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822362 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822409 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.822452 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.822492 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822534 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822578 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.822618 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.822669 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822712 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822753 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.822793 139933136287744 utils.py:1372] Variable param_states/decoder/layers_16/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.822832 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822872 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.822914 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.822952 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.822991 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823033 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823075 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.823116 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.823155 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823194 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823233 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.823271 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.823310 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823367 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823412 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:56:46.823456 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.823497 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823537 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823580 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.823620 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.823668 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823710 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823753 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.823793 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.823832 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823875 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.823917 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823956 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.823994 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824036 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.824076 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824115 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824156 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824199 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.824238 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824277 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824316 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824357 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.824397 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824439 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824479 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824519 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824593 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.824636 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.824682 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824723 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824761 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.824800 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.824841 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824880 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.824921 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.824962 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.825001 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825042 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825087 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.825127 139933136287744 utils.py:1372] Variable param_states/decoder/layers_17/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.825166 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825207 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825250 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.825288 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.825344 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825388 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825433 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.825472 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.825512 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825552 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825594 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.825638 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.825686 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825730 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.825772 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825810 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825849 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825890 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.825930 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.825974 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826016 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826057 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826097 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.826137 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826176 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826218 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826258 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.826296 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826335 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826380 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826423 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.826463 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826501 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826540 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826578 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.826618 139933136287744 utils.py:1372] Variable param_states/decoder/layers_18/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826663 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826706 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826745 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.826783 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826822 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826861 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.826900 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.826942 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.826981 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827021 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827063 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.827103 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.827143 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827188 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.827232 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827290 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827333 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827376 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.827419 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827463 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827506 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827546 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827588 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.827630 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.827678 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827719 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827759 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.827798 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.827839 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827878 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.827917 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.827956 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.827994 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828034 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828075 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.828115 139933136287744 utils.py:1372] Variable param_states/decoder/layers_19/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.828154 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828196 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828238 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.828277 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.828315 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828355 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828396 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.828438 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.828477 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828517 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828597 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.828639 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.828687 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828733 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.828775 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828819 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828859 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828901 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.828946 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.828989 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829028 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829067 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829108 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.829148 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.829187 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829227 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829288 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.829333 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.829373 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829415 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829457 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.829496 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.829535 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829576 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829618 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.829668 139933136287744 utils.py:1372] Variable param_states/decoder/layers_2/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.829714 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829756 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829797 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.829838 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.829877 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829919 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.829960 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.829998 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.830038 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830079 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830119 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.830159 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.830199 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830242 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.830283 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830323 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830365 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830407 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.830452 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830491 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830530 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830569 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830608 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.830653 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.830697 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830736 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830779 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.830824 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.830866 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830906 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.830948 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.830988 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.831027 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831067 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831107 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.831147 139933136287744 utils.py:1372] Variable param_states/decoder/layers_20/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.831187 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831244 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831292 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.831336 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.831379 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831421 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831463 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.831503 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.831542 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831584 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831626 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.831674 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.831714 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831757 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.831801 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831842 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831882 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.831925 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.831967 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832006 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832046 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832089 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832128 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.832168 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.832207 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832246 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832288 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.832328 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.832368 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832410 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832451 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.832490 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.832528 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832609 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832656 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.832700 139933136287744 utils.py:1372] Variable param_states/decoder/layers_21/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.832740 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832781 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832823 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_col                             size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.832863 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_0/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.832901 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/m                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832941 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v                                 size 1            shape (1,)                                     partition spec None
I0512 01:56:46.832979 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_col                             size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.833018 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wi_1/kernel/v_row                             size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.833058 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/m                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833099 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v                                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833137 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_col                               size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.833176 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/mlp/wo/kernel/v_row                               size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.833235 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833282 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.833325 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833364 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833403 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833448 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.833490 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833531 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833571 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833610 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833658 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.833700 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.833742 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833782 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833822 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.833863 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.833903 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833943 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.833982 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.834023 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.834061 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834101 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834139 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.834215 139933136287744 utils.py:1372] Variable param_states/decoder/layers_22/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.834258 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834299 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834341 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_col                       size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.834381 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_0/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.834423 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834465 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834503 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_col                       size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.834541 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wi_1/kernel/v_row                       size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.834579 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834618 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834664 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_col                         size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.834705 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/extra_mlp/wo/kernel/v_row                         size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.834743 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834782 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834822 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_col                      size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:56:46.834865 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_0/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.834905 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/m                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834944 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.834983 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_col                      size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.835023 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wi_1/kernel/v_row                      size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.835063 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835104 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835146 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_col                        size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.835203 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/expert/wo/kernel/v_row                        size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.835249 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835295 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v              size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.835337 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835383 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/mlp/router/router_weights/w/kernel/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835426 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/m                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835469 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v                  size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.835510 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_col              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835549 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_extra_mlp_layer_norm/scale/v_row              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835587 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835629 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v                        size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.835678 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_col                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835721 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_mlp_layer_norm/scale/v_row                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835761 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/m             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835803 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v             size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.835845 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_col         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835888 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/pre_self_attention_layer_norm/scale/v_row         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835932 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.835978 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836027 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.836073 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/key/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.836117 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/m                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836165 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836216 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_col                   size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.836266 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/out/kernel/v_row                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.836314 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836361 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836409 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.836459 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/query/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.836506 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/m                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836588 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836643 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_col                 size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.836702 139933136287744 utils.py:1372] Variable param_states/decoder/layers_23/self_attention/value/kernel/v_row                 size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.836749 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836797 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836846 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.836893 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.836941 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.836987 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837036 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.837077 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.837116 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837155 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837194 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.837236 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.837277 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837321 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.837380 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837425 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837469 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837512 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.837553 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837591 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837630 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837679 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837723 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.837765 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.837803 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837842 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837881 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.837920 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.837958 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.837997 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838035 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.838073 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.838112 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838150 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838189 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.838228 139933136287744 utils.py:1372] Variable param_states/decoder/layers_3/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.838267 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838307 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838345 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.838384 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.838426 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838467 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838509 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.838548 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.838587 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838626 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838674 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.838714 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.838753 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838795 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.838836 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838875 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838914 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.838955 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.838995 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839037 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839077 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839117 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839159 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.839201 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.839240 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839298 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839345 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.839384 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.839425 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839466 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839507 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.839547 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.839586 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839625 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839673 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.839714 139933136287744 utils.py:1372] Variable param_states/decoder/layers_4/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.839753 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839793 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839832 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_col                        size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.839870 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_0/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.839909 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/m                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839949 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v                            size 1            shape (1,)                                     partition spec None
I0512 01:56:46.839993 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_col                        size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.840033 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wi_1/kernel/v_row                        size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.840072 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/m                              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840111 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v                              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840152 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_col                          size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.840191 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/extra_mlp/wo/kernel/v_row                          size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.840229 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840273 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840314 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_col                       size 131072       shape (8, 16384)                               partition spec ('expert',)
I0512 01:56:46.840355 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_0/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.840395 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/m                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840438 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v                           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840483 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_col                       size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.840525 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wi_1/kernel/v_row                       size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.840596 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/m                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840638 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v                             size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840685 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_col                         size 65536        shape (8, 8192)                                partition spec ('expert',)
I0512 01:56:46.840730 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/expert/wo/kernel/v_row                         size 16384        shape (8, 2048)                                partition spec ('expert',)
I0512 01:56:46.840774 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/m               size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840818 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v               size 16384        shape (embed=2048, unmodeled=8)                partition spec ('model', None)
I0512 01:56:46.840860 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_col           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840899 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/mlp/router/router_weights/w/kernel/v_row           size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840938 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/m                   size 1            shape (1,)                                     partition spec None
I0512 01:56:46.840981 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v                   size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.841022 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_col               size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841063 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_extra_mlp_layer_norm/scale/v_row               size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841104 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841147 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.841188 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841232 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841292 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841341 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.841384 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841426 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841467 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841506 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841548 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.841587 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.841624 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841671 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841712 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.841751 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.841790 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841829 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841868 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.841906 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.841944 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.841983 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842021 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.842060 139933136287744 utils.py:1372] Variable param_states/decoder/layers_5/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.842100 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842141 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.842227 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.842268 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842307 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842346 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.842384 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.842424 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842464 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842503 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.842540 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.842579 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842620 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.842669 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842710 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842752 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842796 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.842837 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842875 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842914 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842952 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.842993 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.843032 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843071 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843113 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843155 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.843212 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843255 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843296 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843334 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.843373 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843412 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843455 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843497 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.843537 139933136287744 utils.py:1372] Variable param_states/decoder/layers_6/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843575 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843615 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843660 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.843702 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843743 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843782 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843821 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.843859 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.843896 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843935 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.843973 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.844013 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.844053 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844096 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.844138 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844176 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844215 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844255 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.844299 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844337 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844377 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844420 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844462 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.844502 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.844573 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844620 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844666 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.844709 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.844748 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844788 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844826 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.844864 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.844902 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844941 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.844980 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.845021 139933136287744 utils.py:1372] Variable param_states/decoder/layers_7/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.845061 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845100 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845160 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.845206 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.845245 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845285 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845326 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.845364 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.845402 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845444 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845483 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.845522 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.845560 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845603 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.845651 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845694 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845734 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845775 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.845815 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845857 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845896 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845940 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.845983 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.846022 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846060 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846101 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846142 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.846183 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846223 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846262 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846300 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.846339 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846379 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846421 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846467 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.846507 139933136287744 utils.py:1372] Variable param_states/decoder/layers_8/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846547 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846586 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846626 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_col                              size 16384        shape (16384,)                                 partition spec None
I0512 01:56:46.846672 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_0/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846712 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/m                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846750 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v                                  size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846789 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_col                              size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.846827 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wi_1/kernel/v_row                              size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.846868 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/m                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846906 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v                                    size 1            shape (1,)                                     partition spec None
I0512 01:56:46.846946 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_col                                size 8192         shape (8192,)                                  partition spec None
I0512 01:56:46.846984 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/mlp/wo/kernel/v_row                                size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.847022 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/m                         size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847081 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v                         size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.847126 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_col                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847168 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_mlp_layer_norm/scale/v_row                     size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847213 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/m              size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847254 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v              size 2048         shape (embed=2048)                             partition spec ('model',)
I0512 01:56:46.847295 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_col          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847334 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/pre_self_attention_layer_norm/scale/v_row          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847372 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847412 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847454 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.847491 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/key/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.847532 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/m                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847574 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v                        size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847614 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_col                    size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.847659 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/out/kernel/v_row                    size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.847699 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847740 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847780 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.847818 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/query/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.847857 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/m                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847897 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v                      size 1            shape (1,)                                     partition spec None
I0512 01:56:46.847936 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_col                  size 3072         shape (3072,)                                  partition spec None
I0512 01:56:46.847975 139933136287744 utils.py:1372] Variable param_states/decoder/layers_9/self_attention/value/kernel/v_row                  size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.848013 139933136287744 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/m                                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.848054 139933136287744 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v                                       size 1            shape (1,)                                     partition spec None
I0512 01:56:46.848093 139933136287744 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_col                                   size 256384       shape (256384,)                                partition spec None
I0512 01:56:46.848132 139933136287744 utils.py:1372] Variable param_states/decoder/logits_dense/kernel/v_row                                   size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.848171 139933136287744 utils.py:1372] Variable param_states/token_embedder/embedding/m                                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.848215 139933136287744 utils.py:1372] Variable param_states/token_embedder/embedding/v                                          size 1            shape (1,)                                     partition spec None
I0512 01:56:46.848256 139933136287744 utils.py:1372] Variable param_states/token_embedder/embedding/v_col                                      size 256384       shape (256384,)                                partition spec None
I0512 01:56:46.848295 139933136287744 utils.py:1372] Variable param_states/token_embedder/embedding/v_row                                      size 2048         shape (2048,)                                  partition spec None
I0512 01:56:46.848334 139933136287744 utils.py:1372] Variable step                                                                             size 1            shape ()                                       partition spec None
I0512 01:56:53.957955 140139156215808 train.py:421] Initialize/restore complete (692.27 seconds).
I0512 01:57:07.340709 140139156215808 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.340942 139867290535936 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.340447 140600233973760 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.342686 139861676046336 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.343057 139836023437312 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.343020 139995375843328 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:07.341922 140497406441472 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:09.192577 139837062190656 logging_writer.py:64] [0] collection=train Got texts: {'config': "    from __gin__ import dynamic_registration\n    import __main__ as train_script\n    from flax import linen\n    import flaxformer\n    from flaxformer.architectures.moe import moe_architecture\n    from flaxformer.architectures.moe import moe_enums\n    from flaxformer.architectures.moe import moe_layers\n    from flaxformer.architectures.moe import routing\n    from flaxformer.architectures.t5 import t5_architecture\n    from flaxformer.components.attention import dense_attention\n    from flaxformer.components.attention import memory_efficient_attention\n    from flaxformer.components import dense\n    from flaxformer.components import embedding\n    from flaxformer.components import layer_norm\n    from gin import config\n    import seqio\n    import t5.data.mixtures\n    from t5x import adafactor\n    from t5x.contrib.moe import adafactor_utils\n    from t5x.contrib.moe import models\n    from t5x.contrib.moe import partitioning as moe_partitioning\n    from t5x.contrib.moe import trainer as moe_trainer\n    from t5x import gin_utils\n    from t5x import partitioning\n    from t5x import trainer\n    from t5x import utils\n    \n#### Macros:\n\n    ACTIVATION_DTYPE = 'bfloat16'\n    ACTIVATION_PARTITIONING_DIMS = 1\n    ARCHITECTURE = @t5_architecture.DecoderOnly()\n    AUX_LOSS_FACTOR = 0.01\n    BATCH_SIZE = 384\n    BIAS_INIT = @bias_init/linen.initializers.normal()\n    DECODER_SPARSE_LAYOUT = %flaxformer.architectures.moe.moe_enums.LayerLayout.MIXED\n    DROPOUT_FACTORY = @dropout_factory/linen.Dropout\n    DROPOUT_RATE = 0.0\n    EMBED_DIM = 2048\n    EVAL_EXPERT_CAPACITY_FACTOR = 2.0\n    EXPERT_DROPOUT_RATE = %DROPOUT_RATE\n    EXPERT_MLP_DIM = %MLP_DIM\n    GROUP_SIZE = 4096\n    HEAD_DIM = 128\n    JITTER_NOISE = 0.0\n    LABEL_SMOOTHING = 0.0\n    LOSS_NORMALIZING_FACTOR = 'NUM_REAL_TARGET_TOKENS'\n    MIXTURE_OR_TASK_MODULE = None\n    MIXTURE_OR_TASK_NAME = 'mix_ul2_test'\n    MLP_DIM = 8192\n    MODEL = @models.MoeDecoderOnlyModel()\n    MODEL_DIR = 'gs://rosinality-tpu-bucket/openmoe_8b_ul2/training'\n    MODEL_PARALLEL_SUBMESH = None\n    MOE_TRUNCATED_DTYPE = 'bfloat16'\n    NUM_DECODER_LAYERS = 24\n    NUM_DECODER_SPARSE_LAYERS = 4\n    NUM_EMBEDDINGS = 256384\n    NUM_EXPERT_PARTITIONS = 8\n    NUM_EXPERTS = 8\n    NUM_HEADS = 24\n    NUM_MODEL_PARTITIONS = 4\n    NUM_SELECTED_EXPERTS = 2\n    OPTIMIZER = @adafactor.Adafactor()\n    RANDOM_SEED = None\n    ROUTER_Z_LOSS_FACTOR = 0.0001\n    SCALE = 0.1\n    SHUFFLE_TRAIN_EXAMPLES = True\n    TASK_FEATURE_LENGTHS = {'inputs': 1024, 'targets': 1024}\n    TRAIN_EXPERT_CAPACITY_FACTOR = 1.25\n    TRAIN_STEPS = 500000\n    USE_CACHED_TASKS = False\n    USE_HARDWARE_RNG = False\n    VOCABULARY = @seqio.SentencePieceVocabulary()\n    Z_LOSS = 0.0001\n    \n#### Parameters for adafactor.Adafactor:\n\n    adafactor.Adafactor.decay_rate = 0.8\n    adafactor.Adafactor.logical_factor_rules = @adafactor_utils.logical_factor_rules()\n    adafactor.Adafactor.step_offset = 0\n    \n#### Parameters for utils.CheckpointConfig:\n\n    utils.CheckpointConfig.restore = @utils.RestoreCheckpointConfig()\n    utils.CheckpointConfig.save = @utils.SaveCheckpointConfig()\n    \n#### Parameters for moe_partitioning.compute_num_model_partitions:\n\n    moe_partitioning.compute_num_model_partitions.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.compute_num_model_partitions.num_model_partitions = \\\n        %NUM_MODEL_PARTITIONS\n    \n#### Parameters for utils.create_learning_rate_scheduler:\n\n    utils.create_learning_rate_scheduler.base_learning_rate = 1.0\n    utils.create_learning_rate_scheduler.factors = 'constant * rsqrt_decay'\n    utils.create_learning_rate_scheduler.warmup_steps = 10000\n    \n#### Parameters for train/utils.DatasetConfig:\n\n    train/utils.DatasetConfig.batch_size = %BATCH_SIZE\n    train/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train/utils.DatasetConfig.pack = True\n    train/utils.DatasetConfig.seed = 42\n    train/utils.DatasetConfig.shuffle = %SHUFFLE_TRAIN_EXAMPLES\n    train/utils.DatasetConfig.split = 'train'\n    train/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for train_eval/utils.DatasetConfig:\n\n    train_eval/utils.DatasetConfig.batch_size = 128\n    train_eval/utils.DatasetConfig.mixture_or_task_name = %MIXTURE_OR_TASK_NAME\n    train_eval/utils.DatasetConfig.module = %MIXTURE_OR_TASK_MODULE\n    train_eval/utils.DatasetConfig.pack = True\n    train_eval/utils.DatasetConfig.seed = 42\n    train_eval/utils.DatasetConfig.shuffle = False\n    train_eval/utils.DatasetConfig.split = 'validation'\n    train_eval/utils.DatasetConfig.task_feature_lengths = %TASK_FEATURE_LENGTHS\n    train_eval/utils.DatasetConfig.use_cached = %USE_CACHED_TASKS\n    \n#### Parameters for t5_architecture.DecoderLayer:\n\n    t5_architecture.DecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    t5_architecture.DecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    t5_architecture.DecoderLayer.encoder_decoder_attention = None\n    t5_architecture.DecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    t5_architecture.DecoderLayer.mlp = @dense.MlpBlock()\n    t5_architecture.DecoderLayer.scanned = False\n    t5_architecture.DecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for t5_architecture.DecoderOnly:\n\n    t5_architecture.DecoderOnly.decoder_factory = @moe_architecture.SparseDecoder\n    t5_architecture.DecoderOnly.dtype = %ACTIVATION_DTYPE\n    t5_architecture.DecoderOnly.shared_token_embedder_factory = @embedding.Embed\n    \n#### Parameters for output_logits/dense.DenseGeneral:\n\n    output_logits/dense.DenseGeneral.bias_init = %BIAS_INIT\n    output_logits/dense.DenseGeneral.dtype = 'float32'\n    output_logits/dense.DenseGeneral.features = %NUM_EMBEDDINGS\n    output_logits/dense.DenseGeneral.kernel_axis_names = ['embed', 'vocab']\n    output_logits/dense.DenseGeneral.kernel_init = \\\n        @output_logits_kernel_init/linen.initializers.variance_scaling()\n    output_logits/dense.DenseGeneral.use_bias = False\n    \n#### Parameters for dropout_factory/linen.Dropout:\n\n    dropout_factory/linen.Dropout.broadcast_dims = (-2,)\n    dropout_factory/linen.Dropout.rate = %DROPOUT_RATE\n    \n#### Parameters for embedding.Embed:\n\n    embedding.Embed.attend_dtype = 'float32'\n    embedding.Embed.cast_input_dtype = 'int32'\n    embedding.Embed.dtype = %ACTIVATION_DTYPE\n    embedding.Embed.embedding_init = @token_embedder_init/linen.initializers.normal()\n    embedding.Embed.features = %EMBED_DIM\n    embedding.Embed.name = 'token_embedder'\n    embedding.Embed.num_embeddings = %NUM_EMBEDDINGS\n    embedding.Embed.one_hot = True\n    \n#### Parameters for dense.MlpBlock:\n\n    dense.MlpBlock.activations = ('swiglu', 'linear')\n    dense.MlpBlock.bias_init = %BIAS_INIT\n    dense.MlpBlock.dtype = %ACTIVATION_DTYPE\n    dense.MlpBlock.final_dropout_rate = 0\n    dense.MlpBlock.input_axis_name = 'mlp_embed'\n    dense.MlpBlock.intermediate_dim = %MLP_DIM\n    dense.MlpBlock.intermediate_dropout_rate = %DROPOUT_RATE\n    dense.MlpBlock.kernel_init = @mlp_kernel_init/linen.initializers.variance_scaling()\n    dense.MlpBlock.output_axis_name = 'mlp_embed'\n    dense.MlpBlock.use_bias = False\n    \n#### Parameters for expert/dense.MlpBlock:\n\n    expert/dense.MlpBlock.activation_partitioning_dims = 1\n    expert/dense.MlpBlock.activations = ('swiglu', 'linear')\n    expert/dense.MlpBlock.bias_init = %BIAS_INIT\n    expert/dense.MlpBlock.data_sharding_constraints = ('expert_replicas', 'mlp')\n    expert/dense.MlpBlock.dtype = %MOE_TRUNCATED_DTYPE\n    expert/dense.MlpBlock.final_dropout_rate = 0.0\n    expert/dense.MlpBlock.input_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.intermediate_axis_name = 'expert_mlp'\n    expert/dense.MlpBlock.intermediate_dim = %EXPERT_MLP_DIM\n    expert/dense.MlpBlock.intermediate_dropout_rate = %EXPERT_DROPOUT_RATE\n    expert/dense.MlpBlock.kernel_init = \\\n        @expert_kernel_init/linen.initializers.variance_scaling()\n    expert/dense.MlpBlock.output_axis_name = 'mlp_embed'\n    expert/dense.MlpBlock.use_bias = False\n    \n#### Parameters for models.MoeDecoderOnlyModel:\n\n    models.MoeDecoderOnlyModel.aux_loss_factor = %AUX_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.inputs_bidirectional_attention = True\n    models.MoeDecoderOnlyModel.label_smoothing = %LABEL_SMOOTHING\n    models.MoeDecoderOnlyModel.loss_normalizing_factor = %LOSS_NORMALIZING_FACTOR\n    models.MoeDecoderOnlyModel.module = %ARCHITECTURE\n    models.MoeDecoderOnlyModel.optimizer_def = %OPTIMIZER\n    models.MoeDecoderOnlyModel.router_z_loss_factor = %ROUTER_Z_LOSS_FACTOR\n    models.MoeDecoderOnlyModel.vocabulary = %VOCABULARY\n    models.MoeDecoderOnlyModel.z_loss = %Z_LOSS\n    \n#### Parameters for moe_layers.MoeLayer:\n\n    moe_layers.MoeLayer.dtype = %MOE_TRUNCATED_DTYPE\n    moe_layers.MoeLayer.eval_capacity_factor = %EVAL_EXPERT_CAPACITY_FACTOR\n    moe_layers.MoeLayer.expert = @expert/dense.MlpBlock()\n    moe_layers.MoeLayer.max_group_size = %GROUP_SIZE\n    moe_layers.MoeLayer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_layers.MoeLayer.num_experts = %NUM_EXPERTS\n    moe_layers.MoeLayer.num_model_partitions = \\\n        @moe_partitioning.compute_num_model_partitions()\n    moe_layers.MoeLayer.train_capacity_factor = %TRAIN_EXPERT_CAPACITY_FACTOR\n    \n#### Parameters for sparse_decoder/moe_layers.MoeLayer:\n\n    sparse_decoder/moe_layers.MoeLayer.router = \\\n        @sparse_decoder/routing.TokensChooseMaskedRouter()\n    \n#### Parameters for moe_partitioning.MoePjitPartitioner:\n\n    moe_partitioning.MoePjitPartitioner.model_parallel_submesh = \\\n        %MODEL_PARALLEL_SUBMESH\n    moe_partitioning.MoePjitPartitioner.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_partitioning.MoePjitPartitioner.num_partitions = %NUM_MODEL_PARTITIONS\n    \n#### Parameters for moe_trainer.MoeTrainer:\n\n    moe_trainer.MoeTrainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    moe_trainer.MoeTrainer.num_expert_partitions = %NUM_EXPERT_PARTITIONS\n    moe_trainer.MoeTrainer.num_microbatches = 8\n    \n#### Parameters for dense_attention.MultiHeadDotProductAttention:\n\n    dense_attention.MultiHeadDotProductAttention.bias_init = %BIAS_INIT\n    dense_attention.MultiHeadDotProductAttention.broadcast_dropout = True\n    dense_attention.MultiHeadDotProductAttention.dropout_rate = %DROPOUT_RATE\n    dense_attention.MultiHeadDotProductAttention.dtype = %ACTIVATION_DTYPE\n    dense_attention.MultiHeadDotProductAttention.head_dim = %HEAD_DIM\n    dense_attention.MultiHeadDotProductAttention.kernel_init = \\\n        @attention_kernel_init/linen.initializers.variance_scaling()\n    dense_attention.MultiHeadDotProductAttention.num_heads = %NUM_HEADS\n    dense_attention.MultiHeadDotProductAttention.use_bias = False\n    dense_attention.MultiHeadDotProductAttention.use_rotary_embedding = True\n    \n#### Parameters for bias_init/linen.initializers.normal:\n\n    bias_init/linen.initializers.normal.stddev = 1e-06\n    \n#### Parameters for router_init/linen.initializers.normal:\n\n    router_init/linen.initializers.normal.stddev = 0.02\n    \n#### Parameters for token_embedder_init/linen.initializers.normal:\n\n    token_embedder_init/linen.initializers.normal.stddev = 1.0\n    \n#### Parameters for partitioning.PjitPartitioner:\n\n    partitioning.PjitPartitioner.logical_axis_rules = \\\n        @partitioning.standard_logical_axis_rules()\n    partitioning.PjitPartitioner.model_parallel_submesh = None\n    partitioning.PjitPartitioner.num_partitions = 1\n    \n#### Parameters for utils.RestoreCheckpointConfig:\n\n    utils.RestoreCheckpointConfig.path = []\n    \n#### Parameters for routing.RouterWeights:\n\n    routing.RouterWeights.bias_init = %BIAS_INIT\n    routing.RouterWeights.dtype = 'float32'\n    routing.RouterWeights.kernel_init = @router_init/linen.initializers.normal()\n    routing.RouterWeights.use_bias = False\n    \n#### Parameters for utils.SaveCheckpointConfig:\n\n    utils.SaveCheckpointConfig.dtype = 'float32'\n    utils.SaveCheckpointConfig.keep = 20\n    utils.SaveCheckpointConfig.period = 2500\n    utils.SaveCheckpointConfig.save_dataset = True\n    \n#### Parameters for seqio.SentencePieceVocabulary:\n\n    seqio.SentencePieceVocabulary.extra_ids = 300\n    seqio.SentencePieceVocabulary.sentencepiece_model_file = \\\n        'gs://rosinality-tpu-bucket/sentencepiece.model'\n    \n#### Parameters for moe_architecture.SparseDecoder:\n\n    moe_architecture.SparseDecoder.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoder.dtype = %ACTIVATION_DTYPE\n    moe_architecture.SparseDecoder.layer_factory = @t5_architecture.DecoderLayer\n    moe_architecture.SparseDecoder.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoder.num_layers = %NUM_DECODER_LAYERS\n    moe_architecture.SparseDecoder.num_sparse_layers = %NUM_DECODER_SPARSE_LAYERS\n    moe_architecture.SparseDecoder.output_logits_factory = \\\n        @output_logits/dense.DenseGeneral\n    moe_architecture.SparseDecoder.sparse_layer_factory = \\\n        @moe_architecture.SparseDecoderLayer\n    moe_architecture.SparseDecoder.sparse_layout = %DECODER_SPARSE_LAYOUT\n    \n#### Parameters for moe_architecture.SparseDecoderLayer:\n\n    moe_architecture.SparseDecoderLayer.activation_partitioning_dims = \\\n        %ACTIVATION_PARTITIONING_DIMS\n    moe_architecture.SparseDecoderLayer.dropout_factory = %DROPOUT_FACTORY\n    moe_architecture.SparseDecoderLayer.encoder_decoder_attention = None\n    moe_architecture.SparseDecoderLayer.extra_mlp = @dense.MlpBlock()\n    moe_architecture.SparseDecoderLayer.layer_norm_factory = @layer_norm.T5LayerNorm\n    moe_architecture.SparseDecoderLayer.mlp = @sparse_decoder/moe_layers.MoeLayer()\n    moe_architecture.SparseDecoderLayer.scanned = False\n    moe_architecture.SparseDecoderLayer.self_attention = \\\n        @dense_attention.MultiHeadDotProductAttention()\n    \n#### Parameters for layer_norm.T5LayerNorm:\n\n    layer_norm.T5LayerNorm.dtype = %ACTIVATION_DTYPE\n    \n#### Parameters for routing.TokensChooseMaskedRouter:\n\n    routing.TokensChooseMaskedRouter.dtype = 'float32'\n    routing.TokensChooseMaskedRouter.ignore_padding_tokens = False\n    routing.TokensChooseMaskedRouter.jitter_noise = %JITTER_NOISE\n    routing.TokensChooseMaskedRouter.num_selected_experts = %NUM_SELECTED_EXPERTS\n    routing.TokensChooseMaskedRouter.router_weights = @routing.RouterWeights()\n    \n#### Parameters for sparse_decoder/routing.TokensChooseMaskedRouter:\n\n    sparse_decoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for sparse_encoder/routing.TokensChooseMaskedRouter:\n\n    sparse_encoder/routing.TokensChooseMaskedRouter.batch_prioritized_routing = False\n    \n#### Parameters for train_script.train:\n\n    train_script.train.checkpoint_cfg = @utils.CheckpointConfig()\n    train_script.train.eval_period = 2500\n    train_script.train.eval_steps = 20\n    train_script.train.infer_eval_dataset_cfg = None\n    train_script.train.model = %MODEL\n    train_script.train.model_dir = %MODEL_DIR\n    train_script.train.partitioner = @moe_partitioning.MoePjitPartitioner()\n    train_script.train.random_seed = 42\n    train_script.train.stats_period = 10\n    train_script.train.summarize_config_fn = @gin_utils.summarize_gin_config\n    train_script.train.total_steps = %TRAIN_STEPS\n    train_script.train.train_dataset_cfg = @train/utils.DatasetConfig()\n    train_script.train.train_eval_dataset_cfg = None\n    train_script.train.trainer_cls = @moe_trainer.MoeTrainer\n    train_script.train.use_hardware_rng = %USE_HARDWARE_RNG\n    \n#### Parameters for trainer.Trainer:\n\n    trainer.Trainer.learning_rate_fn = @utils.create_learning_rate_scheduler()\n    trainer.Trainer.num_microbatches = None\n    \n#### Parameters for attention_kernel_init/linen.initializers.variance_scaling:\n\n    attention_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    attention_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    attention_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for expert_kernel_init/linen.initializers.variance_scaling:\n\n    expert_kernel_init/linen.initializers.variance_scaling.distribution = 'normal'\n    expert_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    expert_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for mlp_kernel_init/linen.initializers.variance_scaling:\n\n    mlp_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    mlp_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    mlp_kernel_init/linen.initializers.variance_scaling.scale = %SCALE\n    \n#### Parameters for output_logits_kernel_init/linen.initializers.variance_scaling:\n\n    output_logits_kernel_init/linen.initializers.variance_scaling.distribution = \\\n        'truncated_normal'\n    output_logits_kernel_init/linen.initializers.variance_scaling.mode = 'fan_in'\n    output_logits_kernel_init/linen.initializers.variance_scaling.scale = %SCALE"}.
I0512 01:57:09.850217 139837062190656 logging_writer.py:48] [0] collection=train timing/init_or_restore_seconds=684.864
I0512 01:57:09.867022 139933136287744 train.py:573] Saving checkpoint before the training loop starts.
I0512 01:57:10.119198 140497406441472 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.160365 140139156215808 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.234433 139861676046336 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.288622 139933136287744 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.303746 139995375843328 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.310272 139867290535936 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.422916 140600233973760 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
I0512 01:57:10.502687 139836023437312 checkpoints.py:786] Saving checkpoint for step 0 to gs://rosinality-tpu-bucket/openmoe_8b_ul2/training/checkpoint_0.tmp-1715479029
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479031.988345  291751 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715479031.998015  297394 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.002320  284980 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.004492  268433 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.012762  264564 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.012440  264715 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.024558  280635 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.046223  265344 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
I0000 00:00:1715479032.055587  265473 gcs_resource.cc:109] Using default AdmissionQueue with limit 32
I0000 00:00:1715479032.225266  290697 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.231790  274106 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.248897  270191 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.269282  286576 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.270619  270334 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.330784  270984 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0000 00:00:1715479032.512017  271101 google_auth_provider.cc:180] Running on GCE, using service account 1081612546678-compute@developer.gserviceaccount.com
I0512 01:57:57.978779 139995375843328 checkpoints.py:797] Writing dataset iterator state to 'train_ds-002-of-008'.
I0512 01:57:57.978713 139861676046336 checkpoints.py:797] Writing dataset iterator state to 'train_ds-001-of-008'.
I0512 01:57:57.978747 140139156215808 checkpoints.py:797] Writing dataset iterator state to 'train_ds-006-of-008'.
I0512 01:57:57.981233 140497406441472 checkpoints.py:797] Writing dataset iterator state to 'train_ds-005-of-008'.
I0512 01:57:57.978867 139867290535936 checkpoints.py:797] Writing dataset iterator state to 'train_ds-003-of-008'.
I0512 01:57:57.978813 139836023437312 checkpoints.py:797] Writing dataset iterator state to 'train_ds-007-of-008'.
I0512 01:57:57.979010 140600233973760 checkpoints.py:797] Writing dataset iterator state to 'train_ds-004-of-008'.
I0512 01:57:57.979738 139933136287744 checkpoints.py:797] Writing dataset iterator state to 'train_ds-000-of-008'.
2024-05-12 02:04:38.337305: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:04:38.337365: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:04:38.338038 140139156215808 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f7316f4a3b0>)
I0000 00:00:1715479479.975899  265473 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 02:04:55.669701: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:04:55.669772: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:04:55.670359 139861676046336 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f327bdf63b0>)
I0000 00:00:1715479496.648582  264564 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 7 failed with exit status 1. Continuing.
2024-05-12 02:05:13.582169: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:05:13.582255: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:05:13.583644 140600233973760 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7fde714ea3b0>)
I0000 00:00:1715479515.151973  265344 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
2024-05-12 02:05:16.773877: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:05:16.773956: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:05:16.774660 140497406441472 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7fc68043e440>)
I0000 00:00:1715479518.137183  264715 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 4 failed with exit status 1. Continuing.
2024-05-12 02:05:26.019587: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:05:26.019668: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:05:26.020238 139836023437312 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f2c82c3a3b0>)
I0000 00:00:1715479527.040450  284980 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 1 failed with exit status 1. Continuing.
##### Command execution on worker 2 failed with exit status 1. Continuing.
##### Command execution on worker 5 failed with exit status 1. Continuing.
2024-05-12 02:05:56.472107: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:05:56.472169: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:05:56.472716 139867290535936 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f33caa1a3b0>)
I0000 00:00:1715479557.570667  280635 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 3 failed with exit status 1. Continuing.
2024-05-12 02:06:25.458407: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:06:25.458481: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:06:25.459086 139933136287744 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f431f23a3b0>)
I0000 00:00:1715479586.507393  291751 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
##### Command execution on worker 6 failed with exit status 1. Continuing.
2024-05-12 02:07:24.724461: W tensorflow/core/framework/op_kernel.cc:1839] OP_REQUIRES failed at iterator_ops.cc:1056 : FAILED_PRECONDITION: SentencepieceOp is stateful.
2024-05-12 02:07:24.724543: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: FAILED_PRECONDITION: SentencepieceOp is stateful.
E0512 02:07:24.725661 139995375843328 checkpoints.py:803] Input pipeline must be stateless in order to checkpoint. Cache stateful steps offline or disable iterator checkpointing.
Traceback (most recent call last):
  File "/home/rosinality/./t5x/t5x/train.py", line 964, in <module>
    config_utils.run(main)
  File "/home/rosinality/t5x/t5x/config_utils.py", line 214, in run
    gin_utils.run(main)
  File "/home/rosinality/t5x/t5x/gin_utils.py", line 127, in run
    app.run(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 308, in run
    _run_main(main, args)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/absl/app.py", line 254, in _run_main
    sys.exit(main(argv))
  File "/home/rosinality/./t5x/t5x/train.py", line 898, in main
    _main(argv)
  File "/home/rosinality/./t5x/t5x/train.py", line 959, in _main
    train_using_gin()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1605, in gin_wrapper
    utils.augment_exception_message_and_reraise(e, err_str)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/utils.py", line 41, in augment_exception_message_and_reraise
    raise proxy.with_traceback(exception.__traceback__) from None
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/gin/config.py", line 1582, in gin_wrapper
    return fn(*new_args, **new_kwargs)
  File "/home/rosinality/./t5x/t5x/train.py", line 574, in train
    checkpoint_manager.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 406, in save
    self._checkpointer.save(
  File "/home/rosinality/t5x/t5x/utils.py", line 281, in save
    self._save_checkpointer.save(
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 806, in save
    raise e
  File "/home/rosinality/t5x/t5x/checkpoints.py", line 800, in save
    self._dataset_iterator.save(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 288, in save
    self._it.save(filename)
  File "/home/rosinality/t5x/t5x/utils.py", line 697, in save
    return self._iterator.save(filename)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/clu/data/dataset_iterator.py", line 204, in save
    self._ckpt.write(os.fspath(filename))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2320, in write
    return self._write(file_prefix, options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 2369, in _write
    output = self._saver.save(file_prefix=file_prefix, options=options)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1356, in save
    save_path, new_feed_additions = self._save_cached_when_graph_building(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1284, in _save_cached_when_graph_building
    self._gather_serialized_tensors(object_graph_tensor))
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/checkpoint.py", line 1245, in _gather_serialized_tensors
    save_util.serialize_graph_view(self._graph_view,
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 309, in serialize_graph_view
    serialized_tensors = _get_and_write_tensors_to_serialize(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 157, in _get_and_write_tensors_to_serialize
    trackable, tensor_dict = _get_tensors_from_legacy_saveable(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util.py", line 190, in _get_tensors_from_legacy_saveable
    save_util_v1.generate_saveable_objects(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/checkpoint/save_util_v1.py", line 180, in generate_saveable_objects
    maybe_saveable = saveable_object_util.create_saveable_object(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 473, in create_saveable_object
    return factory(name=key)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/training/saving/saveable_object_util.py", line 536, in create_saveable
    tensor_dict = save_fn()
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py", line 883, in _serialize_to_tensors
    serialized_iterator = gen_dataset_ops.serialize_iterator(
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py", line 6523, in serialize_iterator
    _ops.raise_from_not_ok_status(e, name)
  File "/home/rosinality/openmoe_venv/lib/python3.10/site-packages/tensorflow/python/framework/ops.py", line 5983, in raise_from_not_ok_status
    raise core._status_to_exception(e) from None  # pylint: disable=protected-access
tensorflow.python.framework.errors_impl.FailedPreconditionError: {{function_node __wrapped__SerializeIterator_device_/job:localhost/replica:0/task:0/device:CPU:0}} SentencepieceOp is stateful. [Op:SerializeIterator] name: 
  In call to configurable 'train' (<function train at 0x7f519d1de3b0>)
I0000 00:00:1715479646.295748  268433 tfrt_cpu_pjrt_client.cc:352] TfrtCpuClient destroyed.
E0512 02:07:30.022989  269082 coredump_hook.cc:447] RAW: Remote crash data gathering hook invoked.
E0512 02:07:30.023008  269082 coredump_hook.cc:486] RAW: Called via ReportEvent and disabled coredump
E0512 02:07:30.023019  269082 client.cc:272] RAW: Coroner client retries enabled (b/136286901), will retry for up to 30 sec.
E0512 02:07:30.023022  269082 coredump_hook.cc:542] RAW: Sending fingerprint to remote end.
E0512 02:07:30.023073  269082 coredump_hook.cc:551] RAW: Cannot send fingerprint to Coroner: [NOT_FOUND] stat failed on crash reporting socket /var/google/services/logmanagerd/remote_coredump.socket (Is the listener running?): No such file or directory
E0512 02:07:30.023079  269082 coredump_hook.cc:603] RAW: Dumping core locally.
##### Command execution on worker 0 failed with exit status 1. Continuing.
